14 Ben Sigelman, “Three Pillars with Zero Answers—Towards a New Scorecard for Observability,” Lightstep
(blog post), December 5, 2018, https://oreil.ly/R3LwC.
Suitable for Your Scale
Much of the work in the space of distributed systems observability has been inspired
by work done in large-scale distributed systems. This unfortunately can lead to us
trying to re-create solutions for systems of much larger scale than our own without
understanding the trade-offs.
Systems with massive scale will often have to make specific trade-offs to reduce the
functionality of their systems in order to handle the scale they operate at. Dapper, for
example, had to make use of a highly aggressive random sampling of data (effectively
“dropping” a lot of information) to be able to cope with Google scale. As Ben Sigel‐
man, founder of LightStep and creator of Dapper, puts it:14
Google’s microservices generate about 5 billion RPCs per second; building observabil‐
ity tools that scale to 5B RPCs/sec therefore boils down to building observability tools
that are profoundly feature poor. If your organization is doing more like 5 million
RPCs/sec, that’s still quite impressive, but you should almost certainly not use what
Google uses: at 1/1000th the scale, you can afford much more powerful features.
You also ideally want a tool that can scale as you scale. Again, cost effectiveness can
come into play here. Even if your tool of choice can technically scale to support the
expected growth of your system, can you afford to keep paying for it?
The Expert in the Machine
I’ve talked a lot about tools in this chapter, perhaps more than in any other chapter in
the book. This is partly due to the fundamental shift from viewing the world purely in
terms of monitoring to instead thinking about how to make our systems more
observable; this change in behavior requires tooling to help support it. It would be a
mistake, however, to see this shift as being purely about new tools, as I hope I’ve
already explained. Nonetheless, with a large number of different vendors vying for
our attention, we also have to be cautious.
For over a decade now, I have seen multiple vendors claim some degree of smarts in
terms of how their system will be the one that will magically detect problems and tell
us exactly what we need to do to fix things. This seems to come in waves, but with the
recent buzz around machine learning (ML) and artificial intelligence (AI) only
increasing, I’m seeing more claims around automated anomaly detection being made.
I am dubious as to how effective this can be in a fully automated fashion, and even
then it would be problematic to assume that all the expertise you need could be auto‐
mated away.
340 
| 
Chapter 10: From Monitoring to Observability
