Are We Doing OK?
We’ve talked a lot about the things you could be doing as the operator of a system—
the mindset you need, the information you might need to gather. But how do you
know if you’re doing too much—or not enough? How do you know if you are doing a
good enough job, or that your system is working well enough?
Binary concepts of a system being “up” or “down” start to have less and less meaning
as the system becomes more complex. With a single-process monolithic system, it’s
easier to see system health as a black-and-white quality. But what about a distributed
system? If one instance of a microservice is unreachable, is that a problem? Is a
microservice “healthy” if it is reachable? What about a situation in which our
Returns microservice is available, but half the functionality it provides requires the
use of the downstream Inventory microservice, which is currently experiencing
issues? Does that mean we consider the Returns microservice healthy or not healthy?
As things become more complex, it becomes increasingly important to take a step
back and think about things from a different vantage point. Think of a beehive. You
could look at an individual bee and determine that it’s not happy. Perhaps it’s lost
one of its wings and thus can no longer fly. This is certainly a problem for that indi‐
vidual bee, but can you extend from that any observation about the health of the hive
itself? No—you’d need to look at the health of the hive in a more holistic fashion.
One bee being sick doesn’t mean the whole hive is sick.
We can try to work out if a service is healthy by deciding, for example, what a good
CPU level is, or what makes for an acceptable response time. If our monitoring sys‐
tem detects that the actual values fall outside this safe level, we can trigger an alert.
However, in many ways, these values are one step removed from what we actually
want to track—namely, is the system working? The more complex the interactions
between the services, the further removed we are from actually answering that ques‐
tion by just looking at one metric in isolation.
So we can gather a lot of information, but by itself that doesn’t help us answer the
question of whether the system is working properly. For that, we need to start think‐
ing a bit more in terms of defining what acceptable behavior looks like. Much work
has been done in the space of site reliability engineering (SRE), the focus of which is
how we can ensure that our systems can be reliable while still allowing for change.
From this space, we have a few useful concepts to explore.
Strap in—we’re about to enter acronym city.
Service-level agreement
A service-level agreement (SLA) is an agreement reached between the people building
the system and the people using the system. It describes not only what the users can
expect but also what happens if the system doesn’t reach this level of acceptable
Building Blocks for Observability 
| 
327
