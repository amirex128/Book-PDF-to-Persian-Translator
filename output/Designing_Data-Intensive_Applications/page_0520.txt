• Since the stream pipeline and the batch pipeline produce separate outputs, they
need to be merged in order to respond to user requests. This merge is fairly easy
if the computation is a simple aggregation over a tumbling window, but it
becomes significantly harder if the view is derived using more complex opera‐
tions such as joins and sessionization, or if the output is not a time series.
• Although it is great to have the ability to reprocess the entire historical dataset,
doing so frequently is expensive on large datasets. Thus, the batch pipeline often
needs to be set up to process incremental batches (e.g., an hour’s worth of data at
the end of every hour) rather than reprocessing everything. This raises the prob‐
lems discussed in “Reasoning About Time” on page 468, such as handling strag‐
glers and handling windows that cross boundaries between batches.
Incrementalizing a batch computation adds complexity, making it more akin to
the streaming layer, which runs counter to the goal of keeping the batch layer as
simple as possible.
Unifying batch and stream processing
More recent work has enabled the benefits of the lambda architecture to be enjoyed
without its downsides, by allowing both batch computations (reprocessing historical
data) and stream computations (processing events as they arrive) to be implemented
in the same system [15].
Unifying batch and stream processing in one system requires the following features,
which are becoming increasingly widely available:
• The ability to replay historical events through the same processing engine that
handles the stream of recent events. For example, log-based message brokers
have the ability to replay messages (see “Replaying old messages” on page 451),
and some stream processors can read input from a distributed filesystem like
HDFS.
• Exactly-once semantics for stream processors—that is, ensuring that the output
is the same as if no faults had occurred, even if faults did in fact occur (see “Fault
Tolerance” on page 476). Like with batch processing, this requires discarding the
partial output of any failed tasks.
• Tools for windowing by event time, not by processing time, since processing
time is meaningless when reprocessing historical events (see “Reasoning About
Time” on page 468). For example, Apache Beam provides an API for expressing
such computations, which can then be run using Apache Flink or Google Cloud
Dataflow. 
498 
| 
Chapter 12: The Future of Data Systems
