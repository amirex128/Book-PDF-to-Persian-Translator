This approach works, but implementing it with MapReduce is often very inefficient,
because MapReduce does not account for the iterative nature of the algorithm: it will
always read the entire input dataset and produce a completely new output dataset,
even if only a small part of the graph has changed compared to the last iteration.
The Pregel processing model
As an optimization for batch processing graphs, the bulk synchronous parallel (BSP)
model of computation [70] has become popular. Among others, it is implemented by
Apache Giraph [37], Spark’s GraphX API, and Flink’s Gelly API [71]. It is also
known as the Pregel model, as Google’s Pregel paper popularized this approach for
processing graphs [72].
Recall that in MapReduce, mappers conceptually “send a message” to a particular call
of the reducer because the framework collects together all the mapper outputs with
the same key. A similar idea is behind Pregel: one vertex can “send a message” to
another vertex, and typically those messages are sent along the edges in a graph.
In each iteration, a function is called for each vertex, passing it all the messages that
were sent to it—much like a call to the reducer. The difference from MapReduce is
that in the Pregel model, a vertex remembers its state in memory from one iteration
to the next, so the function only needs to process new incoming messages. If no mes‐
sages are being sent in some part of the graph, no work needs to be done.
It’s a bit similar to the actor model (see “Distributed actor frameworks” on page 138),
if you think of each vertex as an actor, except that vertex state and messages between
vertices are fault-tolerant and durable, and communication proceeds in fixed rounds:
at every iteration, the framework delivers all messages sent in the previous iteration.
Actors normally have no such timing guarantee.
Fault tolerance
The fact that vertices can only communicate by message passing (not by querying
each other directly) helps improve the performance of Pregel jobs, since messages can
be batched and there is less waiting for communication. The only waiting is between
iterations: since the Pregel model guarantees that all messages sent in one iteration
are delivered in the next iteration, the prior iteration must completely finish, and all
of its messages must be copied over the network, before the next one can start.
Even though the underlying network may drop, duplicate, or arbitrarily delay mes‐
sages (see “Unreliable Networks” on page 277), Pregel implementations guarantee
that messages are processed exactly once at their destination vertex in the following
iteration. Like MapReduce, the framework transparently recovers from faults in
order to simplify the programming model for algorithms on top of Pregel.
Beyond MapReduce 
| 
425
