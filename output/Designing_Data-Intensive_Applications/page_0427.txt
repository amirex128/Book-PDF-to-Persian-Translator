In order to achieve good throughput in a batch process, the computation must be (as
much as possible) local to one machine. Making random-access requests over the
network for every record you want to process is too slow. Moreover, querying a
remote database would mean that the batch job becomes nondeterministic, because
the data in the remote database might change.
Thus, a better approach would be to take a copy of the user database (for example,
extracted from a database backup using an ETL process—see “Data Warehousing” on
page 91) and to put it in the same distributed filesystem as the log of user activity
events. You would then have the user database in one set of files in HDFS and the
user activity records in another set of files, and could use MapReduce to bring
together all of the relevant records in the same place and process them efficiently.
Sort-merge joins
Recall that the purpose of the mapper is to extract a key and value from each input
record. In the case of Figure 10-2, this key would be the user ID: one set of mappers
would go over the activity events (extracting the user ID as the key and the activity
event as the value), while another set of mappers would go over the user database
(extracting the user ID as the key and the user’s date of birth as the value). This pro‐
cess is illustrated in Figure 10-3.
Figure 10-3. A reduce-side sort-merge join on user ID. If the input datasets are parti‐
tioned into multiple files, each could be processed with multiple mappers in parallel.
When the MapReduce framework partitions the mapper output by key and then sorts
the key-value pairs, the effect is that all the activity events and the user record with
the same user ID become adjacent to each other in the reducer input. The Map‐
Reduce job can even arrange the records to be sorted such that the reducer always
MapReduce and Distributed Filesystems 
| 
405
