HDFS has scaled well: at the time of writing, the biggest HDFS deployments run on
tens of thousands of machines, with combined storage capacity of hundreds of peta‐
bytes [23]. Such large scale has become viable because the cost of data storage and
access on HDFS, using commodity hardware and open source software, is much
lower than that of the equivalent capacity on a dedicated storage appliance [24]. 
MapReduce Job Execution
MapReduce is a programming framework with which you can write code to process
large datasets in a distributed filesystem like HDFS. The easiest way of understanding
it is by referring back to the web server log analysis example in “Simple Log Analysis”
on page 391. The pattern of data processing in MapReduce is very similar to this
example:
1. Read a set of input files, and break it up into records. In the web server log exam‐
ple, each record is one line in the log (that is, \n is the record separator).
2. Call the mapper function to extract a key and value from each input record. In
the preceding example, the mapper function is awk '{print $7}': it extracts the
URL ($7) as the key, and leaves the value empty.
3. Sort all of the key-value pairs by key. In the log example, this is done by the first
sort command.
4. Call the reducer function to iterate over the sorted key-value pairs. If there are
multiple occurrences of the same key, the sorting has made them adjacent in the
list, so it is easy to combine those values without having to keep a lot of state in
memory. In the preceding example, the reducer is implemented by the command
uniq -c, which counts the number of adjacent records with the same key.
Those four steps can be performed by one MapReduce job. Steps 2 (map) and 4
(reduce) are where you write your custom data processing code. Step 1 (breaking files
into records) is handled by the input format parser. Step 3, the sort step, is implicit
in MapReduce—you don’t have to write it, because the output from the mapper is
always sorted before it is given to the reducer.
To create a MapReduce job, you need to implement two callback functions, the map‐
per and reducer, which behave as follows (see also “MapReduce Querying” on page
46):
Mapper
The mapper is called once for every input record, and its job is to extract the key
and value from the input record. For each input, it may generate any number of
key-value pairs (including none). It does not keep any state from one input
record to the next, so each record is handled independently.
MapReduce and Distributed Filesystems 
| 
399
