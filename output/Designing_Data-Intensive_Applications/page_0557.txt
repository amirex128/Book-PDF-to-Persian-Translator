past, moral imagination is required, and that’s something only humans can provide
[87]. Data and models should be our tools, not our masters.
Responsibility and accountability
Automated decision making opens the question of responsibility and accountability
[87]. If a human makes a mistake, they can be held accountable, and the person affec‐
ted by the decision can appeal. Algorithms make mistakes too, but who is accounta‐
ble if they go wrong [88]? When a self-driving car causes an accident, who is
responsible? If an automated credit scoring algorithm systematically discriminates
against people of a particular race or religion, is there any recourse? If a decision by
your machine learning system comes under judicial review, can you explain to the
judge how the algorithm made its decision?
Credit rating agencies are an old example of collecting data to make decisions about
people. A bad credit score makes life difficult, but at least a credit score is normally
based on relevant facts about a person’s actual borrowing history, and any errors in
the record can be corrected (although the agencies normally do not make this easy).
However, scoring algorithms based on machine learning typically use a much wider
range of inputs and are much more opaque, making it harder to understand how a
particular decision has come about and whether someone is being treated in an
unfair or discriminatory way [89].
A credit score summarizes “How did you behave in the past?” whereas predictive
analytics usually work on the basis of “Who is similar to you, and how did people like
you behave in the past?” Drawing parallels to others’ behavior implies stereotyping
people, for example based on where they live (a close proxy for race and socioeco‐
nomic class). What about people who get put in the wrong bucket? Furthermore, if a
decision is incorrect due to erroneous data, recourse is almost impossible [87].
Much data is statistical in nature, which means that even if the probability distribu‐
tion on the whole is correct, individual cases may well be wrong. For example, if the
average life expectancy in your country is 80 years, that doesn’t mean you’re expected
to drop dead on your 80th birthday. From the average and the probability distribu‐
tion, you can’t say much about the age to which one particular person will live. Simi‐
larly, the output of a prediction system is probabilistic and may well be wrong in
individual cases.
A blind belief in the supremacy of data for making decisions is not only delusional, it
is positively dangerous. As data-driven decision making becomes more widespread,
we will need to figure out how to make algorithms accountable and transparent, how
to avoid reinforcing existing biases, and how to fix them when they inevitably make
mistakes.
We will also need to figure out how to prevent data being used to harm people, and
realize its positive potential instead. For example, analytics can reveal financial and
Doing the Right Thing 
| 
535
