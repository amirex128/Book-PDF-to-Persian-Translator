Even if particular users cannot be personally reidentified from the bucket of people
targeted by a particular ad, they have lost their agency about the disclosure of some
intimate information, such as whether they suffer from some illness. It is not the user
who decides what is revealed to whom on the basis of their personal preferences—it
is the company that exercises the privacy right with the goal of maximizing its profit.
Many companies have a goal of not being perceived as creepy—avoiding the question
of how intrusive their data collection actually is, and instead focusing on managing
user perceptions. And even these perceptions are often managed poorly: for example,
something may be factually correct, but if it triggers painful memories, the user may
not want to be reminded about it [100]. With any kind of data we should expect the
possibility that it is wrong, undesirable, or inappropriate in some way, and we need to
build mechanisms for handling those failures. Whether something is “undesirable” or
“inappropriate” is of course down to human judgment; algorithms are oblivious to
such notions unless we explicitly program them to respect human needs. As engi‐
neers of these systems we must be humble, accepting and planning for such failings.
Privacy settings that allow a user of an online service to control which aspects of their
data other users can see are a starting point for handing back some control to users.
However, regardless of the setting, the service itself still has unfettered access to the
data, and is free to use it in any way permitted by the privacy policy. Even if the ser‐
vice promises not to sell the data to third parties, it usually grants itself unrestricted
rights to process and analyze the data internally, often going much further than what
is overtly visible to users.
This kind of large-scale transfer of privacy rights from individuals to corporations is
historically unprecedented [99]. Surveillance has always existed, but it used to be
expensive and manual, not scalable and automated. Trust relationships have always
existed, for example between a patient and their doctor, or between a defendant and
their attorney—but in these cases the use of data has been strictly governed by ethical,
legal, and regulatory constraints. Internet services have made it much easier to amass
huge amounts of sensitive information without meaningful consent, and to use it at
massive scale without users understanding what is happening to their private data.
Data as assets and power
Since behavioral data is a byproduct of users interacting with a service, it is some‐
times called “data exhaust”—suggesting that the data is worthless waste material.
Viewed this way, behavioral and predictive analytics can be seen as a form of recy‐
cling that extracts value from data that would have otherwise been thrown away.
More correct would be to view it the other way round: from an economic point of
view, if targeted advertising is what pays for a service, then behavioral data about
people is the service’s core asset. In this case, the application with which the user
interacts is merely a means to lure users into feeding more and more personal infor‐
540 
| 
Chapter 12: The Future of Data Systems
