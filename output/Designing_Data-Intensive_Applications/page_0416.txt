same principle as we discussed in “SSTables and LSM-Trees” on page 76: chunks of
data can be sorted in memory and written out to disk as segment files, and then mul‐
tiple sorted segments can be merged into a larger sorted file. Mergesort has sequential
access patterns that perform well on disks. (Remember that optimizing for sequential
I/O was a recurring theme in Chapter 3. The same pattern reappears here.)
The sort utility in GNU Coreutils (Linux) automatically handles larger-than-
memory datasets by spilling to disk, and automatically parallelizes sorting across
multiple CPU cores [9]. This means that the simple chain of Unix commands we saw
earlier easily scales to large datasets, without running out of memory. The bottleneck
is likely to be the rate at which the input file can be read from disk. 
The Unix Philosophy
It’s no coincidence that we were able to analyze a log file quite easily, using a chain of
commands like in the previous example: this was in fact one of the key design ideas of
Unix, and it remains astonishingly relevant today. Let’s look at it in some more depth
so that we can borrow some ideas from Unix [10].
Doug McIlroy, the inventor of Unix pipes, first described them like this in 1964 [11]:
“We should have some ways of connecting programs like [a] garden hose—screw in
another segment when it becomes necessary to massage data in another way. This is
the way of I/O also.” The plumbing analogy stuck, and the idea of connecting pro‐
grams with pipes became part of what is now known as the Unix philosophy—a set of
design principles that became popular among the developers and users of Unix. The
philosophy was described in 1978 as follows [12, 13]:
1. Make each program do one thing well. To do a new job, build afresh rather than
complicate old programs by adding new “features”.
2. Expect the output of every program to become the input to another, as yet
unknown, program. Don’t clutter output with extraneous information. Avoid
stringently columnar or binary input formats. Don’t insist on interactive input.
3. Design and build software, even operating systems, to be tried early, ideally within
weeks. Don’t hesitate to throw away the clumsy parts and rebuild them.
4. Use tools in preference to unskilled help to lighten a programming task, even if
you have to detour to build the tools and expect to throw some of them out after
you’ve finished using them.
This approach—automation, rapid prototyping, incremental iteration, being friendly
to experimentation, and breaking down large projects into manageable chunks—
sounds remarkably like the Agile and DevOps movements of today. Surprisingly little
has changed in four decades.
394 
| 
Chapter 10: Batch Processing
