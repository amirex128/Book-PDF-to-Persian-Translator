seek. If that part of the data file is already in the filesystem cache, a read doesn’t
require any disk I/O at all.
A storage engine like Bitcask is well suited to situations where the value for each key
is updated frequently. For example, the key might be the URL of a cat video, and the
value might be the number of times it has been played (incremented every time
someone hits the play button). In this kind of workload, there are a lot of writes, but
there are not too many distinct keys—you have a large number of writes per key, but
it’s feasible to keep all keys in memory.
As described so far, we only ever append to a file—so how do we avoid eventually
running out of disk space? A good solution is to break the log into segments of a cer‐
tain size by closing a segment file when it reaches a certain size, and making subse‐
quent writes to a new segment file. We can then perform compaction on these
segments, as illustrated in Figure 3-2. Compaction means throwing away duplicate
keys in the log, and keeping only the most recent update for each key.
Figure 3-2. Compaction of a key-value update log (counting the number of times each
cat video was played), retaining only the most recent value for each key.
Moreover, since compaction often makes segments much smaller (assuming that a
key is overwritten several times on average within one segment), we can also merge
several segments together at the same time as performing the compaction, as shown
in Figure 3-3. Segments are never modified after they have been written, so the
merged segment is written to a new file. The merging and compaction of frozen seg‐
ments can be done in a background thread, and while it is going on, we can still con‐
tinue to serve read and write requests as normal, using the old segment files. After the
merging process is complete, we switch read requests to using the new merged seg‐
ment instead of the old segments—and then the old segment files can simply be
deleted.
Data Structures That Power Your Database 
| 
73
