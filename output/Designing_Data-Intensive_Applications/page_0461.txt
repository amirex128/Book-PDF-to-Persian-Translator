CHAPTER 11
Stream Processing
A complex system that works is invariably found to have evolved from a simple system that
works. The inverse proposition also appears to be true: A complex system designed from
scratch never works and cannot be made to work.
—John Gall, Systemantics (1975)
In Chapter 10 we discussed batch processing—techniques that read a set of files as
input and produce a new set of output files. The output is a form of derived data; that
is, a dataset that can be recreated by running the batch process again if necessary. We
saw how this simple but powerful idea can be used to create search indexes, recom‐
mendation systems, analytics, and more.
However, one big assumption remained throughout Chapter 10: namely, that the
input is bounded—i.e., of a known and finite size—so the batch process knows when
it has finished reading its input. For example, the sorting operation that is central to
MapReduce must read its entire input before it can start producing output: it could
happen that the very last input record is the one with the lowest key, and thus needs
to be the very first output record, so starting the output early is not an option.
In reality, a lot of data is unbounded because it arrives gradually over time: your users
produced data yesterday and today, and they will continue to produce more data
tomorrow. Unless you go out of business, this process never ends, and so the dataset
is never “complete” in any meaningful way [1]. Thus, batch processors must artifi‐
cially divide the data into chunks of fixed duration: for example, processing a day’s
worth of data at the end of every day, or processing an hour’s worth of data at the end
of every hour.
The problem with daily batch processes is that changes in the input are only reflected
in the output a day later, which is too slow for many impatient users. To reduce the
delay, we can run the processing more frequently—say, processing a second’s worth
of data at the end of every second—or even continuously, abandoning the fixed time
439
