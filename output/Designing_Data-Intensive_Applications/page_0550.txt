Taken together, these observations mean that dataflow systems can provide the data
management services for many applications without requiring coordination, while
still giving strong integrity guarantees. Such coordination-avoiding data systems have
a lot of appeal: they can achieve better performance and fault tolerance than systems
that need to perform synchronous coordination [56].
For example, such a system could operate distributed across multiple datacenters in a
multi-leader configuration, asynchronously replicating between regions. Any one
datacenter can continue operating independently from the others, because no syn‐
chronous cross-region coordination is required. Such a system would have weak
timeliness guarantees—it could not be linearizable without introducing coordination
—but it can still have strong integrity guarantees.
In this context, serializable transactions are still useful as part of maintaining derived
state, but they can be run at a small scope where they work well [8]. Heterogeneous
distributed transactions such as XA transactions (see “Distributed Transactions in
Practice” on page 360) are not required. Synchronous coordination can still be intro‐
duced in places where it is needed (for example, to enforce strict constraints before
an operation from which recovery is not possible), but there is no need for everything
to pay the cost of coordination if only a small part of an application needs it [43].
Another way of looking at coordination and constraints: they reduce the number of
apologies you have to make due to inconsistencies, but potentially also reduce the
performance and availability of your system, and thus potentially increase the num‐
ber of apologies you have to make due to outages. You cannot reduce the number of
apologies to zero, but you can aim to find the best trade-off for your needs—the
sweet spot where there are neither too many inconsistencies nor too many availability
problems. 
Trust, but Verify
All of our discussion of correctness, integrity, and fault-tolerance has been under the
assumption that certain things might go wrong, but other things won’t. We call these
assumptions our system model (see “Mapping system models to the real world” on
page 309): for example, we should assume that processes can crash, machines can
suddenly lose power, and the network can arbitrarily delay or drop messages. But we
might also assume that data written to disk is not lost after fsync, that data in mem‐
ory is not corrupted, and that the multiplication instruction of our CPU always
returns the correct result.
These assumptions are quite reasonable, as they are true most of the time, and it
would be difficult to get anything done if we had to constantly worry about our com‐
puters making mistakes. Traditionally, system models take a binary approach toward
faults: we assume that some things can happen, and other things can never happen.
In reality, it is more a question of probabilities: some things are more likely, other
528 
| 
Chapter 12: The Future of Data Systems
