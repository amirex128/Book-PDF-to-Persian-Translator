action requiring data to be changed in several different places. With event sourcing,
you can design an event such that it is a self-contained description of a user action.
The user action then requires only a single write in one place—namely appending the
events to the log—which is easy to make atomic.
If the event log and the application state are partitioned in the same way (for exam‐
ple, processing an event for a customer in partition 3 only requires updating partition
3 of the application state), then a straightforward single-threaded log consumer needs
no concurrency control for writes—by construction, it only processes a single event
at a time (see also “Actual Serial Execution” on page 252). The log removes the non‐
determinism of concurrency by defining a serial order of events in a partition [24]. If
an event touches multiple state partitions, a bit more work is required, which we will
discuss in Chapter 12. 
Limitations of immutability
Many systems that don’t use an event-sourced model nevertheless rely on immutabil‐
ity: various databases internally use immutable data structures or multi-version data
to support point-in-time snapshots (see “Indexes and snapshot isolation” on page
241). Version control systems such as Git, Mercurial, and Fossil also rely on immuta‐
ble data to preserve version history of files.
To what extent is it feasible to keep an immutable history of all changes forever? The
answer depends on the amount of churn in the dataset. Some workloads mostly add
data and rarely update or delete; they are easy to make immutable. Other workloads
have a high rate of updates and deletes on a comparatively small dataset; in these
cases, the immutable history may grow prohibitively large, fragmentation may
become an issue, and the performance of compaction and garbage collection
becomes crucial for operational robustness [60, 61].
Besides the performance reasons, there may also be circumstances in which you need
data to be deleted for administrative reasons, in spite of all immutability. For exam‐
ple, privacy regulations may require deleting a user’s personal information after they
close their account, data protection legislation may require erroneous information to
be removed, or an accidental leak of sensitive information may need to be contained.
In these circumstances, it’s not sufficient to just append another event to the log to
indicate that the prior data should be considered deleted—you actually want to
rewrite history and pretend that the data was never written in the first place. For
example, Datomic calls this feature excision [62], and the Fossil version control sys‐
tem has a similar concept called shunning [63].
Truly deleting data is surprisingly hard [64], since copies can live in many places: for
example, storage engines, filesystems, and SSDs often write to a new location rather
than overwriting in place [52], and backups are often deliberately immutable to pre‐
vent accidental deletion or corruption. Deletion is more a matter of “making it harder
Databases and Streams 
| 
463
