In my limited experience I’ve dealt with long-lived network partitions in a single data
center (DC), PDU [power distribution unit] failures, switch failures, accidental power
cycles of whole racks, whole-DC backbone failures, whole-DC power failures, and a
hypoglycemic driver smashing his Ford pickup truck into a DC’s HVAC [heating, ven‐
tilation, and air conditioning] system. And I’m not even an ops guy.
—Coda Hale
In a distributed system, there may well be some parts of the system that are broken in
some unpredictable way, even though other parts of the system are working fine. This
is known as a partial failure. The difficulty is that partial failures are nondeterministic:
if you try to do anything involving multiple nodes and the network, it may sometimes
work and sometimes unpredictably fail. As we shall see, you may not even know
whether something succeeded or not, as the time it takes for a message to travel
across a network is also nondeterministic!
This nondeterminism and possibility of partial failures is what makes distributed sys‐
tems hard to work with [5].
Cloud Computing and Supercomputing
There is a spectrum of philosophies on how to build large-scale computing systems:
• At one end of the scale is the field of high-performance computing (HPC). Super‐
computers with thousands of CPUs are typically used for computationally inten‐
sive scientific computing tasks, such as weather forecasting or molecular
dynamics (simulating the movement of atoms and molecules).
• At the other extreme is cloud computing, which is not very well defined [6] but is
often associated with multi-tenant datacenters, commodity computers connected
with an IP network (often Ethernet), elastic/on-demand resource allocation, and
metered billing.
• Traditional enterprise datacenters lie somewhere between these extremes.
With these philosophies come very different approaches to handling faults. In a
supercomputer, a job typically checkpoints the state of its computation to durable
storage from time to time. If one node fails, a common solution is to simply stop the
entire cluster workload. After the faulty node is repaired, the computation is restarted
from the last checkpoint [7, 8]. Thus, a supercomputer is more like a single-node
computer than a distributed system: it deals with partial failure by letting it escalate
into total failure—if any part of the system fails, just let everything crash (like a kernel
panic on a single machine).
In this book we focus on systems for implementing internet services, which usually
look very different from supercomputers:
Faults and Partial Failures 
| 
275
