Log compaction
If you can only keep a limited amount of log history, you need to go through the
snapshot process every time you want to add a new derived data system. However,
log compaction provides a good alternative.
We discussed log compaction previously in “Hash Indexes” on page 72, in the con‐
text of log-structured storage engines (see Figure 3-2 for an example). The principle
is simple: the storage engine periodically looks for log records with the same key,
throws away any duplicates, and keeps only the most recent update for each key. This
compaction and merging process runs in the background.
In a log-structured storage engine, an update with a special null value (a tombstone)
indicates that a key was deleted, and causes it to be removed during log compaction.
But as long as a key is not overwritten or deleted, it stays in the log forever. The disk
space required for such a compacted log depends only on the current contents of the
database, not the number of writes that have ever occurred in the database. If the
same key is frequently overwritten, previous values will eventually be garbage-
collected, and only the latest value will be retained.
The same idea works in the context of log-based message brokers and change data
capture. If the CDC system is set up such that every change has a primary key, and
every update for a key replaces the previous value for that key, then it’s sufficient to
keep just the most recent write for a particular key.
Now, whenever you want to rebuild a derived data system such as a search index, you
can start a new consumer from offset 0 of the log-compacted topic, and sequentially
scan over all messages in the log. The log is guaranteed to contain the most recent
value for every key in the database (and maybe some older values)—in other words,
you can use it to obtain a full copy of the database contents without having to take
another snapshot of the CDC source database.
This log compaction feature is supported by Apache Kafka. As we shall see later in
this chapter, it allows the message broker to be used for durable storage, not just for
transient messaging.
API support for change streams
Increasingly, databases are beginning to support change streams as a first-class inter‐
face, rather than the typical retrofitted and reverse-engineered CDC efforts. For
example, RethinkDB allows queries to subscribe to notifications when the results of a
query change [36], Firebase [37] and CouchDB [38] provide data synchronization
based on a change feed that is also made available to applications, and Meteor uses
the MongoDB oplog to subscribe to data changes and update the user interface [39].
VoltDB allows transactions to continuously export data from a database in the form
of a stream [40]. The database represents an output stream in the relational data
456 
| 
Chapter 11: Stream Processing
