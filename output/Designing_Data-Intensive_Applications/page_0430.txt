records relating to a hot key to one of several reducers, chosen at random (in contrast
to conventional MapReduce, which chooses a reducer deterministically based on a
hash of the key). For the other input to the join, records relating to the hot key need
to be replicated to all reducers handling that key [40].
This technique spreads the work of handling the hot key over several reducers, which
allows it to be parallelized better, at the cost of having to replicate the other join input
to multiple reducers. The sharded join method in Crunch is similar, but requires the
hot keys to be specified explicitly rather than using a sampling job. This technique is
also very similar to one we discussed in “Skewed Workloads and Relieving Hot
Spots” on page 205, using randomization to alleviate hot spots in a partitioned data‐
base.
Hive’s skewed join optimization takes an alternative approach. It requires hot keys to
be specified explicitly in the table metadata, and it stores records related to those keys
in separate files from the rest. When performing a join on that table, it uses a map-
side join (see the next section) for the hot keys.
When grouping records by a hot key and aggregating them, you can perform the
grouping in two stages. The first MapReduce stage sends records to a random
reducer, so that each reducer performs the grouping on a subset of records for the
hot key and outputs a more compact aggregated value per key. The second Map‐
Reduce job then combines the values from all of the first-stage reducers into a single
value per key. 
Map-Side Joins
The join algorithms described in the last section perform the actual join logic in the
reducers, and are hence known as reduce-side joins. The mappers take the role of pre‐
paring the input data: extracting the key and value from each input record, assigning
the key-value pairs to a reducer partition, and sorting by key.
The reduce-side approach has the advantage that you do not need to make any
assumptions about the input data: whatever its properties and structure, the mappers
can prepare the data to be ready for joining. However, the downside is that all that
sorting, copying to reducers, and merging of reducer inputs can be quite expensive.
Depending on the available memory buffers, data may be written to disk several
times as it passes through the stages of MapReduce [37].
On the other hand, if you can make certain assumptions about your input data, it is
possible to make joins faster by using a so-called map-side join. This approach uses a
cut-down MapReduce job in which there are no reducers and no sorting. Instead,
each mapper simply reads one input file block from the distributed filesystem and
writes one output file to the filesystem—that is all.
408 
| 
Chapter 10: Batch Processing
