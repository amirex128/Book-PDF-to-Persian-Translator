iv. One difference is that with HDFS, computing tasks can be scheduled to run on the machine that stores a
copy of a particular file, whereas object stores usually keep storage and computation separate. Reading from a
local disk has a performance advantage if network bandwidth is a bottleneck. Note however that if erasure
coding is used, the locality advantage is lost, because the data from several machines must be combined in
order to reconstitute the original file [20].
files are written once, in a sequential fashion (not modifying any existing part of a file
once it has been written).
While Unix tools use stdin and stdout as input and output, MapReduce jobs read
and write files on a distributed filesystem. In Hadoop’s implementation of Map‐
Reduce, that filesystem is called HDFS (Hadoop Distributed File System), an open
source reimplementation of the Google File System (GFS) [19].
Various other distributed filesystems besides HDFS exist, such as GlusterFS and the
Quantcast File System (QFS) [20]. Object storage services such as Amazon S3, Azure
Blob Storage, and OpenStack Swift [21] are similar in many ways.iv In this chapter we
will mostly use HDFS as a running example, but the principles apply to any dis‐
tributed filesystem.
HDFS is based on the shared-nothing principle (see the introduction to Part II), in
contrast to the shared-disk approach of Network Attached Storage (NAS) and Storage
Area Network (SAN) architectures. Shared-disk storage is implemented by a central‐
ized storage appliance, often using custom hardware and special network infrastruc‐
ture such as Fibre Channel. On the other hand, the shared-nothing approach requires
no special hardware, only computers connected by a conventional datacenter net‐
work.
HDFS consists of a daemon process running on each machine, exposing a network
service that allows other nodes to access files stored on that machine (assuming that
every general-purpose machine in a datacenter has some disks attached to it). A cen‐
tral server called the NameNode keeps track of which file blocks are stored on which
machine. Thus, HDFS conceptually creates one big filesystem that can use the space
on the disks of all machines running the daemon.
In order to tolerate machine and disk failures, file blocks are replicated on multiple
machines. Replication may mean simply several copies of the same data on multiple
machines, as in Chapter 5, or an erasure coding scheme such as Reed–Solomon codes,
which allows lost data to be recovered with lower storage overhead than full replica‐
tion [20, 22]. The techniques are similar to RAID, which provides redundancy across
several disks attached to the same machine; the difference is that in a distributed file‐
system, file access and replication are done over a conventional datacenter network
without special hardware.
398 
| 
Chapter 10: Batch Processing
