Also useful are spatial algorithms such as k-nearest neighbors [80], which searches for
items that are close to a given item in some multi-dimensional space—a kind of simi‐
larity search. Approximate search is also important for genome analysis algorithms,
which need to find strings that are similar but not identical [81].
Batch processing engines are being used for distributed execution of algorithms from
an increasingly wide range of domains. As batch processing systems gain built-in
functionality and high-level declarative operators, and as MPP databases become
more programmable and flexible, the two are beginning to look more alike: in the
end, they are all just systems for storing and processing data. 
Summary
In this chapter we explored the topic of batch processing. We started by looking at
Unix tools such as awk, grep, and sort, and we saw how the design philosophy of
those tools is carried forward into MapReduce and more recent dataflow engines.
Some of those design principles are that inputs are immutable, outputs are intended
to become the input to another (as yet unknown) program, and complex problems
are solved by composing small tools that “do one thing well.”
In the Unix world, the uniform interface that allows one program to be composed
with another is files and pipes; in MapReduce, that interface is a distributed filesys‐
tem. We saw that dataflow engines add their own pipe-like data transport mecha‐
nisms to avoid materializing intermediate state to the distributed filesystem, but the
initial input and final output of a job is still usually HDFS.
The two main problems that distributed batch processing frameworks need to solve
are:
Partitioning
In MapReduce, mappers are partitioned according to input file blocks. The out‐
put of mappers is repartitioned, sorted, and merged into a configurable number
of reducer partitions. The purpose of this process is to bring all the related data—
e.g., all the records with the same key—together in the same place.
Post-MapReduce dataflow engines try to avoid sorting unless it is required, but
they otherwise take a broadly similar approach to partitioning.
Fault tolerance
MapReduce frequently writes to disk, which makes it easy to recover from an
individual failed task without restarting the entire job but slows down execution
in the failure-free case. Dataflow engines perform less materialization of inter‐
mediate state and keep more in memory, which means that they need to recom‐
pute more data if a node fails. Deterministic operators reduce the amount of data
that needs to be recomputed.
Summary 
| 
429
