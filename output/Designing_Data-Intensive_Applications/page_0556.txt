Naturally, payment networks want to prevent fraudulent transactions, banks want to
avoid bad loans, airlines want to avoid hijackings, and companies want to avoid hir‐
ing ineffective or untrustworthy people. From their point of view, the cost of a missed
business opportunity is low, but the cost of a bad loan or a problematic employee is
much higher, so it is natural for organizations to want to be cautious. If in doubt,
they are better off saying no.
However, as algorithmic decision-making becomes more widespread, someone who
has (accurately or falsely) been labeled as risky by some algorithm may suffer a large
number of those “no” decisions. Systematically being excluded from jobs, air travel,
insurance coverage, property rental, financial services, and other key aspects of soci‐
ety is such a large constraint of the individual’s freedom that it has been called “algo‐
rithmic prison” [82]. In countries that respect human rights, the criminal justice
system presumes innocence until proven guilty; on the other hand, automated sys‐
tems can systematically and arbitrarily exclude a person from participating in society
without any proof of guilt, and with little chance of appeal.
Bias and discrimination
Decisions made by an algorithm are not necessarily any better or any worse than
those made by a human. Every person is likely to have biases, even if they actively try
to counteract them, and discriminatory practices can become culturally institutional‐
ized. There is hope that basing decisions on data, rather than subjective and instinc‐
tive assessments by people, could be more fair and give a better chance to people who
are often overlooked in the traditional system [83].
When we develop predictive analytics systems, we are not merely automating a
human’s decision by using software to specify the rules for when to say yes or no; we
are even leaving the rules themselves to be inferred from data. However, the patterns
learned by these systems are opaque: even if there is some correlation in the data, we
may not know why. If there is a systematic bias in the input to an algorithm, the sys‐
tem will most likely learn and amplify that bias in its output [84].
In many countries, anti-discrimination laws prohibit treating people differently
depending on protected traits such as ethnicity, age, gender, sexuality, disability, or
beliefs. Other features of a person’s data may be analyzed, but what happens if they
are correlated with protected traits? For example, in racially segregated neighbor‐
hoods, a person’s postal code or even their IP address is a strong predictor of race.
Put like this, it seems ridiculous to believe that an algorithm could somehow take
biased data as input and produce fair and impartial output from it [85]. Yet this belief
often seems to be implied by proponents of data-driven decision making, an attitude
that has been satirized as “machine learning is like money laundering for bias” [86].
Predictive analytics systems merely extrapolate from the past; if the past is discrimi‐
natory, they codify that discrimination. If we want the future to be better than the
534 
| 
Chapter 12: The Future of Data Systems
