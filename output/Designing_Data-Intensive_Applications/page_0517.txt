the appropriate outputs. Batch and stream processors are the tools for achieving this
goal.
The outputs of batch and stream processes are derived datasets such as search
indexes, materialized views, recommendations to show to users, aggregate metrics,
and so on (see “The Output of Batch Workflows” on page 411 and “Uses of Stream
Processing” on page 465).
As we saw in Chapter 10 and Chapter 11, batch and stream processing have a lot of
principles in common, and the main fundamental difference is that stream process‐
ors operate on unbounded datasets whereas batch process inputs are of a known,
finite size. There are also many detailed differences in the ways the processing
engines are implemented, but these distinctions are beginning to blur.
Spark performs stream processing on top of a batch processing engine by breaking
the stream into microbatches, whereas Apache Flink performs batch processing on
top of a stream processing engine [5]. In principle, one type of processing can be
emulated on top of the other, although the performance characteristics vary: for
example, microbatching may perform poorly on hopping or sliding windows [6].
Maintaining derived state
Batch processing has a quite strong functional flavor (even if the code is not written
in a functional programming language): it encourages deterministic, pure functions
whose output depends only on the input and which have no side effects other than
the explicit outputs, treating inputs as immutable and outputs as append-only.
Stream processing is similar, but it extends operators to allow managed, fault-tolerant
state (see “Rebuilding state after a failure” on page 478).
The principle of deterministic functions with well-defined inputs and outputs is not
only good for fault tolerance (see “Idempotence” on page 478), but also simplifies
reasoning about the dataflows in an organization [7]. No matter whether the derived
data is a search index, a statistical model, or a cache, it is helpful to think in terms of
data pipelines that derive one thing from another, pushing state changes in one sys‐
tem through functional application code and applying the effects to derived systems.
In principle, derived data systems could be maintained synchronously, just like a
relational database updates secondary indexes synchronously within the same trans‐
action as writes to the table being indexed. However, asynchrony is what makes sys‐
tems based on event logs robust: it allows a fault in one part of the system to be
contained locally, whereas distributed transactions abort if any one participant fails,
so they tend to amplify failures by spreading them to the rest of the system (see “Lim‐
itations of distributed transactions” on page 363).
We saw in “Partitioning and Secondary Indexes” on page 206 that secondary indexes
often cross partition boundaries. A partitioned system with secondary indexes either
Data Integration 
| 
495
