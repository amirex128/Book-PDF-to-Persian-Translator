Imagine a fictitious system with a network that guaranteed a maximum delay for
packets—every packet is either delivered within some time d, or it is lost, but delivery
never takes longer than d. Furthermore, assume that you can guarantee that a non-
failed node always handles a request within some time r. In this case, you could guar‐
antee that every successful request receives a response within time 2d + r—and if you
don’t receive a response within that time, you know that either the network or the
remote node is not working. If this was true, 2d + r would be a reasonable timeout to
use.
Unfortunately, most systems we work with have neither of those guarantees: asyn‐
chronous networks have unbounded delays (that is, they try to deliver packets as
quickly as possible, but there is no upper limit on the time it may take for a packet to
arrive), and most server implementations cannot guarantee that they can handle
requests within some maximum time (see “Response time guarantees” on page 298).
For failure detection, it’s not sufficient for the system to be fast most of the time: if
your timeout is low, it only takes a transient spike in round-trip times to throw the
system off-balance.
Network congestion and queueing
When driving a car, travel times on road networks often vary most due to traffic con‐
gestion. Similarly, the variability of packet delays on computer networks is most often
due to queueing [25]:
• If several different nodes simultaneously try to send packets to the same destina‐
tion, the network switch must queue them up and feed them into the destination
network link one by one (as illustrated in Figure 8-2). On a busy network link, a
packet may have to wait a while until it can get a slot (this is called network con‐
gestion). If there is so much incoming data that the switch queue fills up, the
packet is dropped, so it needs to be resent—even though the network is function‐
ing fine.
• When a packet reaches the destination machine, if all CPU cores are currently
busy, the incoming request from the network is queued by the operating system
until the application is ready to handle it. Depending on the load on the machine,
this may take an arbitrary length of time.
• In virtualized environments, a running operating system is often paused for tens
of milliseconds while another virtual machine uses a CPU core. During this time,
the VM cannot consume any data from the network, so the incoming data is
queued (buffered) by the virtual machine monitor [26], further increasing the
variability of network delays.
• TCP performs flow control (also known as congestion avoidance or backpressure),
in which a node limits its own rate of sending in order to avoid overloading a
282 
| 
Chapter 8: The Trouble with Distributed Systems
