to retrieve the data” than actually “making it impossible to retrieve the data.” Never‐
theless, you sometimes have to try, as we shall see in “Legislation and self-regulation”
on page 542. 
Processing Streams
So far in this chapter we have talked about where streams come from (user activity
events, sensors, and writes to databases), and we have talked about how streams are
transported (through direct messaging, via message brokers, and in event logs).
What remains is to discuss what you can do with the stream once you have it—
namely, you can process it. Broadly, there are three options:
1. You can take the data in the events and write it to a database, cache, search index,
or similar storage system, from where it can then be queried by other clients. As
shown in Figure 11-5, this is a good way of keeping a database in sync with
changes happening in other parts of the system—especially if the stream con‐
sumer is the only client writing to the database. Writing to a storage system is the
streaming equivalent of what we discussed in “The Output of Batch Workflows”
on page 411.
2. You can push the events to users in some way, for example by sending email
alerts or push notifications, or by streaming the events to a real-time dashboard
where they are visualized. In this case, a human is the ultimate consumer of the
stream.
3. You can process one or more input streams to produce one or more output
streams. Streams may go through a pipeline consisting of several such processing
stages before they eventually end up at an output (option 1 or 2).
In the rest of this chapter, we will discuss option 3: processing streams to produce
other, derived streams. A piece of code that processes streams like this is known as an
operator or a job. It is closely related to the Unix processes and MapReduce jobs we
discussed in Chapter 10, and the pattern of dataflow is similar: a stream processor
consumes input streams in a read-only fashion and writes its output to a different
location in an append-only fashion.
The patterns for partitioning and parallelization in stream processors are also very
similar to those in MapReduce and the dataflow engines we saw in Chapter 10, so we
won’t repeat those topics here. Basic mapping operations such as transforming and
filtering records also work the same.
The one crucial difference to batch jobs is that a stream never ends. This difference
has many implications: as discussed at the start of this chapter, sorting does not make
sense with an unbounded dataset, and so sort-merge joins (see “Reduce-Side Joins
and Grouping” on page 403) cannot be used. Fault-tolerance mechanisms must also
464 
| 
Chapter 11: Stream Processing
