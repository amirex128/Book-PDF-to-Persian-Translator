This approach only works if both of the join’s inputs have the same number of parti‐
tions, with records assigned to partitions based on the same key and the same hash
function. If the inputs are generated by prior MapReduce jobs that already perform
this grouping, then this can be a reasonable assumption to make.
Partitioned hash joins are known as bucketed map joins in Hive [37].
Map-side merge joins
Another variant of a map-side join applies if the input datasets are not only parti‐
tioned in the same way, but also sorted based on the same key. In this case, it does not
matter whether the inputs are small enough to fit in memory, because a mapper can
perform the same merging operation that would normally be done by a reducer:
reading both input files incrementally, in order of ascending key, and matching
records with the same key.
If a map-side merge join is possible, it probably means that prior MapReduce jobs
brought the input datasets into this partitioned and sorted form in the first place. In
principle, this join could have been performed in the reduce stage of the prior job.
However, it may still be appropriate to perform the merge join in a separate map-
only job, for example if the partitioned and sorted datasets are also needed for other
purposes besides this particular join.
MapReduce workflows with map-side joins
When the output of a MapReduce join is consumed by downstream jobs, the choice
of map-side or reduce-side join affects the structure of the output. The output of a
reduce-side join is partitioned and sorted by the join key, whereas the output of a
map-side join is partitioned and sorted in the same way as the large input (since one
map task is started for each file block of the join’s large input, regardless of whether a
partitioned or broadcast join is used).
As discussed, map-side joins also make more assumptions about the size, sorting, and
partitioning of their input datasets. Knowing about the physical layout of datasets in
the distributed filesystem becomes important when optimizing join strategies: it is
not sufficient to just know the encoding format and the name of the directory in
which the data is stored; you must also know the number of partitions and the keys
by which the data is partitioned and sorted.
In the Hadoop ecosystem, this kind of metadata about the partitioning of datasets is
often maintained in HCatalog and the Hive metastore [37]. 
410 
| 
Chapter 10: Batch Processing
