Beyond MapReduce
Although MapReduce became very popular and received a lot of hype in the late
2000s, it is just one among many possible programming models for distributed sys‐
tems. Depending on the volume of data, the structure of the data, and the type of pro‐
cessing being done with it, other tools may be more appropriate for expressing a
computation.
We nevertheless spent a lot of time in this chapter discussing MapReduce because it
is a useful learning tool, as it is a fairly clear and simple abstraction on top of a dis‐
tributed filesystem. That is, simple in the sense of being able to understand what it is
doing, not in the sense of being easy to use. Quite the opposite: implementing a com‐
plex processing job using the raw MapReduce APIs is actually quite hard and labori‐
ous—for instance, you would need to implement any join algorithms from scratch
[37].
In response to the difficulty of using MapReduce directly, various higher-level pro‐
gramming models (Pig, Hive, Cascading, Crunch) were created as abstractions on top
of MapReduce. If you understand how MapReduce works, they are fairly easy to
learn, and their higher-level constructs make many common batch processing tasks
significantly easier to implement.
However, there are also problems with the MapReduce execution model itself, which
are not fixed by adding another level of abstraction and which manifest themselves as
poor performance for some kinds of processing. On the one hand, MapReduce is
very robust: you can use it to process almost arbitrarily large quantities of data on an
unreliable multi-tenant system with frequent task terminations, and it will still get the
job done (albeit slowly). On the other hand, other tools are sometimes orders of mag‐
nitude faster for some kinds of processing.
In the rest of this chapter, we will look at some of those alternatives for batch process‐
ing. In Chapter 11 we will move to stream processing, which can be regarded as
another way of speeding up batch processing.
Materialization of Intermediate State
As discussed previously, every MapReduce job is independent from every other job.
The main contact points of a job with the rest of the world are its input and output
directories on the distributed filesystem. If you want the output of one job to become
the input to a second job, you need to configure the second job’s input directory to be
the same as the first job’s output directory, and an external workflow scheduler must
start the second job only once the first job has completed.
This setup is reasonable if the output from the first job is a dataset that you want to
publish widely within your organization. In that case, you need to be able to refer to it
Beyond MapReduce 
| 
419
