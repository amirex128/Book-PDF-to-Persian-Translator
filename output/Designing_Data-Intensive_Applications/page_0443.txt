Dataflow engines
In order to fix these problems with MapReduce, several new execution engines for
distributed batch computations were developed, the most well known of which are
Spark [61, 62], Tez [63, 64], and Flink [65, 66]. There are various differences in the
way they are designed, but they have one thing in common: they handle an entire
workflow as one job, rather than breaking it up into independent subjobs.
Since they explicitly model the flow of data through several processing stages, these
systems are known as dataflow engines. Like MapReduce, they work by repeatedly
calling a user-defined function to process one record at a time on a single thread.
They parallelize work by partitioning inputs, and they copy the output of one func‐
tion over the network to become the input to another function.
Unlike in MapReduce, these functions need not take the strict roles of alternating
map and reduce, but instead can be assembled in more flexible ways. We call these
functions operators, and the dataflow engine provides several different options for
connecting one operator’s output to another’s input:
• One option is to repartition and sort records by key, like in the shuffle stage of
MapReduce (see “Distributed execution of MapReduce” on page 400). This fea‐
ture enables sort-merge joins and grouping in the same way as in MapReduce.
• Another possibility is to take several inputs and to partition them in the same
way, but skip the sorting. This saves effort on partitioned hash joins, where the
partitioning of records is important but the order is irrelevant because building
the hash table randomizes the order anyway.
• For broadcast hash joins, the same output from one operator can be sent to all
partitions of the join operator.
This style of processing engine is based on research systems like Dryad [67] and
Nephele [68], and it offers several advantages compared to the MapReduce model:
• Expensive work such as sorting need only be performed in places where it is
actually required, rather than always happening by default between every map
and reduce stage.
• There are no unnecessary map tasks, since the work done by a mapper can often
be incorporated into the preceding reduce operator (because a mapper does not
change the partitioning of a dataset).
• Because all joins and data dependencies in a workflow are explicitly declared, the
scheduler has an overview of what data is required where, so it can make locality
optimizations. For example, it can try to place the task that consumes some data
on the same machine as the task that produces it, so that the data can be
Beyond MapReduce 
| 
421
