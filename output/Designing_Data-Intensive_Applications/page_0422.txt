Reducer
The MapReduce framework takes the key-value pairs produced by the mappers,
collects all the values belonging to the same key, and calls the reducer with an
iterator over that collection of values. The reducer can produce output records
(such as the number of occurrences of the same URL).
In the web server log example, we had a second sort command in step 5, which
ranked URLs by number of requests. In MapReduce, if you need a second sorting
stage, you can implement it by writing a second MapReduce job and using the output
of the first job as input to the second job. Viewed like this, the role of the mapper is to
prepare the data by putting it into a form that is suitable for sorting, and the role of
the reducer is to process the data that has been sorted. 
Distributed execution of MapReduce
The main difference from pipelines of Unix commands is that MapReduce can paral‐
lelize a computation across many machines, without you having to write code to
explicitly handle the parallelism. The mapper and reducer only operate on one record
at a time; they don’t need to know where their input is coming from or their output is
going to, so the framework can handle the complexities of moving data between
machines.
It is possible to use standard Unix tools as mappers and reducers in a distributed
computation [25], but more commonly they are implemented as functions in a con‐
ventional programming language. In Hadoop MapReduce, the mapper and reducer
are each a Java class that implements a particular interface. In MongoDB and
CouchDB, mappers and reducers are JavaScript functions (see “MapReduce Query‐
ing” on page 46).
Figure 10-1 shows the dataflow in a Hadoop MapReduce job. Its parallelization is
based on partitioning (see Chapter 6): the input to a job is typically a directory in
HDFS, and each file or file block within the input directory is considered to be a sepa‐
rate partition that can be processed by a separate map task (marked by m 1, m 2, and
m 3 in Figure 10-1).
Each input file is typically hundreds of megabytes in size. The MapReduce scheduler
(not shown in the diagram) tries to run each mapper on one of the machines that
stores a replica of the input file, provided that machine has enough spare RAM and
CPU resources to run the map task [26]. This principle is known as putting the com‐
putation near the data [27]: it saves copying the input file over the network, reducing
network load and increasing locality.
400 
| 
Chapter 10: Batch Processing
