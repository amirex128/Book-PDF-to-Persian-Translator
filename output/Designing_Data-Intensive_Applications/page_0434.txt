Since querying a search index by keyword is a read-only operation, these index files
are immutable once they have been created.
If the indexed set of documents changes, one option is to periodically rerun the entire
indexing workflow for the entire set of documents, and replace the previous index
files wholesale with the new index files when it is done. This approach can be compu‐
tationally expensive if only a small number of documents have changed, but it has the
advantage that the indexing process is very easy to reason about: documents in,
indexes out.
Alternatively, it is possible to build indexes incrementally. As discussed in Chapter 3,
if you want to add, remove, or update documents in an index, Lucene writes out new
segment files and asynchronously merges and compacts segment files in the back‐
ground. We will see more on such incremental processing in Chapter 11.
Key-value stores as batch process output
Search indexes are just one example of the possible outputs of a batch processing
workflow. Another common use for batch processing is to build machine learning
systems such as classifiers (e.g., spam filters, anomaly detection, image recognition)
and recommendation systems (e.g., people you may know, products you may be
interested in, or related searches [29]).
The output of those batch jobs is often some kind of database: for example, a data‐
base that can be queried by user ID to obtain suggested friends for that user, or a
database that can be queried by product ID to get a list of related products [45].
These databases need to be queried from the web application that handles user
requests, which is usually separate from the Hadoop infrastructure. So how does the
output from the batch process get back into a database where the web application can
query it?
The most obvious choice might be to use the client library for your favorite database
directly within a mapper or reducer, and to write from the batch job directly to the
database server, one record at a time. This will work (assuming your firewall rules
allow direct access from your Hadoop environment to your production databases),
but it is a bad idea for several reasons:
• As discussed previously in the context of joins, making a network request for
every single record is orders of magnitude slower than the normal throughput of
a batch task. Even if the client library supports batching, performance is likely to
be poor.
• MapReduce jobs often run many tasks in parallel. If all the mappers or reducers
concurrently write to the same output database, with a rate expected of a batch
process, that database can easily be overwhelmed, and its performance for quer‐
412 
| 
Chapter 10: Batch Processing
