Derived views allow gradual evolution. If you want to restructure a dataset, you do
not need to perform the migration as a sudden switch. Instead, you can maintain the
old schema and the new schema side by side as two independently derived views onto
the same underlying data. You can then start shifting a small number of users to the
new view in order to test its performance and find any bugs, while most users con‐
tinue to be routed to the old view. Gradually, you can increase the proportion of
users accessing the new view, and eventually you can drop the old view [10].
The beauty of such a gradual migration is that every stage of the process is easily
reversible if something goes wrong: you always have a working system to go back to.
By reducing the risk of irreversible damage, you can be more confident about going
ahead, and thus move faster to improve your system [11].
The lambda architecture
If batch processing is used to reprocess historical data, and stream processing is used
to process recent updates, then how do you combine the two? The lambda architec‐
ture [12] is a proposal in this area that has gained a lot of attention.
The core idea of the lambda architecture is that incoming data should be recorded by
appending immutable events to an always-growing dataset, similarly to event sourc‐
ing (see “Event Sourcing” on page 457). From these events, read-optimized views are
derived. The lambda architecture proposes running two different systems in parallel:
a batch processing system such as Hadoop MapReduce, and a separate stream-
processing system such as Storm.
In the lambda approach, the stream processor consumes the events and quickly pro‐
duces an approximate update to the view; the batch processor later consumes the
same set of events and produces a corrected version of the derived view. The reason‐
ing behind this design is that batch processing is simpler and thus less prone to bugs,
while stream processors are thought to be less reliable and harder to make fault-
tolerant (see “Fault Tolerance” on page 476). Moreover, the stream process can use
fast approximate algorithms while the batch process uses slower exact algorithms.
The lambda architecture was an influential idea that shaped the design of data sys‐
tems for the better, particularly by popularizing the principle of deriving views onto
streams of immutable events and reprocessing events when needed. However, I also
think that it has a number of practical problems:
• Having to maintain the same logic to run both in a batch and in a stream pro‐
cessing framework is significant additional effort. Although libraries such as
Summingbird [13] provide an abstraction for computations that can be run in
either a batch or a streaming context, the operational complexity of debugging,
tuning, and maintaining two different systems remains [14].
Data Integration 
| 
497
