cessed. This observation suggests that the connection between databases and streams
runs deeper than just the physical storage of logs on disk—it is quite fundamental.
In fact, a replication log (see “Implementation of Replication Logs” on page 158) is a
stream of database write events, produced by the leader as it processes transactions.
The followers apply that stream of writes to their own copy of the database and thus
end up with an accurate copy of the same data. The events in the replication log
describe the data changes that occurred.
We also came across the state machine replication principle in “Total Order Broad‐
cast” on page 348, which states: if every event represents a write to the database, and
every replica processes the same events in the same order, then the replicas will all
end up in the same final state. (Processing an event is assumed to be a deterministic
operation.) It’s just another case of event streams!
In this section we will first look at a problem that arises in heterogeneous data sys‐
tems, and then explore how we can solve it by bringing ideas from event streams to
databases.
Keeping Systems in Sync
As we have seen throughout this book, there is no single system that can satisfy all
data storage, querying, and processing needs. In practice, most nontrivial applica‐
tions need to combine several different technologies in order to satisfy their require‐
ments: for example, using an OLTP database to serve user requests, a cache to speed
up common requests, a full-text index to handle search queries, and a data warehouse
for analytics. Each of these has its own copy of the data, stored in its own representa‐
tion that is optimized for its own purposes.
As the same or related data appears in several different places, they need to be kept in
sync with one another: if an item is updated in the database, it also needs to be upda‐
ted in the cache, search indexes, and data warehouse. With data warehouses this syn‐
chronization is usually performed by ETL processes (see “Data Warehousing” on
page 91), often by taking a full copy of a database, transforming it, and bulk-loading
it into the data warehouse—in other words, a batch process. Similarly, we saw in
“The Output of Batch Workflows” on page 411 how search indexes, recommendation
systems, and other derived data systems might be created using batch processes.
If periodic full database dumps are too slow, an alternative that is sometimes used is
dual writes, in which the application code explicitly writes to each of the systems
when data changes: for example, first writing to the database, then updating the
search index, then invalidating the cache entries (or even performing those writes
concurrently).
However, dual writes have some serious problems, one of which is a race condition
illustrated in Figure 11-4. In this example, two clients concurrently want to update an
452 
| 
Chapter 11: Stream Processing
