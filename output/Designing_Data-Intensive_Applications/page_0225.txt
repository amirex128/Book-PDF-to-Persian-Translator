However, the downside of key range partitioning is that certain access patterns can
lead to hot spots. If the key is a timestamp, then the partitions correspond to ranges
of time—e.g., one partition per day. Unfortunately, because we write data from the
sensors to the database as the measurements happen, all the writes end up going to
the same partition (the one for today), so that partition can be overloaded with writes
while others sit idle [5].
To avoid this problem in the sensor database, you need to use something other than
the timestamp as the first element of the key. For example, you could prefix each
timestamp with the sensor name so that the partitioning is first by sensor name and
then by time. Assuming you have many sensors active at the same time, the write
load will end up more evenly spread across the partitions. Now, when you want to
fetch the values of multiple sensors within a time range, you need to perform a sepa‐
rate range query for each sensor name.
Partitioning by Hash of Key
Because of this risk of skew and hot spots, many distributed datastores use a hash
function to determine the partition for a given key.
A good hash function takes skewed data and makes it uniformly distributed. Say you
have a 32-bit hash function that takes a string. Whenever you give it a new string, it
returns a seemingly random number between 0 and 232 − 1. Even if the input strings
are very similar, their hashes are evenly distributed across that range of numbers.
For partitioning purposes, the hash function need not be cryptographically strong:
for example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler–
Noll–Vo function. Many programming languages have simple hash functions built in
(as they are used for hash tables), but they may not be suitable for partitioning: for
example, in Java’s Object.hashCode() and Ruby’s Object#hash, the same key may
have a different hash value in different processes [6].
Once you have a suitable hash function for keys, you can assign each partition a
range of hashes (rather than a range of keys), and every key whose hash falls within a
partition’s range will be stored in that partition. This is illustrated in Figure 6-3.
Partitioning of Key-Value Data 
| 
203
