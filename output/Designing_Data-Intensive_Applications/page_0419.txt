iii. Except by using a separate tool, such as netcat or curl. Unix started out trying to represent everything as
files, but the BSD sockets API deviated from that convention [17]. The research operating systems Plan 9 and
Inferno are more consistent in their use of files: they represent a TCP connection as a file in /net/tcp [18].
However, there are limits to what you can do with stdin and stdout. Programs that
need multiple inputs or outputs are possible but tricky. You can’t pipe a program’s
output into a network connection [17, 18].iii If a program directly opens files for read‐
ing and writing, or starts another program as a subprocess, or opens a network con‐
nection, then that I/O is wired up by the program itself. It can still be configurable
(through command-line options, for example), but the flexibility of wiring up inputs
and outputs in a shell is reduced.
Transparency and experimentation
Part of what makes Unix tools so successful is that they make it quite easy to see what
is going on:
• The input files to Unix commands are normally treated as immutable. This
means you can run the commands as often as you want, trying various
command-line options, without damaging the input files.
• You can end the pipeline at any point, pipe the output into less, and look at it to
see if it has the expected form. This ability to inspect is great for debugging.
• You can write the output of one pipeline stage to a file and use that file as input
to the next stage. This allows you to restart the later stage without rerunning the
entire pipeline.
Thus, even though Unix tools are quite blunt, simple tools compared to a query opti‐
mizer of a relational database, they remain amazingly useful, especially for experi‐
mentation.
However, the biggest limitation of Unix tools is that they run only on a single
machine—and that’s where tools like Hadoop come in. 
MapReduce and Distributed Filesystems
MapReduce is a bit like Unix tools, but distributed across potentially thousands of
machines. Like Unix tools, it is a fairly blunt, brute-force, but surprisingly effective
tool. A single MapReduce job is comparable to a single Unix process: it takes one or
more inputs and produces one or more outputs.
As with most Unix tools, running a MapReduce job normally does not modify the
input and does not have any side effects other than producing the output. The output
MapReduce and Distributed Filesystems 
| 
397
