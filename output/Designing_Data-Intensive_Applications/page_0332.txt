The theoretical description of an algorithm can declare that certain things are simply
assumed not to happen—and in non-Byzantine systems, we do have to make some
assumptions about faults that can and cannot happen. However, a real implementa‐
tion may still have to include code to handle the case where something happens that
was assumed to be impossible, even if that handling boils down to printf("Sucks to
be you") and exit(666)—i.e., letting a human operator clean up the mess [93].
(This is arguably the difference between computer science and software engineering.)
That is not to say that theoretical, abstract system models are worthless—quite the
opposite. They are incredibly helpful for distilling down the complexity of real sys‐
tems to a manageable set of faults that we can reason about, so that we can under‐
stand the problem and try to solve it systematically. We can prove algorithms correct
by showing that their properties always hold in some system model.
Proving an algorithm correct does not mean its implementation on a real system will
necessarily always behave correctly. But it’s a very good first step, because the theo‐
retical analysis can uncover problems in an algorithm that might remain hidden for a
long time in a real system, and that only come to bite you when your assumptions
(e.g., about timing) are defeated due to unusual circumstances. Theoretical analysis
and empirical testing are equally important. 
Summary
In this chapter we have discussed a wide range of problems that can occur in dis‐
tributed systems, including:
• Whenever you try to send a packet over the network, it may be lost or arbitrarily
delayed. Likewise, the reply may be lost or delayed, so if you don’t get a reply,
you have no idea whether the message got through.
• A node’s clock may be significantly out of sync with other nodes (despite your
best efforts to set up NTP), it may suddenly jump forward or back in time, and
relying on it is dangerous because you most likely don’t have a good measure of
your clock’s error interval.
• A process may pause for a substantial amount of time at any point in its execu‐
tion (perhaps due to a stop-the-world garbage collector), be declared dead by
other nodes, and then come back to life again without realizing that it was
paused.
The fact that such partial failures can occur is the defining characteristic of dis‐
tributed systems. Whenever software tries to do anything involving other nodes,
there is the possibility that it may occasionally fail, or randomly go slow, or not
respond at all (and eventually time out). In distributed systems, we try to build toler‐
310 
| 
Chapter 8: The Trouble with Distributed Systems
