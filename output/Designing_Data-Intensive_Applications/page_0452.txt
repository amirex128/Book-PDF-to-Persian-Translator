We discussed several join algorithms for MapReduce, most of which are also inter‐
nally used in MPP databases and dataflow engines. They also provide a good illustra‐
tion of how partitioned algorithms work:
Sort-merge joins
Each of the inputs being joined goes through a mapper that extracts the join key.
By partitioning, sorting, and merging, all the records with the same key end up
going to the same call of the reducer. This function can then output the joined
records.
Broadcast hash joins
One of the two join inputs is small, so it is not partitioned and it can be entirely
loaded into a hash table. Thus, you can start a mapper for each partition of the
large join input, load the hash table for the small input into each mapper, and
then scan over the large input one record at a time, querying the hash table for
each record.
Partitioned hash joins
If the two join inputs are partitioned in the same way (using the same key, same
hash function, and same number of partitions), then the hash table approach can
be used independently for each partition.
Distributed batch processing engines have a deliberately restricted programming
model: callback functions (such as mappers and reducers) are assumed to be stateless
and to have no externally visible side effects besides their designated output. This
restriction allows the framework to hide some of the hard distributed systems prob‐
lems behind its abstraction: in the face of crashes and network issues, tasks can be
retried safely, and the output from any failed tasks is discarded. If several tasks for a
partition succeed, only one of them actually makes its output visible.
Thanks to the framework, your code in a batch processing job does not need to worry
about implementing fault-tolerance mechanisms: the framework can guarantee that
the final output of a job is the same as if no faults had occurred, even though in real‐
ity various tasks perhaps had to be retried. These reliable semantics are much stron‐
ger than what you usually have in online services that handle user requests and that
write to databases as a side effect of processing a request.
The distinguishing feature of a batch processing job is that it reads some input data
and produces some output data, without modifying the input—in other words, the
output is derived from the input. Crucially, the input data is bounded: it has a known,
fixed size (for example, it consists of a set of log files at some point in time, or a snap‐
shot of a database’s contents). Because it is bounded, a job knows when it has finished
reading the entire input, and so a job eventually completes when it is done.
In the next chapter, we will turn to stream processing, in which the input is unboun‐
ded—that is, you still have a job, but its inputs are never-ending streams of data. In
430 
| 
Chapter 10: Batch Processing
