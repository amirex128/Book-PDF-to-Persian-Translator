As discussed previously, higher-level languages and APIs such as Hive, Pig, Cascad‐
ing, and Crunch became popular because programming MapReduce jobs by hand is
quite laborious. As Tez emerged, these high-level languages had the additional bene‐
fit of being able to move to the new dataflow execution engine without the need to
rewrite job code. Spark and Flink also include their own high-level dataflow APIs,
often taking inspiration from FlumeJava [34].
These dataflow APIs generally use relational-style building blocks to express a com‐
putation: joining datasets on the value of some field; grouping tuples by key; filtering
by some condition; and aggregating tuples by counting, summing, or other functions.
Internally, these operations are implemented using the various join and grouping
algorithms that we discussed earlier in this chapter.
Besides the obvious advantage of requiring less code, these high-level interfaces also
allow interactive use, in which you write analysis code incrementally in a shell and
run it frequently to observe what it is doing. This style of development is very helpful
when exploring a dataset and experimenting with approaches for processing it. It is
also reminiscent of the Unix philosophy, which we discussed in “The Unix Philoso‐
phy” on page 394.
Moreover, these high-level interfaces not only make the humans using the system
more productive, but they also improve the job execution efficiency at a machine
level.
The move toward declarative query languages
An advantage of specifying joins as relational operators, compared to spelling out the
code that performs the join, is that the framework can analyze the properties of the
join inputs and automatically decide which of the aforementioned join algorithms
would be most suitable for the task at hand. Hive, Spark, and Flink have cost-based
query optimizers that can do this, and even change the order of joins so that the
amount of intermediate state is minimized [66, 77, 78, 79].
The choice of join algorithm can make a big difference to the performance of a batch
job, and it is nice not to have to understand and remember all the various join algo‐
rithms we discussed in this chapter. This is possible if joins are specified in a declara‐
tive way: the application simply states which joins are required, and the query
optimizer decides how they can best be executed. We previously came across this idea
in “Query Languages for Data” on page 42.
However, in other ways, MapReduce and its dataflow successors are very different
from the fully declarative query model of SQL. MapReduce was built around the idea
of function callbacks: for each record or group of records, a user-defined function
(the mapper or reducer) is called, and that function is free to call arbitrary code in
order to decide what to output. This approach has the advantage that you can draw
Beyond MapReduce 
| 
427
