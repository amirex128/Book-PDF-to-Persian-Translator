Log-based message broker
The broker assigns all messages in a partition to the same consumer node, and
always delivers messages in the same order. Parallelism is achieved through par‐
titioning, and consumers track their progress by checkpointing the offset of the
last message they have processed. The broker retains messages on disk, so it is
possible to jump back and reread old messages if necessary.
The log-based approach has similarities to the replication logs found in databases
(see Chapter 5) and log-structured storage engines (see Chapter 3). We saw that this
approach is especially appropriate for stream processing systems that consume input
streams and generate derived state or derived output streams.
In terms of where streams come from, we discussed several possibilities: user activity
events, sensors providing periodic readings, and data feeds (e.g., market data in
finance) are naturally represented as streams. We saw that it can also be useful to
think of the writes to a database as a stream: we can capture the changelog—i.e., the
history of all changes made to a database—either implicitly through change data cap‐
ture or explicitly through event sourcing. Log compaction allows the stream to retain
a full copy of the contents of a database.
Representing databases as streams opens up powerful opportunities for integrating
systems. You can keep derived data systems such as search indexes, caches, and ana‐
lytics systems continually up to date by consuming the log of changes and applying
them to the derived system. You can even build fresh views onto existing data by
starting from scratch and consuming the log of changes from the beginning all the
way to the present.
The facilities for maintaining state as streams and replaying messages are also the
basis for the techniques that enable stream joins and fault tolerance in various stream
processing frameworks. We discussed several purposes of stream processing, includ‐
ing searching for event patterns (complex event processing), computing windowed
aggregations (stream analytics), and keeping derived data systems up to date (materi‐
alized views).
We then discussed the difficulties of reasoning about time in a stream processor,
including the distinction between processing time and event timestamps, and the
problem of dealing with straggler events that arrive after you thought your window
was complete.
We distinguished three types of joins that may appear in stream processes:
Stream-stream joins
Both input streams consist of activity events, and the join operator searches for
related events that occur within some window of time. For example, it may
match two actions taken by the same user within 30 minutes of each other. The
480 
| 
Chapter 11: Stream Processing
