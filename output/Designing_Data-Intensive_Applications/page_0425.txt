v. The joins we talk about in this book are generally equi-joins, the most common type of join, in which a
record is associated with other records that have an identical value in a particular field (such as an ID). Some
databases support more general types of joins, for example using a less-than operator instead of an equality
operator, but we do not have space to cover them here.
executions, various workflow schedulers for Hadoop have been developed, including
Oozie, Azkaban, Luigi, Airflow, and Pinball [28].
These schedulers also have management features that are useful when maintaining a
large collection of batch jobs. Workflows consisting of 50 to 100 MapReduce jobs are
common when building recommendation systems [29], and in a large organization,
many different teams may be running different jobs that read each other’s output.
Tool support is important for managing such complex dataflows.
Various higher-level tools for Hadoop, such as Pig [30], Hive [31], Cascading [32],
Crunch [33], and FlumeJava [34], also set up workflows of multiple MapReduce
stages that are automatically wired together appropriately. 
Reduce-Side Joins and Grouping
We discussed joins in Chapter 2 in the context of data models and query languages,
but we have not delved into how joins are actually implemented. It is time that we
pick up that thread again.
In many datasets it is common for one record to have an association with another
record: a foreign key in a relational model, a document reference in a document
model, or an edge in a graph model. A join is necessary whenever you have some
code that needs to access records on both sides of that association (both the record
that holds the reference and the record being referenced). As discussed in Chapter 2,
denormalization can reduce the need for joins but generally not remove it entirely.v
In a database, if you execute a query that involves only a small number of records, the
database will typically use an index to quickly locate the records of interest (see Chap‐
ter 3). If the query involves joins, it may require multiple index lookups. However,
MapReduce has no concept of indexes—at least not in the usual sense.
When a MapReduce job is given a set of files as input, it reads the entire content of all
of those files; a database would call this operation a full table scan. If you only want to
read a small number of records, a full table scan is outrageously expensive compared
to an index lookup. However, in analytic queries (see “Transaction Processing or
Analytics?” on page 90) it is common to want to calculate aggregates over a large
number of records. In this case, scanning the entire input might be quite a reasonable
thing to do, especially if you can parallelize the processing across multiple machines.
MapReduce and Distributed Filesystems 
| 
403
