form, it was feasible to implement a whole range of approaches, which would not
have been possible within the confines of a monolithic MPP database [58].
Crucially, those various processing models can all be run on a single shared-use clus‐
ter of machines, all accessing the same files on the distributed filesystem. In the
Hadoop approach, there is no need to import the data into several different special‐
ized systems for different kinds of processing: the system is flexible enough to sup‐
port a diverse set of workloads within the same cluster. Not having to move data
around makes it a lot easier to derive value from the data, and a lot easier to experi‐
ment with new processing models.
The Hadoop ecosystem includes both random-access OLTP databases such as HBase
(see “SSTables and LSM-Trees” on page 76) and MPP-style analytic databases such as
Impala [41]. Neither HBase nor Impala uses MapReduce, but both use HDFS for
storage. They are very different approaches to accessing and processing data, but they
can nevertheless coexist and be integrated in the same system.
Designing for frequent faults
When comparing MapReduce to MPP databases, two more differences in design
approach stand out: the handling of faults and the use of memory and disk. Batch
processes are less sensitive to faults than online systems, because they do not immedi‐
ately affect users if they fail and they can always be run again.
If a node crashes while a query is executing, most MPP databases abort the entire
query, and either let the user resubmit the query or automatically run it again [3]. As
queries normally run for a few seconds or a few minutes at most, this way of handling
errors is acceptable, since the cost of retrying is not too great. MPP databases also
prefer to keep as much data as possible in memory (e.g., using hash joins) to avoid
the cost of reading from disk.
On the other hand, MapReduce can tolerate the failure of a map or reduce task
without it affecting the job as a whole by retrying work at the granularity of an indi‐
vidual task. It is also very eager to write data to disk, partly for fault tolerance, and
partly on the assumption that the dataset will be too big to fit in memory anyway.
The MapReduce approach is more appropriate for larger jobs: jobs that process so
much data and run for such a long time that they are likely to experience at least one
task failure along the way. In that case, rerunning the entire job due to a single task
failure would be wasteful. Even if recovery at the granularity of an individual task
introduces overheads that make fault-free processing slower, it can still be a reason‐
able trade-off if the rate of task failures is high enough.
But how realistic are these assumptions? In most clusters, machine failures do occur,
but they are not very frequent—probably rare enough that most jobs will not experi‐
MapReduce and Distributed Filesystems 
| 
417
