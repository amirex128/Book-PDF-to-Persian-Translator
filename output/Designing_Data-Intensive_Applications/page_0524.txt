querying requires mapping one data model into another, which takes some thought
but is ultimately quite a manageable problem. I think that keeping the writes to sev‐
eral storage systems in sync is the harder engineering problem, and so I will focus
on it.
The traditional approach to synchronizing writes requires distributed transactions
across heterogeneous storage systems [18], which I think is the wrong solution (see
“Derived data versus distributed transactions” on page 492). Transactions within a
single storage or stream processing system are feasible, but when data crosses the
boundary between different technologies, I believe that an asynchronous event log
with idempotent writes is a much more robust and practical approach.
For example, distributed transactions are used within some stream processors to ach‐
ieve exactly-once semantics (see “Atomic commit revisited” on page 477), and this
can work quite well. However, when a transaction would need to involve systems
written by different groups of people (e.g., when data is written from a stream pro‐
cessor to a distributed key-value store or search index), the lack of a standardized
transaction protocol makes integration much harder. An ordered log of events with
idempotent consumers (see “Idempotence” on page 478) is a much simpler abstrac‐
tion, and thus much more feasible to implement across heterogeneous systems [7].
The big advantage of log-based integration is loose coupling between the various com‐
ponents, which manifests itself in two ways:
1. At a system level, asynchronous event streams make the system as a whole more
robust to outages or performance degradation of individual components. If a
consumer runs slow or fails, the event log can buffer messages (see “Disk space
usage” on page 450), allowing the producer and any other consumers to continue
running unaffected. The faulty consumer can catch up when it is fixed, so it
doesn’t miss any data, and the fault is contained. By contrast, the synchronous
interaction of distributed transactions tends to escalate local faults into large-
scale failures (see “Limitations of distributed transactions” on page 363).
2. At a human level, unbundling data systems allows different software components
and services to be developed, improved, and maintained independently from
each other by different teams. Specialization allows each team to focus on doing
one thing well, with well-defined interfaces to other teams’ systems. Event logs
provide an interface that is powerful enough to capture fairly strong consistency
properties (due to durability and ordering of events), but also general enough to
be applicable to almost any kind of data.
Unbundled versus integrated systems
If unbundling does indeed become the way of the future, it will not replace databases
in their current form—they will still be needed as much as ever. Databases are still
502 
| 
Chapter 12: The Future of Data Systems
