This fault tolerance is achieved by periodically checkpointing the state of all vertices
at the end of an iteration—i.e., writing their full state to durable storage. If a node
fails and its in-memory state is lost, the simplest solution is to roll back the entire
graph computation to the last checkpoint and restart the computation. If the algo‐
rithm is deterministic and messages are logged, it is also possible to selectively
recover only the partition that was lost (like we previously discussed for dataflow
engines) [72].
Parallel execution
A vertex does not need to know on which physical machine it is executing; when it
sends messages to other vertices, it simply sends them to a vertex ID. It is up to the
framework to partition the graph—i.e., to decide which vertex runs on which
machine, and how to route messages over the network so that they end up in the
right place.
Because the programming model deals with just one vertex at a time (sometimes
called “thinking like a vertex”), the framework may partition the graph in arbitrary
ways. Ideally it would be partitioned such that vertices are colocated on the same
machine if they need to communicate a lot. However, finding such an optimized par‐
titioning is hard—in practice, the graph is often simply partitioned by an arbitrarily
assigned vertex ID, making no attempt to group related vertices together.
As a result, graph algorithms often have a lot of cross-machine communication over‐
head, and the intermediate state (messages sent between nodes) is often bigger than
the original graph. The overhead of sending messages over the network can signifi‐
cantly slow down distributed graph algorithms.
For this reason, if your graph can fit in memory on a single computer, it’s quite likely
that a single-machine (maybe even single-threaded) algorithm will outperform a dis‐
tributed batch process [73, 74]. Even if the graph is bigger than memory, it can fit on
the disks of a single computer, single-machine processing using a framework such as
GraphChi is a viable option [75]. If the graph is too big to fit on a single machine, a
distributed approach such as Pregel is unavoidable; efficiently parallelizing graph
algorithms is an area of ongoing research [76]. 
High-Level APIs and Languages
Over the years since MapReduce first became popular, the execution engines for dis‐
tributed batch processing have matured. By now, the infrastructure has become
robust enough to store and process many petabytes of data on clusters of over 10,000
machines. As the problem of physically operating batch processes at such scale has
been considered more or less solved, attention has turned to other areas: improving
the programming model, improving the efficiency of processing, and broadening the
set of problems that these technologies can solve.
426 
| 
Chapter 10: Batch Processing
