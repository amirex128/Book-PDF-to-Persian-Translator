iii. Less facetiously, the set of distinct search queries with nonempty search results is finite, assuming a finite
corpus. However, it would be exponential in the number of terms in the corpus, which is still pretty bad news.
request you read from the derived dataset, perhaps perform some more processing
on the results, and construct the response to the user.
Taken together, the write path and the read path encompass the whole journey of the
data, from the point where it is collected to the point where it is consumed (probably
by another human). The write path is the portion of the journey that is precomputed
—i.e., that is done eagerly as soon as the data comes in, regardless of whether anyone
has asked to see it. The read path is the portion of the journey that only happens
when someone asks for it. If you are familiar with functional programming lan‐
guages, you might notice that the write path is similar to eager evaluation, and the
read path is similar to lazy evaluation.
The derived dataset is the place where the write path and the read path meet, as illus‐
trated in Figure 12-1. It represents a trade-off between the amount of work that needs
to be done at write time and the amount that needs to be done at read time.
Materialized views and caching
A full-text search index is a good example: the write path updates the index, and the
read path searches the index for keywords. Both reads and writes need to do some
work. Writes need to update the index entries for all terms that appear in the docu‐
ment. Reads need to search for each of the words in the query, and apply Boolean
logic to find documents that contain all of the words in the query (an AND operator),
or any synonym of each of the words (an OR operator).
If you didn’t have an index, a search query would have to scan over all documents
(like grep), which would get very expensive if you had a large number of documents.
No index means less work on the write path (no index to update), but a lot more
work on the read path.
On the other hand, you could imagine precomputing the search results for all possi‐
ble queries. In that case, you would have less work to do on the read path: no Boolean
logic, just find the results for your query and return them. However, the write path
would be a lot more expensive: the set of possible search queries that could be asked
is infinite, and thus precomputing all possible search results would require infinite
time and storage space. That wouldn’t work so well.iii
Another option would be to precompute the search results for only a fixed set of the
most common queries, so that they can be served quickly without having to go to the
index. The uncommon queries can still be served from the index. This would gener‐
ally be called a cache of common queries, although we could also call it a materialized
510 
| 
Chapter 12: The Future of Data Systems
