will become faulty, and the software will have to somehow handle it. The fault han‐
dling must be part of the software design, and you (as operator of the software) need
to know what behavior to expect from the software in the case of a fault.
It would be unwise to assume that faults are rare and simply hope for the best. It is
important to consider a wide range of possible faults—even fairly unlikely ones—and
to artificially create such situations in your testing environment to see what happens.
In distributed systems, suspicion, pessimism, and paranoia pay off.
Building a Reliable System from Unreliable Components
You may wonder whether this makes any sense—intuitively it may seem like a system
can only be as reliable as its least reliable component (its weakest link). This is not the
case: in fact, it is an old idea in computing to construct a more reliable system from a
less reliable underlying base [11]. For example:
• Error-correcting codes allow digital data to be transmitted accurately across a
communication channel that occasionally gets some bits wrong, for example due
to radio interference on a wireless network [12].
• IP (the Internet Protocol) is unreliable: it may drop, delay, duplicate, or reorder
packets. TCP (the Transmission Control Protocol) provides a more reliable
transport layer on top of IP: it ensures that missing packets are retransmitted,
duplicates are eliminated, and packets are reassembled into the order in which
they were sent.
Although the system can be more reliable than its underlying parts, there is always a
limit to how much more reliable it can be. For example, error-correcting codes can
deal with a small number of single-bit errors, but if your signal is swamped by inter‐
ference, there is a fundamental limit to how much data you can get through your
communication channel [13]. TCP can hide packet loss, duplication, and reordering
from you, but it cannot magically remove delays in the network.
Although the more reliable higher-level system is not perfect, it’s still useful because it
takes care of some of the tricky low-level faults, and so the remaining faults are usu‐
ally easier to reason about and deal with. We will explore this matter further in “The
end-to-end argument” on page 519. 
Unreliable Networks
As discussed in the introduction to Part II, the distributed systems we focus on in this
book are shared-nothing systems: i.e., a bunch of machines connected by a network.
The network is the only way those machines can communicate—we assume that each
Unreliable Networks 
| 
277
