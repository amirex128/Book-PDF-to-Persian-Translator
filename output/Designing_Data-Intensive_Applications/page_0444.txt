exchanged through a shared memory buffer rather than having to copy it over
the network.
• It is usually sufficient for intermediate state between operators to be kept in
memory or written to local disk, which requires less I/O than writing it to HDFS
(where it must be replicated to several machines and written to disk on each rep‐
lica). MapReduce already uses this optimization for mapper output, but dataflow
engines generalize the idea to all intermediate state.
• Operators can start executing as soon as their input is ready; there is no need to
wait for the entire preceding stage to finish before the next one starts.
• Existing Java Virtual Machine (JVM) processes can be reused to run new opera‐
tors, reducing startup overheads compared to MapReduce (which launches a
new JVM for each task).
You can use dataflow engines to implement the same computations as MapReduce
workflows, and they usually execute significantly faster due to the optimizations
described here. Since operators are a generalization of map and reduce, the same pro‐
cessing code can run on either execution engine: workflows implemented in Pig,
Hive, or Cascading can be switched from MapReduce to Tez or Spark with a simple
configuration change, without modifying code [64].
Tez is a fairly thin library that relies on the YARN shuffle service for the actual copy‐
ing of data between nodes [58], whereas Spark and Flink are big frameworks that
include their own network communication layer, scheduler, and user-facing APIs.
We will discuss those high-level APIs shortly.
Fault tolerance
An advantage of fully materializing intermediate state to a distributed filesystem is
that it is durable, which makes fault tolerance fairly easy in MapReduce: if a task fails,
it can just be restarted on another machine and read the same input again from the
filesystem.
Spark, Flink, and Tez avoid writing intermediate state to HDFS, so they take a differ‐
ent approach to tolerating faults: if a machine fails and the intermediate state on that
machine is lost, it is recomputed from other data that is still available (a prior inter‐
mediary stage if possible, or otherwise the original input data, which is normally on
HDFS).
To enable this recomputation, the framework must keep track of how a given piece of
data was computed—which input partitions it used, and which operators were
applied to it. Spark uses the resilient distributed dataset (RDD) abstraction for track‐
ing the ancestry of data [61], while Flink checkpoints operator state, allowing it to
resume running an operator that ran into a fault during its execution [66].
422 
| 
Chapter 10: Batch Processing
