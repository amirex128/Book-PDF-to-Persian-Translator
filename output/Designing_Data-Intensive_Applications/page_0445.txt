When recomputing data, it is important to know whether the computation is deter‐
ministic: that is, given the same input data, do the operators always produce the same
output? This question matters if some of the lost data has already been sent to down‐
stream operators. If the operator is restarted and the recomputed data is not the same
as the original lost data, it becomes very hard for downstream operators to resolve the
contradictions between the old and new data. The solution in the case of nondeter‐
ministic operators is normally to kill the downstream operators as well, and run them
again on the new data.
In order to avoid such cascading faults, it is better to make operators deterministic.
Note however that it is easy for nondeterministic behavior to accidentally creep in:
for example, many programming languages do not guarantee any particular order
when iterating over elements of a hash table, many probabilistic and statistical
algorithms explicitly rely on using random numbers, and any use of the system clock
or external data sources is nondeterministic. Such causes of nondeterminism need to
be removed in order to reliably recover from faults, for example by generating
pseudorandom numbers using a fixed seed.
Recovering from faults by recomputing data is not always the right answer: if the
intermediate data is much smaller than the source data, or if the computation is very
CPU-intensive, it is probably cheaper to materialize the intermediate data to files
than to recompute it.
Discussion of materialization
Returning to the Unix analogy, we saw that MapReduce is like writing the output of
each command to a temporary file, whereas dataflow engines look much more like
Unix pipes. Flink especially is built around the idea of pipelined execution: that is,
incrementally passing the output of an operator to other operators, and not waiting
for the input to be complete before starting to process it.
A sorting operation inevitably needs to consume its entire input before it can pro‐
duce any output, because it’s possible that the very last input record is the one with
the lowest key and thus needs to be the very first output record. Any operator that
requires sorting will thus need to accumulate state, at least temporarily. But many
other parts of a workflow can be executed in a pipelined manner.
When the job completes, its output needs to go somewhere durable so that users can
find it and use it—most likely, it is written to the distributed filesystem again. Thus,
when using a dataflow engine, materialized datasets on HDFS are still usually the
inputs and the final outputs of a job. Like with MapReduce, the inputs are immutable
and the output is completely replaced. The improvement over MapReduce is that you
save yourself writing all the intermediate state to the filesystem as well. 
Beyond MapReduce 
| 
423
