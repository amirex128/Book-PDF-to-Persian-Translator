Failure detection
Clients maintain a long-lived session on ZooKeeper servers, and the client and
server periodically exchange heartbeats to check that the other node is still alive.
Even if the connection is temporarily interrupted, or a ZooKeeper node fails, the
session remains active. However, if the heartbeats cease for a duration that is
longer than the session timeout, ZooKeeper declares the session to be dead. Any
locks held by a session can be configured to be automatically released when the
session times out (ZooKeeper calls these ephemeral nodes).
Change notifications
Not only can one client read locks and values that were created by another client,
but it can also watch them for changes. Thus, a client can find out when another
client joins the cluster (based on the value it writes to ZooKeeper), or if another
client fails (because its session times out and its ephemeral nodes disappear). By
subscribing to notifications, a client avoids having to frequently poll to find out
about changes.
Of these features, only the linearizable atomic operations really require consensus.
However, it is the combination of these features that makes systems like ZooKeeper
so useful for distributed coordination.
Allocating work to nodes
One example in which the ZooKeeper/Chubby model works well is if you have sev‐
eral instances of a process or service, and one of them needs to be chosen as leader or
primary. If the leader fails, one of the other nodes should take over. This is of course
useful for single-leader databases, but it’s also useful for job schedulers and similar
stateful systems.
Another example arises when you have some partitioned resource (database, message
streams, file storage, distributed actor system, etc.) and need to decide which parti‐
tion to assign to which node. As new nodes join the cluster, some of the partitions
need to be moved from existing nodes to the new nodes in order to rebalance the
load (see “Rebalancing Partitions” on page 209). As nodes are removed or fail, other
nodes need to take over the failed nodes’ work.
These kinds of tasks can be achieved by judicious use of atomic operations, ephem‐
eral nodes, and notifications in ZooKeeper. If done correctly, this approach allows
the application to automatically recover from faults without human intervention. It’s
not easy, despite the appearance of libraries such as Apache Curator [17] that have
sprung up to provide higher-level tools on top of the ZooKeeper client API—but it is
still much better than attempting to implement the necessary consensus algorithms
from scratch, which has a poor success record [107].
Distributed Transactions and Consensus 
| 
371
