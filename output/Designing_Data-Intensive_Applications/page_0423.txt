Figure 10-1. A MapReduce job with three mappers and three reducers.
In most cases, the application code that should run in the map task is not yet present
on the machine that is assigned the task of running it, so the MapReduce framework
first copies the code (e.g., JAR files in the case of a Java program) to the appropriate
machines. It then starts the map task and begins reading the input file, passing one
record at a time to the mapper callback. The output of the mapper consists of key-
value pairs.
The reduce side of the computation is also partitioned. While the number of map
tasks is determined by the number of input file blocks, the number of reduce tasks is
configured by the job author (it can be different from the number of map tasks). To
ensure that all key-value pairs with the same key end up at the same reducer, the
framework uses a hash of the key to determine which reduce task should receive a
particular key-value pair (see “Partitioning by Hash of Key” on page 203).
The key-value pairs must be sorted, but the dataset is likely too large to be sorted with
a conventional sorting algorithm on a single machine. Instead, the sorting is per‐
formed in stages. First, each map task partitions its output by reducer, based on the
hash of the key. Each of these partitions is written to a sorted file on the mapper’s
local disk, using a technique similar to what we discussed in “SSTables and LSM-
Trees” on page 76.
MapReduce and Distributed Filesystems 
| 
401
