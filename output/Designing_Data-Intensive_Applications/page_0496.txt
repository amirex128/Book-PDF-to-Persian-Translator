stream of activity events containing a user ID, and the output is a stream of activity
events in which the user ID has been augmented with profile information about the
user. This process is sometimes known as enriching the activity events with informa‐
tion from the database.
To perform this join, the stream process needs to look at one activity event at a time,
look up the event’s user ID in the database, and add the profile information to the
activity event. The database lookup could be implemented by querying a remote
database; however, as discussed in “Example: analysis of user activity events” on page
404, such remote queries are likely to be slow and risk overloading the database [75].
Another approach is to load a copy of the database into the stream processor so that
it can be queried locally without a network round-trip. This technique is very similar
to the hash joins we discussed in “Map-Side Joins” on page 408: the local copy of the
database might be an in-memory hash table if it is small enough, or an index on the
local disk.
The difference to batch jobs is that a batch job uses a point-in-time snapshot of the
database as input, whereas a stream processor is long-running, and the contents of
the database are likely to change over time, so the stream processor’s local copy of the
database needs to be kept up to date. This issue can be solved by change data capture:
the stream processor can subscribe to a changelog of the user profile database as well
as the stream of activity events. When a profile is created or modified, the stream
processor updates its local copy. Thus, we obtain a join between two streams: the
activity events and the profile updates.
A stream-table join is actually very similar to a stream-stream join; the biggest differ‐
ence is that for the table changelog stream, the join uses a window that reaches back
to the “beginning of time” (a conceptually infinite window), with newer versions of
records overwriting older ones. For the stream input, the join might not maintain a
window at all.
Table-table join (materialized view maintenance)
Consider the Twitter timeline example that we discussed in “Describing Load” on
page 11. We said that when a user wants to view their home timeline, it is too expen‐
sive to iterate over all the people the user is following, find their recent tweets, and
merge them.
Instead, we want a timeline cache: a kind of per-user “inbox” to which tweets are
written as they are sent, so that reading the timeline is a single lookup. Materializing
and maintaining this cache requires the following event processing:
• When user u sends a new tweet, it is added to the timeline of every user who is
following u.
474 
| 
Chapter 11: Stream Processing
