by name and reuse it as input to several different jobs (including jobs developed by
other teams). Publishing data to a well-known location in the distributed filesystem
allows loose coupling so that jobs don’t need to know who is producing their input or
consuming their output (see “Separation of logic and wiring” on page 396).
However, in many cases, you know that the output of one job is only ever used as
input to one other job, which is maintained by the same team. In this case, the files
on the distributed filesystem are simply intermediate state: a means of passing data
from one job to the next. In the complex workflows used to build recommendation
systems consisting of 50 or 100 MapReduce jobs [29], there is a lot of such intermedi‐
ate state.
The process of writing out this intermediate state to files is called materialization.
(We came across the term previously in the context of materialized views, in “Aggre‐
gation: Data Cubes and Materialized Views” on page 101. It means to eagerly com‐
pute the result of some operation and write it out, rather than computing it on
demand when requested.)
By contrast, the log analysis example at the beginning of the chapter used Unix pipes
to connect the output of one command with the input of another. Pipes do not fully
materialize the intermediate state, but instead stream the output to the input incre‐
mentally, using only a small in-memory buffer.
MapReduce’s approach of fully materializing intermediate state has downsides com‐
pared to Unix pipes:
• A MapReduce job can only start when all tasks in the preceding jobs (that gener‐
ate its inputs) have completed, whereas processes connected by a Unix pipe are
started at the same time, with output being consumed as soon as it is produced.
Skew or varying load on different machines means that a job often has a few
straggler tasks that take much longer to complete than the others. Having to wait
until all of the preceding job’s tasks have completed slows down the execution of
the workflow as a whole.
• Mappers are often redundant: they just read back the same file that was just writ‐
ten by a reducer, and prepare it for the next stage of partitioning and sorting. In
many cases, the mapper code could be part of the previous reducer: if the reducer
output was partitioned and sorted in the same way as mapper output, then
reducers could be chained together directly, without interleaving with mapper
stages.
• Storing intermediate state in a distributed filesystem means those files are repli‐
cated across several nodes, which is often overkill for such temporary data.
420 
| 
Chapter 10: Batch Processing
