The Output of Batch Workflows
We have talked a lot about the various algorithms for implementing workflows of
MapReduce jobs, but we neglected an important question: what is the result of all of
that processing, once it is done? Why are we running all these jobs in the first place?
In the case of database queries, we distinguished transaction processing (OLTP) pur‐
poses from analytic purposes (see “Transaction Processing or Analytics?” on page
90). We saw that OLTP queries generally look up a small number of records by key,
using indexes, in order to present them to a user (for example, on a web page). On
the other hand, analytic queries often scan over a large number of records, perform‐
ing groupings and aggregations, and the output often has the form of a report: a
graph showing the change in a metric over time, or the top 10 items according to
some ranking, or a breakdown of some quantity into subcategories. The consumer of
such a report is often an analyst or a manager who needs to make business decisions.
Where does batch processing fit in? It is not transaction processing, nor is it analyt‐
ics. It is closer to analytics, in that a batch process typically scans over large portions
of an input dataset. However, a workflow of MapReduce jobs is not the same as a
SQL query used for analytic purposes (see “Comparing Hadoop to Distributed Data‐
bases” on page 414). The output of a batch process is often not a report, but some
other kind of structure.
Building search indexes
Google’s original use of MapReduce was to build indexes for its search engine, which
was implemented as a workflow of 5 to 10 MapReduce jobs [1]. Although Google
later moved away from using MapReduce for this purpose [43], it helps to under‐
stand MapReduce if you look at it through the lens of building a search index. (Even
today, Hadoop MapReduce remains a good way of building indexes for Lucene/Solr
[44].)
We saw briefly in “Full-text search and fuzzy indexes” on page 88 how a full-text
search index such as Lucene works: it is a file (the term dictionary) in which you can
efficiently look up a particular keyword and find the list of all the document IDs con‐
taining that keyword (the postings list). This is a very simplified view of a search
index—in reality it requires various additional data, in order to rank search results by
relevance, correct misspellings, resolve synonyms, and so on—but the principle
holds.
If you need to perform a full-text search over a fixed set of documents, then a batch
process is a very effective way of building the indexes: the mappers partition the set of
documents as needed, each reducer builds the index for its partition, and the index
files are written to the distributed filesystem. Building such document-partitioned
indexes (see “Partitioning and Secondary Indexes” on page 206) parallelizes very well.
MapReduce and Distributed Filesystems 
| 
411
