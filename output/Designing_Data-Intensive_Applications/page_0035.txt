iii. In an ideal world, the running time of a batch job is the size of the dataset divided by the throughput. In
practice, the running time is often longer, due to skew (data not being spread evenly across worker processes)
and needing to wait for the slowest task to complete.
have over 30 million followers. This means that a single tweet may result in over 30
million writes to home timelines! Doing this in a timely manner—Twitter tries to
deliver tweets to followers within five seconds—is a significant challenge.
In the example of Twitter, the distribution of followers per user (maybe weighted by
how often those users tweet) is a key load parameter for discussing scalability, since it
determines the fan-out load. Your application may have very different characteristics,
but you can apply similar principles to reasoning about its load.
The final twist of the Twitter anecdote: now that approach 2 is robustly implemented,
Twitter is moving to a hybrid of both approaches. Most users’ tweets continue to be
fanned out to home timelines at the time when they are posted, but a small number
of users with a very large number of followers (i.e., celebrities) are excepted from this
fan-out. Tweets from any celebrities that a user may follow are fetched separately and
merged with that user’s home timeline when it is read, like in approach 1. This hybrid
approach is able to deliver consistently good performance. We will revisit this exam‐
ple in Chapter 12 after we have covered some more technical ground.
Describing Performance
Once you have described the load on your system, you can investigate what happens
when the load increases. You can look at it in two ways:
• When you increase a load parameter and keep the system resources (CPU, mem‐
ory, network bandwidth, etc.) unchanged, how is the performance of your system
affected?
• When you increase a load parameter, how much do you need to increase the
resources if you want to keep performance unchanged?
Both questions require performance numbers, so let’s look briefly at describing the
performance of a system.
In a batch processing system such as Hadoop, we usually care about throughput—the
number of records we can process per second, or the total time it takes to run a job
on a dataset of a certain size.iii In online systems, what’s usually more important is the
service’s response time—that is, the time between a client sending a request and
receiving a response.
Scalability 
| 
13
