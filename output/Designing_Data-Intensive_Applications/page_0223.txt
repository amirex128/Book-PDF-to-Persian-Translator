Figure 6-1. Combining replication and partitioning: each node acts as leader for some
partitions and follower for other partitions.
Partitioning of Key-Value Data
Say you have a large amount of data, and you want to partition it. How do you decide
which records to store on which nodes?
Our goal with partitioning is to spread the data and the query load evenly across
nodes. If every node takes a fair share, then—in theory—10 nodes should be able to
handle 10 times as much data and 10 times the read and write throughput of a single
node (ignoring replication for now).
If the partitioning is unfair, so that some partitions have more data or queries than
others, we call it skewed. The presence of skew makes partitioning much less effective.
In an extreme case, all the load could end up on one partition, so 9 out of 10 nodes
are idle and your bottleneck is the single busy node. A partition with disproportion‐
ately high load is called a hot spot.
The simplest approach for avoiding hot spots would be to assign records to nodes
randomly. That would distribute the data quite evenly across the nodes, but it has a
big disadvantage: when you’re trying to read a particular item, you have no way of
knowing which node it is on, so you have to query all nodes in parallel.
We can do better. Let’s assume for now that you have a simple key-value data model,
in which you always access a record by its primary key. For example, in an old-
fashioned paper encyclopedia, you look up an entry by its title; since all the entries
are alphabetically sorted by title, you can quickly find the one you’re looking for.
Partitioning of Key-Value Data 
| 
201
