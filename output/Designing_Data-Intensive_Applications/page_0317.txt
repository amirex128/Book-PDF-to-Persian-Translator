vi. There are distributed sequence number generators, such as Twitter’s Snowflake, that generate approxi‐
mately monotonically increasing unique IDs in a scalable way (e.g., by allocating blocks of the ID space to
different nodes). However, they typically cannot guarantee an ordering that is consistent with causality,
because the timescale at which blocks of IDs are assigned is longer than the timescale of database reads and
writes. See also “Ordering Guarantees” on page 339.
be consistent. With lots of small, rapid transactions, creating transaction IDs in a dis‐
tributed system becomes an untenable bottleneck.vi
Can we use the timestamps from synchronized time-of-day clocks as transaction IDs?
If we could get the synchronization good enough, they would have the right proper‐
ties: later transactions have a higher timestamp. The problem, of course, is the uncer‐
tainty about clock accuracy.
Spanner implements snapshot isolation across datacenters in this way [59, 60]. It uses
the clock’s confidence interval as reported by the TrueTime API, and is based on the
following observation: if you have two confidence intervals, each consisting of an ear‐
liest and latest possible timestamp (A = [Aearliest, Alatest] and B = [Bearliest, Blatest]), and
those two intervals do not overlap (i.e., Aearliest < Alatest < Bearliest < Blatest), then B defi‐
nitely happened after A—there can be no doubt. Only if the intervals overlap are we
unsure in which order A and B happened.
In order to ensure that transaction timestamps reflect causality, Spanner deliberately
waits for the length of the confidence interval before committing a read-write trans‐
action. By doing so, it ensures that any transaction that may read the data is at a suffi‐
ciently later time, so their confidence intervals do not overlap. In order to keep the
wait time as short as possible, Spanner needs to keep the clock uncertainty as small as
possible; for this purpose, Google deploys a GPS receiver or atomic clock in each
datacenter, allowing clocks to be synchronized to within about 7 ms [41].
Using clock synchronization for distributed transaction semantics is an area of active
research [57, 61, 62]. These ideas are interesting, but they have not yet been imple‐
mented in mainstream databases outside of Google. 
Process Pauses
Let’s consider another example of dangerous clock use in a distributed system. Say
you have a database with a single leader per partition. Only the leader is allowed to
accept writes. How does a node know that it is still leader (that it hasn’t been declared
dead by the others), and that it may safely accept writes?
One option is for the leader to obtain a lease from the other nodes, which is similar to
a lock with a timeout [63]. Only one node can hold the lease at any one time—thus,
when a node obtains a lease, it knows that it is the leader for some amount of time,
until the lease expires. In order to remain leader, the node must periodically renew
Unreliable Clocks 
| 
295
