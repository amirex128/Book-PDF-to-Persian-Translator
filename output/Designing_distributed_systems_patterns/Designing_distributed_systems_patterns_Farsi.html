<!DOCTYPE html>
<html lang="fa" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Designing_distributed_systems_patterns - فارسی</title>
    <link rel="stylesheet" href="fontiran.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.css" rel="stylesheet" />
    <style>
        @page {
            size: A4;
            margin: 2cm;
        }
        body {
            font-family: IRANSansX, Tahoma, Arial, sans-serif;
            line-height: 1.8;
            text-align: right;
            direction: rtl;
            margin: 0;
            padding: 20px;
            background-color: white;
        }
        .book-title {
            text-align: center;
            font-size: 24pt;
            margin: 3cm 0 1cm 0;
        }
        .book-subtitle {
            text-align: center;
            font-size: 18pt;
            margin-bottom: 3cm;
        }
        .toc {
            margin: 2cm 0;
            padding: 1cm;
            background-color: #f8f9fa;
            border-radius: 5px;
            page-break-after: always;
        }
        .toc h2 {
            margin-bottom: 1cm;
        }
        .toc ul {
            list-style-type: none;
            padding: 0;
        }
        .toc li {
            margin: 0.5cm 0;
            padding-right: 1cm;
        }
        .toc a {
            text-decoration: none;
            color: #2980b9;
        }
        .chapter {
            margin-bottom: 1cm;
            page-break-before: always;
        }
        .chapter:first-of-type {
            page-break-before: avoid;
        }
        .chapter-content {
            margin-bottom: 1cm;
        }
        .persian-translation {
            font-size: 14pt;
        }
        .page-images {
            text-align: center;
            margin: 1cm 0;
            page-break-before: always;
        }
        .page-images img {
            max-width: 100%;
            height: auto;
            margin: 0.5cm 0;
        }
        pre {
            direction: ltr;
            text-align: left;
            background-color: #f5f5f5;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 15px 0;
        }
        code {
            font-family: 'Courier New', Courier, monospace;
            direction: ltr;
            text-align: left;
        }
        span[dir="ltr"] {
            display: inline-block;
            direction: ltr;
            text-align: left;
        }
        .page-number {
            text-align: center;
            margin-top: 1cm;
            font-size: 10pt;
            color: #7f8c8d;
        }
    </style>
</head>
<body>
    <h1 class="book-title">Designing_distributed_systems_patterns</h1>
    <h2 class="book-subtitle">نسخه ترجمه شده</h2>
        <!-- Page 0001 -->
        <div class="chapter" id="page-0001">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<h3><strong>Brendan Burns</strong></h3>
<p><em>Designing</em></p>
<p><em>Distributed</em></p>
<p><em>Systems</em></p>
<p>PATTERNS AND PARADIGMS FOR SCALABLE, RELIABLE SERVICES</p>
</div>
 ```
        </div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 1" src="page_0001/image_1.png"/></div>
<div class="page-image"><img alt="Image from page 1" src="page_0001/image_2.png"/></div>
</div>
                <div class="page-number">صفحه 0001</div>
            </div>
        </div>
        <!-- Page 0003 -->
        <div class="chapter" id="page-0003">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<p><strong>Brendan Burns</strong></p>
<p><em>Designing Distributed Systems</em></p>
<p>Patterns and Paradigms for</p>
<p>Scalable, Reliable Services</p>
<br/>
<p>Boston</p>
<p>Farnham</p>
<p>Sebastopol</p>
<p>Tokyo</p>
<p>Beijing</p>
<br/>
<p>Boston</p>
<p>Farnham</p>
<p>Sebastopol</p>
<p>Tokyo</p>
<p>Beijing</p>
</div>
 ```

        </div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0003</div>
            </div>
        </div>
        <!-- Page 0004 -->
        <div class="chapter" id="page-0004">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<p>978-1-491-98364-5</p>
<p>[LSI]</p>
<p><em>Designing Distributed Systems</em></p>
<p>by Brendan Burns</p>
<p>Copyright © 2018 Brendan Burns. All rights reserved.</p>
<p>Printed in the United States of America.</p>
<p>Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.</p>
<p>O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are</p>
<p>also available for most titles (http://oreilly.com/safari). For more information, contact our corporate/insti‐</p>
<p>tutional sales department: 800-998-9938 or corporate@oreilly.com.</p>
<p>Editor: Angela Rufino</p>
<p>Production Editor: Colleen Cole</p>
<p>Copyeditor: Gillian McGarvey</p>
<p>Proofreader: Christina Edwards</p>
<p>Indexer: WordCo Indexing Services, Inc.</p>
<p>Interior Designer: David Futato</p>
<p>Cover Designer: Randy Comer</p>
<p>Illustrator: Rebecca Demarest</p>
<p>February 2018:</p>
<p> First Edition</p>
<br/>
<p>Revision History for the First Edition</p>
<p>2018-02-20: First Release</p>
<p>See http://oreilly.com/catalog/errata.csp?isbn=9781491983645 for release details.</p>
<p>The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Designing Distributed Systems, the</p>
<p>cover image, and related trade dress are trademarks of O’Reilly Media, Inc.</p>
<p>While the publisher and the author have used good faith efforts to ensure that the information and</p>
<p>instructions contained in this work are accurate, the publisher and the author disclaim all responsibility</p>
<p>for errors or omissions, including without limitation responsibility for damages resulting from the use of</p>
<p>or reliance on this work. Use of the information and instructions contained in this work is at your own</p>
<p>risk. If any code samples or other technology this work contains or describes is subject to open source</p>
<p>licenses or the intellectual property rights of others, it is your responsibility to ensure that your use</p>
<p>thereof complies with such licenses and/or rights.</p>
</div>
 ```

        </div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0004</div>
            </div>
        </div>
        <!-- Page 0005 -->
        <div class="chapter" id="page-0005">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<p>Table of Contents</p>
<p>Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii</p>
<p><strong>1. Introduction</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1</p>
<p>A Brief History of Systems Development 1</p>
<p>A Brief History of Patterns in Software Development 2</p>
<p>Formalization of Algorithmic Programming 3</p>
<p>Patterns for Object-Oriented Programming 3</p>
<p>The Rise of Open Source Software 3</p>
<p>The Value of Patterns, Practices, and Components 4</p>
<p>Standing on the Shoulders of Giants 4</p>
<p>A Shared Language for Discussing Our Practice 5</p>
<p>Shared Components for Easy Reuse 5</p>
<p>Summary 6</p>
<p>Part I.</p>
<p>Single-Node Patterns</p>
<p>Motivations 7</p>
<p>Summary 8</p>
<p><strong>2. The Sidecar Pattern</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11</p>
<p>An Example Sidecar: Adding HTTPS to a Legacy Service 11</p>
<p>Dynamic Configuration with Sidecars 12</p>
<p>Modular Application Containers 14</p>
<p>Hands On: Deploying the topz Container 14</p>
<p>Building a Simple PaaS with Sidecars 15</p>
<p>Designing Sidecars for Modularity and Reusability 16</p>
<p>Parameterized Containers 17</p>
<p>Define Each Container’s API 17</p>
<p>iii</p>
</div>
 ```

        </div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0005</div>
            </div>
        </div>
        <!-- Page 0006 -->
        <div class="chapter" id="page-0006">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<p>Documenting Your Containers 18</p>
<p>Summary 19</p>
<p><strong>3. Ambassadors</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21</p>
<p>Using an Ambassador to Shard a Service 22</p>
<p>Hands On: Implementing a Sharded Redis 23</p>
<p>Using an Ambassador for Service Brokering 25</p>
<p>Using an Ambassador to Do Experimentation or Request Splitting 26</p>
<p>Hands On: Implementing 10% Experiments 27</p>
<p><strong>4. Adapters</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31</p>
<p>Monitoring 32</p>
<p>Hands On: Using Prometheus for Monitoring 33</p>
<p>Logging 34</p>
<p>Hands On: Normalizing Different Logging Formats with Fluentd 35</p>
<p>Adding a Health Monitor 36</p>
<p>Hands On: Adding Rich Health Monitoring for MySQL 37</p>
<br/>
<p>Part II.</p>
<p>Serving Patterns</p>
<p>Introduction to Microservices 41</p>
<p><strong>5. Replicated Load-Balanced Services</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45</p>
<p>Stateless Services 45</p>
<p>Readiness Probes for Load Balancing 46</p>
<p>Hands On: Creating a Replicated Service in Kubernetes 47</p>
<p>Session Tracked Services 48</p>
<p>Application-Layer Replicated Services 49</p>
<p>Introducing a Caching Layer 49</p>
<p>Deploying Your Cache 50</p>
<p>Hands On: Deploying the Caching Layer 51</p>
<p>Expanding the Caching Layer 53</p>
<p>Rate Limiting and Denial-of-Service Defense 54</p>
<p>SSL Termination 54</p>
<p>Hands On: Deploying nginx and SSL Termination 55</p>
<p>Summary 57</p>
<p><strong>6. Sharded Services</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59</p>
<p>Sharded Caching 59</p>
<p>Why You Might Need a Sharded Cache 60</p>
<p>The Role of the Cache in System Performance 61</p>
<p>Replicated, Sharded Caches 62</p>
<p>iv</p>
<p>| Table of Contents</p>
</div>
 ```

        </div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0006</div>
            </div>
        </div>
        <!-- Page 0007 -->
        <div class="chapter" id="page-0007">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<p>Hands On: Deploying an Ambassador and Memcache for a Sharded Cache 63</p>
<p>An Examination of Sharding Functions 66</p>
<p>Selecting a Key 67</p>
<p>Consistent Hashing Functions 68</p>
<p>Hands On: Building a Consistent HTTP Sharding Proxy 69</p>
<p>Sharded, Replicated Serving 70</p>
<p>Hot Sharding Systems 70</p>
<p><strong>7. Scatter/Gather</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73</p>
<p>Scatter/Gather with Root Distribution 74</p>
<p>Hands On: Distributed Document Search 75</p>
<p>Scatter/Gather with Leaf Sharding 76</p>
<p>Hands On: Sharded Document Search 77</p>
<p>Choosing the Right Number of Leaves 78</p>
<p>Scaling Scatter/Gather for Reliability and Scale 79</p>
<p><strong>8. Functions and Event-Driven Processing</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81</p>
<p>Determining When FaaS Makes Sense 82</p>
<p>The Benefits of FaaS 82</p>
<p>The Challenges of FaaS 82</p>
<p>The Need for Background Processing 83</p>
<p>The Need to Hold Data in Memory 83</p>
<p>The Costs of Sustained Request-Based Processing 84</p>
<p>Patterns for FaaS 84</p>
<p>The Decorator Pattern: Request or Response Transformation 85</p>
<p>Hands On: Adding Request Defaulting Prior to Request Processing 86</p>
<p>Handling Events 87</p>
<p>Hands On: Implementing Two-Factor Authentication 87</p>
<p>Event-Based Pipelines 89</p>
<p>Hands On: Implementing a Pipeline for New-User Signup 89</p>
<p><strong>9. Ownership Election</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93</p>
<p>Determining If You Even Need Master Election 94</p>
<p>The Basics of Master Election 95</p>
<p>Hands On: Deploying etcd 97</p>
<p>Implementing Locks 98</p>
<p>Hands On: Implementing Locks in etcd 100</p>
<p>Implementing Ownership 101</p>
<p>Hands On: Implementing Leases in etcd 102</p>
<p>Handling Concurrent Data Manipulation 103</p>
<p>Table of Contents</p>
<p>| v</p>
</div>
 ```
        </div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0007</div>
            </div>
        </div>
        <!-- Page 0008 -->
        <div class="chapter" id="page-0008">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<p>Part III.</p>
<p>Batch Computational Patterns</p>
<p><strong>10. Work Queue Systems</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109</p>
<p>A Generic Work Queue System 109</p>
<p>The Source Container Interface 110</p>
<p>The Worker Container Interface 112</p>
<p>The Shared Work Queue Infrastructure 113</p>
<p>Hands On: Implementing a Video Thumbnailer 115</p>
<p>Dynamic Scaling of the Workers 117</p>
<p>The Multi-Worker Pattern 118</p>
<p><strong>11. Event-Driven Batch Processing</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121</p>
<p>Patterns of Event-Driven Processing 122</p>
<p>Copier 122</p>
<p>Filter 123</p>
<p>Splitter 124</p>
<p>Sharder 125</p>
<p>Merger 127</p>
<p>Hands On: Building an Event-Driven Flow for New User Sign-Up 128</p>
<p>Publisher/Subscriber Infrastructure 129</p>
<p>Hands On: Deploying Kafka 130</p>
<p><strong>12. Coordinated Batch Processing</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133</p>
<p>Join (or Barrier Synchronization) 134</p>
<p>Reduce 135</p>
<p>Hands On: Count 136</p>
<p>Sum 137</p>
<p>Histogram 137</p>
<p>Hands On: An Image Tagging and Processing Pipeline 138</p>
<p><strong>13. Conclusion: A New Beginning?</strong>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143</p>
<p>Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145</p>
<p>vi</p>
<p>| Table of Contents</p>
</div>
 ```
        </div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0008</div>
            </div>
        </div>
        <!-- Page 0009 -->
        <div class="chapter" id="page-0009">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<p>Preface</p>
<p>Who Should Read This Book</p>
<p>At this point, nearly every developer is a developer or consumer (or both) of dis‐</p>
<p>tributed systems. Even relatively simple mobile applications are backed with cloud</p>
<p>APIs so that their data can be present on whatever device the customer happens to be</p>
<p>using. Whether you are new to developing distributed systems or an expert with scars</p>
<p>on your hands to prove it, the patterns and components described in this book can</p>
<p>transform your development of distributed systems from art to science. Reusable</p>
<p>components and patterns for distributed systems will enable you to focus on the core</p>
<p>details of your application. This book will help any developer become better, faster,</p>
<p>and more efficient at building distributed systems.</p>
<br/>
<p>Why I Wrote This Book</p>
<p>Throughout my career as a developer of a variety of software systems from web</p>
<p>search to the cloud, I have built a large number of scalable, reliable distributed sys‐</p>
<p>tems. Each of these systems was, by and large, built from scratch. In general, this is</p>
<p>true of all distributed applications. Despite having many of the same concepts and</p>
<p>even at times nearly identical logic, the ability to apply patterns or reuse components</p>
<p>is often very, very challenging. This forced me to waste time reimplementing systems,</p>
<p>and each system ended up less polished than it might have otherwise been.</p>
<p>The recent introduction of containers and container orchestrators fundamentally</p>
<p>changed the landscape of distributed system development. Suddenly we have an</p>
<p>object and interface for expressing core distributed system patterns and building</p>
<p>reusable containerized components. I wrote this book to bring together all of the</p>
<p>practitioners of distributed systems, giving us a shared language and common stan‐</p>
<p>dard library so that we can all build better systems more quickly.</p>
<p>vii</p>
</div>
 ```
        </div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0009</div>
            </div>
        </div>
        <!-- Page 0010 -->
        <div class="chapter" id="page-0010">
            <div class="chapter-content">
                <div class="translated-content">
            ```html
 <div>
<p>The World of Distributed Systems Today</p>
<p>Once upon a time, people wrote programs that ran on one machine and were also</p>
<p>accessed from that machine. The world has changed. Now, nearly every application is</p>
<p>a distributed system running on multiple machines and accessed by multiple users</p>
<p>from all over the world. Despite their prevalence, the design and development of</p>
<p>these systems is often a black art practiced by a select group of wizards. But as with</p>
<p>everything in technology, the world of distributed systems is advancing, regularizing,</p>
<p>and abstracting. In this book I capture a collection of repeatable, generic patterns that</p>
<p>can make the development of reliable distributed systems more approachable and</p>
<p>efficient. The adoption of patterns and reusable components frees developers from</p>
<p>reimplementing the same systems over and over again. This time is then freed to</p>
<p>focus on building the core application itself.</p>
<br/>
<p>Navigating This Book</p>
<p>This book is organized into a 4 parts as follows:</p>
<p>Chapter 1, Introduction</p>
<p>Introduces distributed systems and explains why patterns and reusable compo‐</p>
<p>nents can make such a difference in the rapid development of reliable distributed</p>
<p>systems.</p>
<p>Part I, Single-Node Patterns</p>
<p>Chapters 2 through 4 discuss reusable patterns and components that occur on</p>
<p>individual nodes within a distributed system. It covers the side-car, adapter, and</p>
<p>ambassador single-node patterns.</p>
<p>Part II, Serving Patterns</p>
<p>Chapters 8 and 9 cover multi-node distributed patterns for long-running serving</p>
<p>systems like web applications. Patterns for replicating, scaling, and master elec‐</p>
<p>tion are discussed.</p>
<p>Part III, Batch Computational Patterns</p>
<p>Chapters 10 through 12 cover distributed system patterns for large-scale batch</p>
<p>data processing covering work queues, event-based processing, and coordinated</p>
<p>workflows.</p>
<p>If you are an experienced distributed systems engineer, you can likely skip the first</p>
<p>couple of chapters, though you may want to skim them to understand how we expect</p>
<p>these patterns to be applied and why we think the general notion of distributed sys‐</p>
<p>tem patterns is so important.</p>
<p>Everyone will likely find utility in the single-node patterns as they are the most</p>
<p>generic and most reusable patterns in the book.</p>
<p>viii</p>
<p>| Preface</p>
</div>
 ```
        </div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0010</div>
            </div>
        </div>
        <!-- Page 0011 -->
        <div class="chapter" id="page-0011">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong style="font-weight: bold;">مقدمه ای بر ترجمه متن فنی نرم‌افزار</strong></h3>
<p>با توجه به اهداف شما و systemهایی که می‌خواهید توسعه دهید، می‌توانید انتخاب کنید که روی الگوهای بزرگ داده‌ها (large-scale big data patterns) یا الگوهای servers طولانی مدت (long-running servers) (یا هر دو) تمرکز کنید. این دو بخش تا حد زیادی از یکدیگر مستقل هستند و می‌توانند به هر ترتیبی خوانده شوند.</p>
<p>به همین ترتیب، اگر تجربه گسترده‌ای در distributed system دارید، ممکن است متوجه شوید که برخی از فصل‌های اولیه الگوها (به عنوان مثال، بخش دوم در مورد naming، discovery و load balancing) با آنچه قبلاً می‌دانید تکراری هستند، بنابراین می‌توانید برای به دست آوردن insights سطح بالا به سرعت از آن‌ها عبور کنید—اما فراموش نکنید به تمام تصاویر زیبا نگاه کنید!</p>
<h3><strong style="font-weight: bold;"> قراردادهای مورد استفاده در این کتاب</strong></h3>
<p>قراردادهای تایپوگرافی زیر در این کتاب استفاده شده است:</p>
<ul>
<li><em style="font-style: italic;">Italic</em>: نشان‌دهنده اصطلاحات جدید، URLs، آدرس‌های ایمیل، نام فایل‌ها و پسوندهای فایل است.</li>
<li><span style="font-family: monospace;">Constant width</span>: برای لیست‌های برنامه، و همچنین در داخل پاراگراف‌ها برای اشاره به عناصر برنامه مانند نام‌های متغیر یا function، databaseها، انواع data، environment variables، statements و keywords استفاده می‌شود.</li>
<li><span style="font-family: monospace; font-weight: bold;">Constant width bold</span>: commands یا متن دیگری را نشان می‌دهد که باید توسط کاربر به صورت literal تایپ شود.</li>
<li><span style="font-family: monospace; font-style: italic;">Constant width italic</span>: متنی را نشان می‌دهد که باید با مقادیر ارائه شده توسط کاربر یا مقادیری که توسط context تعیین می‌شود، جایگزین شود.</li>
</ul>
<p>
<img alt="tip icon" src="" style="width: 20px; height: 20px; vertical-align: middle;"/> این آیکون، یک tip، پیشنهاد یا یادداشت کلی را نشان می‌دهد.
   </p>
<p>
<img alt="warning icon" src="" style="width: 20px; height: 20px; vertical-align: middle;"/> این آیکون، یک warning یا caution را نشان می‌دهد.
   </p>
<h3><strong style="font-weight: bold;">منابع آنلاین</strong></h3>
<p>اگرچه این کتاب الگوهای generally applicable distributed system را توضیح می‌دهد، اما انتظار دارد که خوانندگان با containers و container orchestration systems آشنا باشند.</p>
<p>Preface | ix</p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 11" src="page_0011/image_1.png"/></div>
<div class="page-image"><img alt="Image from page 11" src="page_0011/image_2.png"/></div>
</div>
                <div class="page-number">صفحه 0011</div>
            </div>
        </div>
        <!-- Page 0012 -->
        <div class="chapter" id="page-0012">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>اگر دانش قبلی زیادی در مورد این موارد ندارید، منابع زیر را توصیه می‌کنیم:</strong></h3>
<ul>
<li><a href="https://docker.io">https://docker.io</a></li>
<li><a href="https://kubernetes.io">https://kubernetes.io</a></li>
<li><a href="https://dcos.io">https://dcos.io</a></li>
</ul>
<h3><strong>استفاده از نمونه‌کدها</strong></h3>
<p>
  ماده‌ی تکمیلی (نمونه‌کدها، تمرین‌ها، و غیره) برای دانلود در <a href="https://github.com/brendandburns/designing-distributed-systems">https://github.com/brendandburns/designing-distributed-systems</a> موجود است.
  </p>
<p>
  این کتاب برای کمک به شما در انجام کارتان در اینجاست. به طور کلی، اگر نمونه کدی همراه با این کتاب ارائه می‌شود، می‌توانید از آن در برنامه‌ها و مستندات خود استفاده کنید. برای دریافت مجوز نیازی به تماس با ما ندارید، مگر اینکه بخش قابل توجهی از کد را تکثیر کنید. به عنوان مثال، نوشتن برنامه‌ای که از چندین بخش از کد این کتاب استفاده می‌کند، نیازی به مجوز ندارد. فروش یا توزیع یک CD-ROM از نمونه‌های کتاب‌های O’Reilly نیاز به مجوز دارد. پاسخ دادن به یک سؤال با استناد به این کتاب و نقل قول از نمونه کد نیازی به مجوز ندارد. گنجاندن مقدار قابل توجهی از نمونه کد این کتاب در مستندات محصول شما نیاز به مجوز دارد.
  </p>
<p>
  ما قدردانی می‌کنیم، اما نیازی به انتساب نداریم. انتساب معمولاً شامل عنوان، نویسنده، ناشر و ISBN می‌شود. به عنوان مثال: “Designing Distributed Systems by Brendan Burns (O’Reilly). Copyright 2018 Brendan Burns, 978-1-491-98364-5.”
  </p>
<p>
  اگر احساس می‌کنید استفاده شما از نمونه کدهای موجود در چارچوب استفاده منصفانه یا مجوز داده شده در بالا نیست، با ما در <a href="mailto:permissions@oreilly.com">permissions@oreilly.com</a> تماس بگیرید.
  </p>
<h3><strong>O’Reilly Safari</strong></h3>
<p>
  Safari (قبلاً Safari Books Online) یک پلتفرم آموزشی و مرجع مبتنی بر عضویت برای شرکت‌ها، دولت، مربیان و افراد است.
  </p>
<p>
  اعضا به هزاران کتاب، ویدیوهای آموزشی، Learning Paths، آموزش‌های تعاملی و لیست‌های پخش تنظیم‌شده از بیش از 250 ناشر، از جمله O’Reilly Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press, John Wiley &amp; Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones &amp; Bartlett, و Course Technology و دیگران دسترسی دارند.
  </p>
<p>x | Preface</p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 12" src="page_0012/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0012</div>
            </div>
        </div>
        <!-- Page 0013 -->
        <div class="chapter" id="page-0013">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
  برای اطلاعات بیشتر، لطفاً از <a href="http://oreilly.com/safari">http://oreilly.com/safari</a> دیدن کنید.
  </p>
<h3><strong>How to Contact Us</strong></h3>
<p>
  لطفاً نظرات و سؤالات مربوط به این کتاب را به ناشر ارسال کنید:
  </p>
<p>
  O’Reilly Media, Inc.
  <br/>
  1005 Gravenstein Highway North
  <br/>
  Sebastopol, CA 95472
  <br/>
  800-998-9938 (در ایالات متحده یا کانادا)
  <br/>
  707-829-0515 (بین‌المللی یا محلی)
  <br/>
  707-829-0104 (فکس)
  </p>
<p>
  ما یک web page برای این کتاب داریم، که در آن errata، examples و هر اطلاعات اضافی را لیست می‌کنیم. شما می‌توانید به این صفحه در <a href="http://bit.ly/designing-distributed-systems">http://bit.ly/designing-distributed-systems</a> دسترسی داشته باشید.
  </p>
<p>
  برای نظر دادن یا پرسیدن سوالات فنی در مورد این کتاب، ایمیل را به <a href="mailto:bookquestions@oreilly.com">bookquestions@oreilly.com</a> ارسال کنید.
  </p>
<p>
  برای اطلاعات بیشتر در مورد کتاب‌ها، دوره‌ها، کنفرانس‌ها و اخبار ما، به وب‌سایت ما به آدرس <a href="http://www.oreilly.com">http://www.oreilly.com</a> مراجعه کنید.
  </p>
<p>
  ما را در Facebook پیدا کنید: <a href="http://facebook.com/oreilly">http://facebook.com/oreilly</a>
</p>
<p>
  ما را در Twitter دنبال کنید: <a href="http://twitter.com/oreillymedia">http://twitter.com/oreillymedia</a>
</p>
<p>
  ما را در YouTube تماشا کنید: <a href="http://www.youtube.com/oreillymedia">http://www.youtube.com/oreillymedia</a>
</p>
<h3><strong>Acknowledgments</strong></h3>
<p>
  من می‌خواهم از همسرم Robin و فرزندانم برای همه کارهایی که برای شاد و عاقل نگه داشتن من انجام می‌دهند، تشکر کنم. از همه افرادی که در طول مسیر وقت گذاشتند تا به من کمک کنند تا همه این موارد را یاد بگیرم، بسیار سپاسگزارم! همچنین از پدر و مادرم برای آن اولین SE/30 تشکر می‌کنم.
  </p>
<p>Preface | xi</p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0013</div>
            </div>
        </div>
        <!-- Page 0015 -->
        <div class="chapter" id="page-0015">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>CHAPTER 1</strong></h3>
<h3>Introduction</h3>
<p>
  دنیای امروزِ applications و APIs همیشه فعال، نیازمند availability و reliability است که تنها چند دهه پیش، تنها برای تعداد کمی از services حیاتی در سراسر جهان مورد نیاز بود. به همین ترتیب، پتانسیل رشد سریع و ویروسی یک service به این معنی است که هر application باید طوری ساخته شود که تقریباً فوراً در پاسخ به تقاضای کاربر مقیاس‌پذیر باشد. این محدودیت‌ها و الزامات به این معنی است که تقریباً هر application ای که ساخته می‌شود - چه یک consumer mobile app باشد یا یک backend payments application - باید یک distributed system باشد.
  </p>
<p>
  اما ساختن distributed systems چالش برانگیز است. اغلب، آن‌ها راه‌حل‌های سفارشی یک‌باره هستند. به این ترتیب، توسعه distributed system شباهت چشمگیری به دنیای توسعه نرم‌افزار قبل از توسعه زبان‌های برنامه‌نویسی modern object-oriented دارد. خوشبختانه، مانند توسعه زبان‌های object-oriented، پیشرفت‌های تکنولوژیکی وجود داشته است که چالش‌های ساختن distributed systems را به طرز چشمگیری کاهش داده است. در این مورد، محبوبیت فزاینده containers و container orchestrators است. همانند مفهوم objects در programming object-oriented، این building blocks containerized اساس توسعه components و patterns قابل استفاده مجدد هستند که به طرز چشمگیری practices ساختن distributed systems قابل اعتماد را ساده و در دسترس می‌کنند. در introduction زیر، یک تاریخچه مختصری از پیشرفت‌هایی که منجر به موقعیت امروزی ما شده‌اند، ارائه می‌دهیم.
  </p>
<h3>A Brief History of Systems Development</h3>
<p>
  در ابتدا، ماشین‌هایی برای اهداف خاص ساخته شدند، مانند محاسبه جداول توپخانه یا جزر و مد، شکستن کدها، یا سایر applications ریاضی دقیق، پیچیده اما تکراری. در نهایت این ماشین‌های purpose-built به ماشین‌های programmable general-purpose تبدیل شدند. و در نهایت آن‌ها از اجرای
  </p>
<p>1</p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0015</div>
            </div>
        </div>
        <!-- Page 0016 -->
        <div class="chapter" id="page-0016">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
  one program at a time to running multiple programs روی یک machine واحد از طریق operating systems زمان-اشتراکی، اما این ماشین‌ها هنوز از یکدیگر جدا بودند.
  </p>
<p>
  به تدریج، ماشین‌ها به هم متصل شدند و معماری‌های client-server متولد شدند، به‌طوری‌که یک ماشین با قدرت نسبتاً کم روی میز کار کسی می‌توانست برای مهار قدرت بیشتر یک mainframe در اتاق یا ساختمان دیگری استفاده شود. در حالی که این نوع programming client-server تا حدودی پیچیده‌تر از نوشتن یک program برای یک ماشین واحد بود، درک آن هنوز نسبتاً ساده بود. client(ها) درخواست‌ها را ثبت می‌کردند؛ server(ها) به این درخواست‌ها رسیدگی می‌کردند.
  </p>
<p>
  در اوایل دهه 2000، ظهور اینترنت و datacenters در مقیاس بزرگ که شامل هزاران کامپیوتر نسبتاً کم‌هزینه و کالایی بود که به هم متصل شده بودند، باعث توسعه گسترده distributed systems شد. برخلاف معماری‌های client-server، applications از نوع distributed system از چندین application مختلف تشکیل شده‌اند که روی ماشین‌های مختلف یا بسیاری از replicas که در سراسر ماشین‌های مختلف اجرا می‌شوند، اجرا می‌شوند، همه با هم ارتباط برقرار می‌کنند تا یک system مانند web-search یا یک platform فروش خرده‌فروشی را پیاده‌سازی کنند.
  </p>
<p>
  به دلیل ماهیت distributed آن‌ها، وقتی به درستی ساختاردهی شوند، distributed systems ذاتاً قابل اعتمادتر هستند. و وقتی به درستی معماری شوند، می‌توانند به مدل‌های سازمانی بسیار مقیاس‌پذیرتری برای تیم‌های مهندسان نرم‌افزاری که این systems را ساخته‌اند، منجر شوند. متأسفانه، این مزایا هزینه‌ای دارند. این distributed systems می‌توانند به طور قابل توجهی پیچیده‌تر از طراحی، ساخت و اشکال‌زدایی صحیح باشند. مهارت‌های مهندسی مورد نیاز برای ساخت یک distributed system قابل اعتماد به طور قابل توجهی بالاتر از مهارت‌های مورد نیاز برای ساخت applications single-machine مانند frontends موبایل یا وب است.
  </p>
<p>
  صرف نظر از این، نیاز به reliable distributed systems همچنان در حال افزایش است. بنابراین نیاز متناظری به ابزارها، patterns و practices برای ساختن آن‌ها وجود دارد. خوشبختانه، technology نیز سهولت ساختن distributed systems را افزایش داده است. Containers، container images و container orchestrators در سال‌های اخیر محبوب شده‌اند زیرا آن‌ها foundation و building blocks برای distributed systems قابل اعتماد هستند. با استفاده از containers و container orchestration به عنوان یک foundation، می‌توانیم مجموعه‌ای از patterns و components قابل استفاده مجدد را ایجاد کنیم. این patterns و components یک toolkit هستند که می‌توانیم از آن‌ها برای ساختن systems خود به طور قابل اعتمادتر و کارآمدتر استفاده کنیم.
  </p>
<h3>A Brief History of Patterns in Software Development</h3>
<p>
  این اولین باری نیست که چنین تحولی در صنعت نرم‌افزار رخ داده است. برای یک context بهتر در مورد چگونگی shape گرفتن patterns, practices و components قابل استفاده مجدد توسعه systems، مفید است که به لحظاتی از گذشته نگاهی بیندازیم که در آن‌ها تحولات مشابهی رخ داده است.
  </p>
<p>2 | Chapter 1: Introduction</p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0016</div>
            </div>
        </div>
        <!-- Page 0017 -->
        <div class="chapter" id="page-0017">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>Formalization of Algorithmic Programming</strong></h3>
<p>
  اگرچه مردم بیش از یک دهه قبل از انتشار آن در سال 1962، programming می‌کردند، مجموعه Donald Knuth، The Art of Computer Programming (Addison-Wesley Professional)، یک فصل مهم در توسعه computer science را نشان می‌دهد. به طور خاص، این کتاب‌ها شامل algorithms هستند که برای هیچ computer خاصی طراحی نشده‌اند، بلکه برای آموزش خواننده در مورد خود algorithms طراحی شده‌اند. سپس این algorithms می‌توانستند با architecture خاص machine مورد استفاده یا problem خاصی که خواننده در حال حل آن بود، سازگار شوند. این formalization مهم بود زیرا یک toolkit مشترک برای ساختن programs خود در اختیار کاربران قرار می‌داد، اما همچنین به این دلیل که نشان می‌داد یک مفهوم general-purpose وجود دارد که برنامه‌نویسان باید یاد بگیرند و متعاقباً در زمینه‌های مختلف اعمال کنند. خود algorithms، مستقل از هر problem خاصی که باید حل شود، به خودی خود ارزش درک داشتند.
  </p>
<h3>Patterns for Object-Oriented Programming</h3>
<p>
  کتاب‌های Knuth یک نقطه عطف مهم در تفکر در مورد computer programming را نشان می‌دهند و algorithms یک component مهم در توسعه computer programming را نشان می‌دهند. با این حال، با افزایش پیچیدگی programs و افزایش تعداد افرادی که یک program واحد را می‌نویسند از یک رقم به دو رقم و در نهایت به هزاران نفر، مشخص شد که زبان‌های programming procedural و algorithms برای tasks برنامه‌نویسی امروزی کافی نیستند. این تغییرات در computer programming منجر به توسعه زبان‌های programming object-oriented شد، که data، reusability و extensibility را به peers of the algorithm در توسعه computer programs ارتقا داد.
  </p>
<p>
  در پاسخ به این تغییرات در computer programming، تغییراتی نیز در patterns و practices برای programming ایجاد شد. در سراسر اوایل تا اواسط دهه 1990، انفجاری از کتاب‌ها در مورد patterns برای programming object-oriented وجود داشت. معروف‌ترین آن‌ها کتاب "gang of four" است، Design Patterns: Elements of Reusable Object-Oriented Programming اثر Erich Gamma و همکاران (Addison-Wesley Professional). Design Patterns یک زبان و framework مشترک برای task of programming ارائه کرد. این series از interface-based patterns را که می‌توانستند در زمینه‌های مختلف مورد استفاده مجدد قرار گیرند، توضیح داد. به دلیل پیشرفت در programming object-oriented و به طور خاص interfaces، این patterns همچنین می‌توانستند به عنوان کتابخانه‌های generic قابل استفاده مجدد پیاده‌سازی شوند.
  </p>
<p>
  این کتابخانه‌ها می‌توانستند یک بار توسط یک community از developers نوشته شوند و بارها مورد استفاده مجدد قرار گیرند، که باعث صرفه‌جویی در زمان و بهبود reliability می‌شود.
  </p>
<h3>The Rise of Open Source Software</h3>
<p>
  اگرچه مفهوم به اشتراک گذاشتن source code توسط developers تقریباً از آغاز computing وجود داشته است و سازمان‌های formal free software از اواسط دهه 1980 وجود داشته‌اند، اما اواخر دهه 1990 و دهه 2000 شاهد یک dramatic
  </p>
<p>A Brief History of Patterns in Software Development | 3</p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0017</div>
            </div>
        </div>
        <!-- Page 0018 -->
        <div class="chapter" id="page-0018">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
  افزایش در توسعه و توزیع open source software. اگرچه open source فقط به‌طور جزئی به توسعه patterns برای distributed systems مرتبط است، از این نظر مهم است که از طریق community های open source مشخص شد که توسعه نرم‌افزار به طور کلی و توسعه distributed systems به طور خاص، تلاش‌های community است. مهم است توجه داشته باشید که تمام technology های container که foundation patterns شرح داده شده در این کتاب را تشکیل می‌دهند، به عنوان open source software توسعه یافته و منتشر شده‌اند. ارزش patterns هم برای توصیف و هم برای بهبود practice توسعه distributed، زمانی که از این perspective community به آن نگاه می‌کنید، به ویژه واضح است.
  </p>
<p>
  یک pattern برای یک distributed system چیست؟ دستورالعمل‌های زیادی وجود دارد که به شما می‌گویند چگونه specific distributed systems (مانند یک database از نوع NoSQL) را نصب کنید. به همین ترتیب، دستورالعمل‌هایی برای مجموعه خاصی از systems (مانند یک MEAN stack) وجود دارد. اما وقتی از patterns صحبت می‌کنم، منظورم blueprint های general برای سازماندهی distributed systems است، بدون اینکه هیچ technology یا application خاصی را الزامی کنم. هدف از یک pattern ارائه advice یا structure general برای هدایت design شما است. امید است که این patterns تفکر شما را هدایت کرده و همچنین به طور کلی برای طیف گسترده‌ای از applications و محیط‌ها قابل اجرا باشد.
  </p>
<h3>The Value of Patterns, Practices, and Components</h3>
<p>
  قبل از صرف هر یک از زمان ارزشمند خود برای خواندن در مورد series از patterns که ادعا می‌کنم practices توسعه شما را بهبود می‌بخشد، مهارت‌های جدیدی به شما می‌آموزد و - بیایید با آن روبرو شویم - زندگی شما را تغییر می‌دهد، منطقی است که بپرسید: “Why?” چه چیزی در مورد design patterns و practices وجود دارد که می‌تواند نحوه design و ساختن software ما را تغییر دهد؟ در این بخش، دلایلی را که فکر می‌کنم این یک موضوع مهم است، بیان خواهم کرد و امیدوارم شما را متقاعد کنم که تا پایان کتاب با من همراه باشید.
  </p>
<h3>Standing on the Shoulders of Giants</h3>
<p>
  به عنوان یک نقطه شروع، ارزشی که patterns برای distributed systems ارائه می‌دهند، این opportunity است که به طور مجازی روی شانه‌های غول‌ها بایستیم. به ندرت پیش می‌آید که problems که ما حل می‌کنیم یا systems که می‌سازیم واقعاً منحصر به فرد باشند. در نهایت، ترکیب قطعاتی که ما کنار هم قرار می‌دهیم و overall business model که software آن را فعال می‌کند، ممکن است چیزی باشد که جهان قبلاً هرگز ندیده است. اما نحوه ساخت system و problems که در تلاش برای reliable، agile و scalable بودن با آن‌ها مواجه می‌شود، جدید نیست.
  </p>
<p>
  بنابراین، این اولین ارزش patterns است: آن‌ها به ما اجازه می‌دهند از اشتباهات دیگران درس بگیریم. شاید شما قبلاً یک distributed system نساخته باشید، یا شاید هرگز این نوع distributed system را نساخته باشید. به جای اینکه امیدوار باشید که یک همکار مقداری experience در این زمینه دارد یا با انجام همان اشتباهاتی که دیگران انجام می‌دهند، یاد بگیرید،
  </p>
<p>4 | Chapter 1: Introduction</p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 18" src="page_0018/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0018</div>
            </div>
        </div>
        <!-- Page 0019 -->
        <div class="chapter" id="page-0019">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
  به patterns به عنوان راهنمای خود روی آورید. یادگیری در مورد patterns برای توسعه distributed system، همان یادگیری در مورد هر practice دیگری در computer programming است. این توانایی شما را برای ساختن software تسریع می‌کند بدون اینکه نیاز داشته باشید experience مستقیمی با systems، اشتباهات و یادگیری firsthand که منجر به codification pattern در وهله اول شد، داشته باشید.
  </p>
<h3>A Shared Language for Discussing Our Practice</h3>
<p>
  یادگیری و تسریع در درک ما از distributed systems تنها اولین ارزش داشتن مجموعه‌ای مشترک از patterns است. Patterns حتی برای developers با experience از distributed system که قبلاً آن‌ها را به خوبی درک کرده‌اند، ارزش دارند. Patterns یک vocabulary مشترک ارائه می‌دهند که ما را قادر می‌سازد تا یکدیگر را به سرعت درک کنیم. این understanding اساس به اشتراک گذاری دانش و یادگیری بیشتر را تشکیل می‌دهد.
  </p>
<p>
  برای درک بهتر این موضوع، تصور کنید که هر دوی ما از یک object یکسان برای ساختن خانه‌مان استفاده می‌کنیم. من آن object را “Foo” می‌نامم در حالی که شما آن object را “Bar” می‌نامید. چه مدت زمان صرف بحث در مورد ارزش Foo در مقابل Bar خواهیم کرد، یا تلاش می‌کنیم properties متفاوت Foo و Bar را توضیح دهیم تا زمانی که متوجه شویم که در مورد یک object یکسان صحبت می‌کنیم؟ تنها زمانی که مشخص کنیم Foo و Bar یکسان هستند، می‌توانیم واقعاً از experience یکدیگر یاد بگیریم.
  </p>
<p>
  بدون یک vocabulary مشترک، ما زمان را در بحث‌های "violent agreement" یا در توضیح مفاهیمی که دیگران درک می‌کنند اما با نام دیگری می‌شناسند، تلف می‌کنیم. در نتیجه، یک ارزش مهم دیگر patterns این است که مجموعه‌ای مشترک از نام‌ها و تعاریف ارائه می‌دهد تا ما وقت خود را صرف نگرانی در مورد نام‌گذاری نکنیم و در عوض مستقیماً به بحث در مورد جزئیات و پیاده‌سازی مفاهیم اصلی بپردازیم.
  </p>
<p>
  من این اتفاق را در مدت زمان کوتاهی که روی containers کار می‌کردم، دیده‌ام. در این مسیر، notion از یک sidecar container (شرح داده شده در Chapter 2 این کتاب) در community container جای خود را باز کرد. به همین دلیل، ما دیگر مجبور نیستیم وقت خود را صرف تعریف اینکه sidecar بودن به چه معناست کنیم و در عوض می‌توانیم فوراً به چگونگی استفاده از مفهوم برای حل یک problem خاص بپردازیم. "اگر فقط از یک sidecar استفاده کنیم"... "بله، و من دقیقاً می‌دانم از چه container می‌توانیم برای آن استفاده کنیم." این مثال منجر به سومین ارزش patterns می‌شود: ساختن components قابل استفاده مجدد.
  </p>
<h3>Shared Components for Easy Reuse</h3>
<p>
  فراتر از قادر ساختن مردم به یادگیری از دیگران و ارائه یک vocabulary مشترک برای بحث در مورد هنر ساختن systems، patterns یک ابزار مهم دیگر برای computer programming ارائه می‌دهند: توانایی شناسایی components مشترکی که می‌توانند یک بار پیاده‌سازی شوند.
  </p>
<p>
  اگر مجبور بودیم تمام کد مورد استفاده programs خود را خودمان ایجاد کنیم، هرگز کارمان تمام نمی‌شد. در واقع، به سختی شروع می‌کردیم. امروزه، هر system ای که تا به حال نوشته شده است، روی شانه‌های هزاران اگر نگوییم صدها هزار سال از human
  </p>
<p>The Value of Patterns, Practices, and Components | 5</p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0019</div>
            </div>
        </div>
        <!-- Page 0020 -->
        <div class="chapter" id="page-0020">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
  effort. Code برای operating systems، printer drivers، distributed databases، container runtimes و container orchestrators—در واقع، کل applications که امروزه می‌سازیم با کتابخانه‌ها و components shared و قابل استفاده مجدد ساخته شده‌اند.
  </p>
<p>
  Patterns اساس تعریف و توسعه چنین components های قابل استفاده مجدد هستند. formalization of algorithms منجر به پیاده‌سازی‌های قابل استفاده مجدد از sorting و سایر algorithms متعارف شد. شناسایی interface-based patterns منجر به ایجاد مجموعه‌ای از کتابخانه‌های generic، object-oriented شد که این patterns را پیاده‌سازی می‌کنند. شناسایی patterns اصلی برای distributed systems ما را قادر می‌سازد تا components مشترک shared بسازیم. پیاده‌سازی این patterns به عنوان container images با interfaces مبتنی بر HTTP به این معنی است که می‌توان آن‌ها را در بسیاری از زبان‌های programming مختلف مورد استفاده مجدد قرار داد. و البته، ساختن components قابل استفاده مجدد کیفیت هر component را بهبود می‌بخشد زیرا code base مشترک استفاده کافی برای شناسایی bugs و ضعف‌ها، و توجه کافی برای اطمینان از رفع آن‌ها را دریافت می‌کند.
  </p>
<h3>Summary</h3>
<p>
  Distributed systems برای پیاده‌سازی سطح reliability، agility و scale مورد انتظار از programs کامپیوتری مدرن مورد نیاز هستند. design از نوع Distributed system همچنان بیشتر یک black art است که توسط wizardها انجام می‌شود تا یک علم که توسط افراد عادی اعمال می‌شود. شناسایی patterns و practices مشترک، practice توسعه algorithmic و programming object-oriented را منظم و بهبود بخشیده است. هدف این کتاب این است که همین کار را برای distributed systems انجام دهد. لذت ببرید!
  </p>
<p>6 | Chapter 1: Introduction</p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0020</div>
            </div>
        </div>
        <!-- Page 0021 -->
        <div class="chapter" id="page-0021">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3 style="font-weight: bold;">PART I</h3>
<h3 style="font-weight: bold;">Single-Node Patterns</h3>
<p>
        این کتاب به سیستم‌های distributed می‌پردازد، که applicationsهایی هستند که از اجزای مختلفی تشکیل شده‌اند و بر روی ماشین‌های مختلف اجرا می‌شوند. با این حال، بخش اول این کتاب به patternsهایی اختصاص دارد که بر روی یک node واحد وجود دارند. انگیزه این کار ساده است. Containersها بلوک‌های ساختمانی اساسی برای patternsهای موجود در این کتاب هستند، اما در نهایت، گروه‌هایی از containers که در یک machine واحد قرار دارند، عناصر اتمی patternsهای سیستم distributed را تشکیل می‌دهند.
    </p>
<h3 style="font-weight: bold;">Motivations</h3>
<p>
        اگرچه مشخص است که چرا ممکن است بخواهید application distributed خود را به مجموعه‌ای از containersهای مختلف که بر روی ماشین‌های مختلف اجرا می‌شوند، تقسیم کنید، شاید کمی کمتر مشخص باشد که چرا ممکن است بخواهید اجزای در حال اجرا بر روی یک machine واحد را نیز به containersهای مختلف تقسیم کنید. برای درک انگیزه این گروه‌های containers، ارزش دارد که اهداف پشت containerization را در نظر بگیریم. به طور کلی، هدف یک container ایجاد مرزهایی در اطراف منابع خاص است (به عنوان مثال، این application به دو هسته و 8 GB حافظه نیاز دارد). به همین ترتیب، این مرز، مالکیت تیم را مشخص می‌کند (به عنوان مثال، این team مالک این image است). در نهایت، این مرز به منظور ارائه separation of concerns در نظر گرفته شده است (به عنوان مثال، این image این یک کار را انجام می‌دهد).
    </p>
<p>
        همه این دلایل، انگیزه ای برای تقسیم یک application بر روی یک machine واحد به گروهی از containers فراهم می‌کند. ابتدا isolation منابع را در نظر بگیرید. ممکن است application شما از دو component تشکیل شده باشد: یکی یک user-facing application server است و دیگری یک background configuration file loader است. بدیهی است، latency در پاسخگویی به درخواست‌های end-user بالاترین اولویت را دارد، بنابراین application user-facing نیاز به منابع کافی دارد تا اطمینان حاصل شود که بسیار responsive است. از طرف دیگر، background
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0021</div>
            </div>
        </div>
        <!-- Page 0022 -->
        <div class="chapter" id="page-0022">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        configuration loader تا حد زیادی یک service با best-effort است؛ اگر در زمان‌های volume بالای user-request کمی به تأخیر بیفتد، system مشکلی نخواهد داشت. به همین ترتیب، background configuration loader نباید بر کیفیت service که end users دریافت می‌کنند، تأثیر بگذارد.
    </p>
<p>
        به همه این دلایل، شما می‌خواهید service با user-facing و background shard loader را به containersهای مختلف تقسیم کنید. این به شما امکان می‌دهد تا resource requirements و اولویت‌های مختلفی را به دو container مختلف اختصاص دهید و، به عنوان مثال، اطمینان حاصل کنید که background loader به‌طور opportunistically از cycles service با user-facing می‌دزدد هر زمان که سبک بارگذاری شده و cycles آزاد هستند. به همین ترتیب، جدا کردن resource requirements برای دو container تضمین می‌کند که background loader قبل از service با user-facing خاتمه می‌یابد، اگر مشکل resource contention ناشی از memory leak یا other overcommitment of memory resources وجود داشته باشد.
    </p>
<p>
        علاوه بر این resource isolation، دلایل دیگری نیز برای تقسیم application single-node شما به containersهای متعدد وجود دارد. task scaling یک team را در نظر بگیرید. دلایل خوبی وجود دارد که معتقد باشیم اندازه تیم ایده‌آل شش تا هشت نفر است. برای ساختاردهی teams به این روش و هنوز هم ساختن systems های مهم، ما باید قطعات کوچک و متمرکزی داشته باشیم که هر تیم مالک آن باشد. علاوه بر این، اغلب برخی از components ها، در صورت factored شدن صحیح، ماژول‌های قابل استفاده مجدد هستند که می‌توانند توسط many teams استفاده شوند.
    </p>
<p>
        به عنوان مثال، task همگام‌سازی یک filesystem محلی با یک git source code repository را در نظر بگیرید. اگر این Git sync tool را به‌عنوان یک container جداگانه بسازید، می‌توانید از آن با PHP، HTML، JavaScript، Python و بسیاری از web-serving environments دیگر استفاده مجدد کنید. اگر در عوض، هر environment را به‌عنوان یک container واحد در نظر بگیرید، به‌عنوان مثال، runtime Python و همگام‌سازی Git جدایی‌ناپذیر هستند، آنگاه این نوع modular reuse (و تیم کوچک مربوطه که مالک آن module قابل استفاده مجدد است) غیرممکن است.
    </p>
<p>
        در نهایت، حتی اگر application شما کوچک باشد و همه containers شما متعلق به یک team واحد باشد، separation of concerns تضمین می‌کند که application شما به خوبی درک شده و به راحتی می‌تواند tested، updated و deployed شود. applications کوچک و متمرکز، درک آنها آسان‌تر است و couplings کمتری با سایر systems دارند. این بدان معناست که، به عنوان مثال، شما می‌توانید container Git synchronization را بدون نیاز به redeploy کردن application server خود، deploy کنید. این منجر به rollouts با وابستگی‌های کمتر و دامنه کوچکتر می‌شود. این، به نوبه خود، منجر به rollouts قابل اعتمادتر (و rollbacks) می‌شود، که منجر به چابکی و انعطاف‌پذیری بیشتر هنگام deploying application شما می‌شود.
    </p>
<h3 style="font-weight: bold;">Summary</h3>
<p>
        امیدوارم که همه این examples شما را به فکر breaking up applications خود، حتی آنهایی که روی یک node واحد هستند، به multiple containers وادار کرده باشد. فصل‌های زیر برخی از patterns را شرح می‌دهند که می‌توانند به شما در ساختن گروه‌های modular از containers کمک کنند. در مقابل patterns distributed چند node، همه این patterns فرض می‌کنند که وابستگی‌های تنگاتنگی بین همه containers در pattern وجود دارد. به طور خاص، آنها فرض می‌کنند که همه containers در pattern را می‌توان به طور قابل اعتماد بر روی یکدیگر coscheduled کرد.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0022</div>
            </div>
        </div>
        <!-- Page 0023 -->
        <div class="chapter" id="page-0023">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        1 Kubernetes یک system open source برای automating deployment، scaling و management of containerized applications است. کتاب من را بررسی کنید، Kubernetes: Up and Running (O’Reilly).
    </p>
<p>
        a single machine. آن‌ها همچنین فرض می‌کنند که همه containers در pattern می‌توانند به صورت اختیاری volumes یا بخش‌هایی از filesystems خود و همچنین other key container resources مانند network namespaces و shared memory را به اشتراک بگذارند. این grouping tight در Kubernetes،1 یک pod نامیده می‌شود، اما این concept به طور کلی برای container orchestratorsهای مختلف قابل اجرا است، اگرچه برخی از آن‌ها از این ویژگی نسبت به بقیه بیشتر پشتیبانی می‌کنند.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0023</div>
            </div>
        </div>
        <!-- Page 0025 -->
        <div class="chapter" id="page-0025">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3 style="font-weight: bold;">CHAPTER 2</h3>
<h3 style="font-weight: bold;">The Sidecar Pattern</h3>
<p>
        اولین pattern single-node، sidecar pattern است. sidecar pattern یک pattern single-node است که از دو container تشکیل شده است. اولی application container است. این container حاوی core logic برای application است. بدون این container، application وجود نخواهد داشت. علاوه بر application container، یک sidecar container وجود دارد. نقش sidecar این است که application container را تقویت و بهبود بخشد، اغلب بدون اطلاع application container. در ساده‌ترین حالت، یک sidecar container می‌تواند برای افزودن functionality به یک container استفاده شود که در غیر این صورت بهبود آن دشوار خواهد بود.
    </p>
<p>
        Sidecar containers از طریق یک گروه container اتمی، مانند شی pod API در Kubernetes، بر روی همان machine coscheduled می‌شوند. علاوه بر برنامه‌ریزی بر روی همان machine، application container و sidecar container تعدادی از resources، از جمله بخش‌هایی از filesystem، hostname و network و بسیاری از namespaces های دیگر را به اشتراک می‌گذارند. یک image generic از این sidecar pattern در شکل 2-1 نشان داده شده است.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 2-1. The generic sidecar pattern</em>
</p>
<h4 style="font-weight: bold;">An Example Sidecar: Adding HTTPS to a Legacy Service</h4>
<p>
        به عنوان مثال، یک legacy web service را در نظر بگیرید. سال‌ها پیش، زمانی که ساخته شد، امنیت network داخلی برای شرکت اولویت بالایی نداشت و بنابراین، application تنها درخواست‌ها را از طریق HTTP رمزگذاری نشده، نه HTTPS، service می‌کند. با توجه به حوادث امنیتی اخیر، این شرکت استفاده از HTTPS را برای همه وب‌سایت‌های شرکت الزامی کرده است.
    </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 25" src="page_0025/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0025</div>
            </div>
        </div>
        <!-- Page 0026 -->
        <div class="chapter" id="page-0026">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        همچنین، رنج تیم اعزامی برای به‌روزرسانی این web service خاص را بیشتر می‌کند، source code برای این application با یک نسخه قدیمی از build system شرکت ساخته شده است که دیگر کار نمی‌کند. Containerizing این application HTTP به اندازه کافی ساده است: binary می‌تواند در یک container با نسخه‌ای از یک توزیع قدیمی Linux در بالای یک kernel مدرن‌تر که توسط container orchestrator تیم اجرا می‌شود، اجرا شود. با این حال، task افزودن HTTPS به این application به طور قابل توجهی چالش برانگیزتر است.
    </p>
<p>
        تیم در تلاش است تا بین احیای build system قدیمی یا port کردن source code application به build system جدید تصمیم بگیرد، زمانی که یکی از اعضای تیم پیشنهاد می‌کند که از sidecar pattern برای حل آسان‌تر این situation استفاده کنند.
    </p>
<p>
        به کارگیری sidecar pattern در این situation ساده است. web service legacy برای سرویس‌دهی انحصاری بر روی localhost (127.0.0.1) پیکربندی شده است، که به این معنی است که تنها servicesهایی که network local را با server به اشتراک می‌گذارند، قادر به دسترسی به service خواهند بود. به طور معمول، این یک انتخاب عملی نخواهد بود زیرا به این معنی است که هیچ کس نمی‌تواند به web service دسترسی داشته باشد. با این حال، با استفاده از sidecar pattern، علاوه بر legacy container، ما یک nginx sidecar container اضافه خواهیم کرد. این nginx container در همان network namespace application web legacy قرار دارد، بنابراین می‌تواند به service که بر روی localhost در حال اجرا است، دسترسی داشته باشد. در عین حال، این nginx service می‌تواند ترافیک HTTPS را بر روی IP address external pod خاتمه دهد و این ترافیک را به application web legacy proxy کند (به شکل 2-2 مراجعه کنید). از آنجایی که این ترافیک رمزگذاری نشده تنها از طریق local loopback adapter در داخل container group ارسال می‌شود، تیم security network از ایمن بودن داده‌ها اطمینان دارد. به همین ترتیب، با استفاده از sidecar pattern، تیم یک application legacy را بدون نیاز به فهمیدن چگونگی بازسازی یک application جدید برای سرویس‌دهی HTTPS، مدرن کرده است.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 2-2. The HTTPS sidecar</em>
</p>
<h4 style="font-weight: bold;">Dynamic Configuration with Sidecars</h4>
<p>
        به سادگی proxy کردن ترافیک به یک application موجود، تنها استفاده از یک sidecar نیست. یک مثال رایج دیگر، همگام‌سازی configuration است. بسیاری از applications از یک configuration file برای پارامترسازی application استفاده می‌کنند؛ این ممکن است یک فایل متنی خام یا چیزی ساختاریافته‌تر مانند XML، JSON یا YAML باشد. بسیاری از applications های از پیش موجود برای فرض اینکه این فایل در filesystem وجود دارد و configuration خود را از آنجا می‌خوانند، نوشته شده‌اند. با این حال، در یک cloud-native environment، اغلب استفاده از یک API برای updating configuration بسیار مفید است. این به شما امکان می‌دهد تا اطلاعات configuration را از طریق یک API، به جای login دستی به هر server و updating configuration file با استفاده از دستورات imperative، به صورت dynamic push کنید.
    </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 26" src="page_0026/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0026</div>
            </div>
        </div>
        <!-- Page 0027 -->
        <div class="chapter" id="page-0027">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        زیرا چنین API هم توسط سهولت استفاده و هم توانایی افزودن اتوماسیون مانند rollback، که پیکربندی (و دوباره پیکربندی) را ایمن‌تر و آسان‌تر می‌کند، هدایت می‌شود.
    </p>
<p>
        مشابه مورد HTTPS، applications جدید را می‌توان با این انتظار نوشت که configuration یک property dynamic است که باید با استفاده از یک cloud API بدست آید، اما تطبیق و به روزرسانی یک application موجود می‌تواند به طور قابل توجهی چالش برانگیزتر باشد. خوشبختانه، sidecar pattern دوباره می‌تواند برای ارائه functionality جدیدی استفاده شود که یک application legacy را بدون تغییر application موجود، افزایش می‌دهد. برای sidecar pattern نشان داده شده در شکل 2-3، دوباره دو container وجود دارد: container که application را سرویس می‌دهد و container که configuration manager است.
    </p>
<p>
        دو containers با هم در یک pod گروه بندی شده‌اند، جایی که یک directory را بین خود به اشتراک می‌گذارند. این shared directory جایی است که configuration file نگهداری می‌شود.
    </p>
<p>
        هنگامی که application legacy شروع می‌شود، configuration خود را از filesystem، همانطور که انتظار می‌رود، بارگذاری می‌کند. هنگامی که configuration manager شروع به کار می‌کند، API configuration را بررسی می‌کند و به دنبال تفاوت‌هایی بین filesystem local و configuration ذخیره شده در API می‌گردد. اگر تفاوت‌هایی وجود داشته باشد، configuration manager configuration جدید را در filesystem local دانلود می‌کند و به application legacy signal می‌دهد که باید خود را با این configuration جدید دوباره پیکربندی کند. مکانیزم واقعی برای این notification بسته به application متفاوت است. برخی از applications در واقع configuration file را برای تغییرات نظارت می‌کنند، در حالی که بقیه به یک signal SIGHUP پاسخ می‌دهند. در موارد شدید، configuration manager ممکن است یک سیگنال SIGKILL را برای متوقف کردن application legacy ارسال کند. پس از متوقف شدن، system container orchestration application legacy را مجدداً راه‌اندازی می‌کند، که در این مرحله configuration جدید خود را بارگذاری می‌کند. همانند افزودن HTTPS به یک application موجود، این pattern نشان می‌دهد که چگونه sidecar pattern می‌تواند به تطبیق applications از پیش موجود با سناریوهای cloud-native بیشتر کمک کند.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 2-3. A sidecar example of managing a dynamic configuration</em>
</p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 27" src="page_0027/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0027</div>
            </div>
        </div>
        <!-- Page 0028 -->
        <div class="chapter" id="page-0028">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4 style="font-weight: bold;">Modular Application Containers</h4>
<p>
        در این مرحله، اگر فکر می‌کردید که تنها دلیل وجود sidecar pattern تطبیق applications legacy بوده است، جایی که دیگر نمی‌خواستید تغییراتی در source code اصلی ایجاد کنید، قابل بخشش است. در حالی که این یک مورد استفاده رایج برای pattern است، انگیزه های زیادی برای طراحی things با استفاده از sidecars وجود دارد. یکی دیگر از مزایای اصلی استفاده از sidecar pattern، modularity و reuse اجزای استفاده شده به عنوان sidecars است. در deploying هر application real-world و قابل اطمینان، functionality وجود دارد که شما برای debugging یا other management از application به آن نیاز دارید، مانند ارائه یک readout از همه processes های مختلف با استفاده از resources در container، مشابه ابزار top command line.
    </p>
<p>
        یک رویکرد برای ارائه این introspection این است که نیاز داشته باشید که هر developer یک interface HTTP /topz را پیاده‌سازی کند که یک readout از resource usage را ارائه می‌دهد. برای آسان‌تر کردن این کار، ممکن است این webhook را به‌عنوان یک plugin خاص زبان پیاده‌سازی کنید که developer می‌تواند به سادگی آن را به application خود لینک کند. اما حتی اگر این کار به این صورت انجام شود، developer مجبور می‌شود آن را لینک کند و سازمان شما مجبور خواهد شد interface را برای هر زبانی که می‌خواهد پشتیبانی کند، پیاده‌سازی کند. مگر اینکه با دقت شدید انجام شود، این رویکرد مطمئناً منجر به variations بین زبان‌ها و همچنین عدم پشتیبانی از functionality هنگام استفاده از زبان‌های جدید می‌شود. در عوض، این topz functionality را می‌توان به‌عنوان یک sidecar container مستقر کرد که namespace process-id (PID) را با application container به اشتراک می‌گذارد. این topz container می‌تواند همه processes های در حال اجرا را introspect کند و یک user interface سازگار ارائه دهد. علاوه بر این، شما می‌توانید از system orchestration برای افزودن خودکار این container به همه applications deployed شده از طریق system orchestration استفاده کنید تا اطمینان حاصل شود که یک مجموعه سازگار از tools برای همه applications در حال اجرا در زیرساخت شما در دسترس است.
    </p>
<p>
        البته، با هر انتخاب technical، trade-offs هایی بین این modular container-based pattern و roll کردن کد خودتان در application شما وجود دارد. رویکرد مبتنی بر library همیشه کمی کمتر متناسب با جزئیات application شما خواهد بود. این بدان معناست که ممکن است از نظر اندازه عملکرد، کارآمدی کمتری داشته باشد، یا اینکه API ممکن است برای مطابقت با محیط شما، نیاز به برخی تطبیق‌ها داشته باشد. من این trade-offs را با تفاوت بین خرید لباس از پیش آماده در مقایسه با مد سفارشی مقایسه می‌کنم. مد سفارشی همیشه بهتر به شما می‌آید، اما رسیدن آن زمان بیشتری می‌برد و هزینه بیشتری برای به دست آوردن آن لازم است. همانطور که در مورد لباس صدق می‌کند، برای اکثر ما معقول است که solution عمومی‌تر را در مورد coding خریداری کنیم. البته، اگر application شما از نظر performance به موارد extreme نیاز دارد، همیشه می‌توانید solution دست‌نویس را انتخاب کنید.
    </p>
<h4 style="font-weight: bold;">Hands On: Deploying the topz Container</h4>
<p>
        برای دیدن topz sidecar در عمل، ابتدا باید یک container دیگر را deploy کنید تا به عنوان application container عمل کند. یک application موجود را که در حال اجرا هستید انتخاب کنید و آن را با استفاده از Docker deploy کنید:
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0028</div>
            </div>
        </div>
        <!-- Page 0029 -->
        <div class="chapter" id="page-0029">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<pre><code class="language-bash">$ docker run -d &lt;my-app-image&gt;</code></pre>
<pre><code>&lt;container-hash-value&gt;</code></pre>
        بعد از اجرای آن image، شناسه مربوط به آن container خاص را دریافت خواهید کرد. این شبیه به این خواهد بود: cccf82b85000… اگر آن را ندارید، همیشه می‌توانید آن را با استفاده از دستور docker ps جستجو کنید، که تمام containers های در حال اجرا را نشان می‌دهد. با فرض اینکه شما آن value را در یک متغیر environment به نام APP_ID ذخیره کرده‌اید، می‌توانید container topz را در همان PID namespace با استفاده از دستور زیر اجرا کنید:
    </p>
<pre><code class="language-bash">$ docker run --pid=container:${APP_ID} \
    -p 8080:8080 \
    brendanburns/topz:db0fa58 \
    /server --address=0.0.0.0:8080</code></pre>
<p>
        این کار، sidecar topz را در همان PID namespace application container راه‌اندازی می‌کند. توجه داشته باشید که ممکن است لازم باشد portی را که sidecar برای سرویس‌دهی استفاده می‌کند، تغییر دهید، اگر application container شما نیز روی port 8080 در حال اجرا است. هنگامی که sidecar در حال اجرا است، می‌توانید از http://localhost:8080/topz بازدید کنید تا یک readout کامل از processes هایی که در application container در حال اجرا هستند و resource usage آنها را دریافت کنید.
    </p>
<p>
        شما می‌توانید این sidecar را با هر container موجود دیگری ترکیب کنید تا به راحتی یک view از نحوه استفاده container از resources خود از طریق یک web interface دریافت کنید.
    </p>
<h4 style="font-weight: bold;">Building a Simple PaaS with Sidecars</h4>
<p>
        sidecar pattern را می‌توان برای موارد بیشتر از adaptation و monitoring استفاده کرد. همچنین می‌توان از آن به‌عنوان وسیله‌ای برای پیاده‌سازی logic کامل برای application خود به روشی ساده و modular استفاده کرد. به‌عنوان مثال، ساخت یک platform as a service (PaaS) ساده را که حول git workflow ساخته شده است، تصور کنید. پس از deploy کردن این PaaS، به سادگی push کردن کد جدید به یک Git repository منجر به deploy شدن آن کد به servers در حال اجرا می‌شود. ما خواهیم دید که چگونه sidecar pattern، ساخت این PaaS را به طرز چشمگیری ساده می‌کند.
    </p>
<p>
        همانطور که قبلاً گفته شد، در sidecar pattern دو container وجود دارد: main application container و sidecar. در application PaaS ساده ما، main container یک server Node.js است که یک web server را پیاده‌سازی می‌کند. سرور Node.js به گونه‌ای instrument شده است که وقتی فایل‌های جدید updated می‌شوند، server را به طور خودکار reload می‌کند. این کار با tool nodemon انجام می‌شود.
    </p>
<p>
        sidecar container یک filesystem را با main application container به اشتراک می‌گذارد و یک حلقه ساده را اجرا می‌کند که filesystem را با یک Git repository موجود همگام‌سازی می‌کند:
    </p>
<pre><code class="language-bash">#!/bin/bash
while true; do
  git pull
  sleep 10
done</code></pre>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0029</div>
            </div>
        </div>
        <!-- Page 0030 -->
        <div class="chapter" id="page-0030">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        بدیهی است که این script می‌تواند پیچیده‌تر باشد و از یک branch خاص به جای صرفاً از HEAD pull کند. این به طور هدفمند ساده باقی مانده است تا خوانایی این مثال را بهبود بخشد.
    </p>
<p>
        application Node.js و Git synchronization sidecar با هم coscheduled و deployed می‌شوند تا PaaS ساده ما را پیاده‌سازی کنند (شکل 2-4). پس از deploy شدن، هر بار که کد جدیدی به یک Git repository push می‌شود، کد به طور خودکار توسط sidecar به‌روزرسانی شده و توسط server دوباره بارگذاری می‌شود.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 2-4. A simple sidecar-based PaaS</em>
</p>
<h4 style="font-weight: bold;">Designing Sidecars for Modularity and Reusability</h4>
<p>
        در همه examplesهای sidecars که در این فصل به تفصیل توضیح دادیم، یکی از مهم‌ترین themesها این است که هر کدام یک artifact modular و قابل استفاده مجدد بودند. برای موفقیت، sidecar باید در طیف گسترده‌ای از applications و deployments قابل استفاده مجدد باشد. با دستیابی به modular reuse، sidecars می‌تواند ساخت application شما را به طرز چشمگیری سرعت بخشد.
    </p>
<p>
        با این حال، این modularity و reusability، درست مانند دستیابی به modularity در توسعه نرم‌افزار با کیفیت بالا، نیازمند تمرکز و انضباط است. به طور خاص، شما باید بر توسعه سه حوزه تمرکز کنید:
    </p>
<ul>
<li>Parameterizing your containers</li>
<li>Creating the API surface of your container</li>
<li>Documenting the operation of your container</li>
</ul>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 30" src="page_0030/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0030</div>
            </div>
        </div>
        <!-- Page 0031 -->
        <div class="chapter" id="page-0031">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4 style="font-weight: bold;">Parameterized Containers</h4>
<p>
        Parameterizing your containers مهم‌ترین کاری است که می‌توانید برای modular و reusable کردن containers خود انجام دهید، صرف نظر از اینکه آنها sidecars هستند یا خیر، اگرچه sidecars و سایر add-on containersها برای parameterize کردن، اهمیت ویژه‌ای دارند.
    </p>
<p>
        منظور من از "parameterize" چیست؟ container خود را به‌عنوان یک function در program خود در نظر بگیرید. چند parameter دارد؟ هر parameter نشان‌دهنده یک ورودی است که می‌تواند یک container generic را برای یک situation خاص، سفارشی کند. به عنوان مثال، sidecar add-on SSL که قبلاً deployed شده را در نظر بگیرید. برای اینکه به طور کلی مفید باشد، احتمالاً به حداقل دو parameter نیاز دارد: اولی نام certificate است که برای ارائه SSL استفاده می‌شود، و دیگری port از application server "legacy" است که روی localhost در حال اجرا است. بدون این parametersها، تصور اینکه این sidecar container برای طیف وسیعی از applications قابل استفاده باشد، دشوار است. parametersهای مشابهی برای همه sidecars های دیگر که در این فصل توضیح داده شده‌اند، وجود دارد.
    </p>
<p>
        اکنون که parametersهایی را که می‌خواهیم exposed کنیم، می‌دانیم، چگونه آن‌ها را در واقع به کاربران exposed می‌کنیم و چگونه آن‌ها را در داخل container مصرف می‌کنیم. دو راه وجود دارد که در آن چنین parametersهایی را می‌توان به container شما منتقل کرد: از طریق environment variables یا command line. اگرچه هر دو امکان‌پذیر هستند، اما من به طور کلی ترجیح می‌دهم parametersها را از طریق environment variables منتقل کنم. یک مثال از passing چنین parametersهایی به یک sidecar container به این صورت است:
    </p>
<pre><code class="language-bash">docker run -e=PORT=&lt;port&gt; -d &lt;image&gt;</code></pre>
<p>
        البته، تحویل values به داخل container تنها بخشی از نبرد است. بخش دیگر، در واقع استفاده از این variables در داخل container است. به‌طور معمول، برای انجام این کار، از یک shell script ساده استفاده می‌شود که environment variables ارائه شده با sidecar container را بارگذاری می‌کند و یا configuration files را تنظیم می‌کند یا application اساسی را parameterize می‌کند.
    </p>
<p>
        به عنوان مثال، ممکن است certificate path و port را به‌عنوان environment variables منتقل کنید:
    </p>
<pre><code class="language-bash">docker run -e=PROXY_PORT=8080 -e=CERTIFICATE_PATH=/path/to/cert.crt ...</code></pre>
<p>
        در container خود، از آن variables برای پیکربندی یک فایل nginx.conf استفاده می‌کنید که web server را به فایل صحیح و proxy location اشاره می‌کند.
    </p>
<h4 style="font-weight: bold;">Define Each Container’s API</h4>
<p>
        با توجه به اینکه شما در حال parameterizing کردن containers خود هستید، بدیهی است که containers شما در حال تعریف یک "function" هستند که هر زمان که container اجرا می‌شود، فراخوانی می‌شود. این function به وضوح بخشی از API است که توسط container شما تعریف شده است، اما قسمت‌های دیگری نیز در این API وجود دارد، از جمله calls هایی که container به سایر services ها و همچنین APIs سنتی HTTP یا سایر services ها که container ارائه می‌دهد، انجام می‌دهد.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0031</div>
            </div>
        </div>
        <!-- Page 0032 -->
        <div class="chapter" id="page-0032">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        همانطور که به تعریف containers های modular و reusable فکر می‌کنید، مهم است که بدانید که همه جنبه‌های نحوه تعامل container شما با دنیای آن، بخشی از API تعریف شده توسط آن container reusable است. همانند دنیای microservices ها، این micro-containers ها برای اطمینان از جدایی clean بین main application container و sidecar به APIs متکی هستند. علاوه بر این API وجود دارد تا اطمینان حاصل شود که همه مصرف‌کنندگان sidecar با انتشار نسخه‌های بعدی، به درستی به کار خود ادامه می‌دهند. به همین ترتیب، داشتن یک API clean برای یک sidecar، developer sidecar را قادر می‌سازد تا سریع‌تر حرکت کند زیرا آن‌ها یک تعریف clear (و امیدواریم unit tests) برای services هایی که به عنوان بخشی از sidecar ارائه می‌دهند، دارند.
    </p>
<p>
        برای دیدن یک example concrete از اینکه چرا این API surface area مهم است، sidecar configuration management را که قبلاً مورد بحث قرار دادیم، در نظر بگیرید. یک configuration مفید برای این sidecar ممکن است UPDATE_FREQUENCY باشد، که نشان می‌دهد چند وقت یک‌بار configuration باید با filesystem همگام‌سازی شود. واضح است که اگر، در زمانی دیگر، نام parameter به UPDATE_PERIOD تغییر کند، این تغییر نقض API sidecar خواهد بود و به وضوح آن را برای برخی از users ها می‌شکند.
    </p>
<p>
        در حالی که این example واضح است، حتی تغییرات ظریف‌تر می‌توانند API sidecar را بشکنند. به عنوان مثال تصور کنید که UPDATE_FREQUENCY در ابتدا یک عدد را در seconds دریافت می‌کرد. با گذشت زمان و با بازخورد users، developer sidecar تشخیص داد که مشخص کردن seconds برای مقادیر زمانی بزرگ (به عنوان مثال، minutes) برای usersها آزاردهنده است و parameter را به پذیرش رشته‌ها (10 m، 5 s و غیره) تغییر داد. از آنجایی که مقادیر قدیمی parameter (به عنوان مثال، 10، برای 10 seconds) در این طرح جدید parse نمی‌شوند، این یک تغییر API breaking است.
    </p>
<p>
        هنوز هم فرض کنید که developer این را پیش‌بینی کرده است، اما valuesهایی را بدون units ایجاد کرده است که به milliseconds parse می‌شوند، جایی که قبلاً به seconds parse می‌شدند. حتی این تغییر، علیرغم عدم منجر شدن به خطا، یک تغییر API breaking برای sidecar است زیرا منجر به بررسی‌های configuration بسیار مکرر و به همان نسبت بار بیشتر بر روی cloud configuration server می‌شود.
    </p>
<p>
        من امیدوارم که این بحث به شما نشان داده باشد که برای modularity واقعی، شما باید نسبت به API که sidecar شما ارائه می‌دهد، بسیار آگاه باشید، و تغییرات "breaking" در آن API ممکن است همیشه به اندازه تغییر نام یک parameter، آشکار نباشد.
    </p>
<h4 style="font-weight: bold;">Documenting Your Containers</h4>
<p>
        تا کنون، شما دیده‌اید که چگونه می‌توانید containers sidecar خود را parameterize کنید تا آنها را modular و reusable کنید. شما در مورد اهمیت حفظ یک API stable برای اطمینان از اینکه sidecars ها را برای usersهای خود خراب نمی‌کنید، آموخته‌اید. اما یک گام نهایی برای ساخت containers های modular و reusable وجود دارد: اطمینان از اینکه مردم در وهله اول می‌توانند از آنها استفاده کنند.
    </p>
<p>
        همانند کتابخانه‌های نرم‌افزاری، کلید ساختن چیزی واقعاً مفید، توضیح چگونگی استفاده از آن است. اگر هیچ‌کس نتواند بفهمد که چگونه از آن استفاده کند، در ساخت یک container modular انعطاف‌پذیر و قابل اعتماد، هیچ خوبی وجود ندارد. متأسفانه، ابزارهای رسمی کمی برای doc وجود دارد.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0032</div>
            </div>
        </div>
        <!-- Page 0033 -->
        <div class="chapter" id="page-0033">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        umenting container images، اما برخی از best practices وجود دارد که می‌توانید برای انجام این کار استفاده کنید.
    </p>
<p>
        برای هر container image، آشکارترین مکان برای جستجوی documentation، Dockerfile است که container از آن ساخته شده است. بخش‌هایی از Docker file وجود دارد که از قبل نحوه عملکرد container را document می‌کنند. یک مثال از این مورد، دستورالعمل EXPOSE است، که نشان‌دهنده port هایی است که image به آنها گوش می‌دهد. اگرچه EXPOSE ضروری نیست، اما قرار دادن آن در Dockerfile و همچنین افزودن یک comment که دقیقاً چه چیزی به آن port گوش می‌دهد، یک practice خوب است. به عنوان مثال:
    </p>
<pre><code class="language-bash">...
# Main web server runs on port 8080
EXPOSE 8080
...</code></pre>
<p>
        علاوه بر این، اگر از environment variables برای parameterize کردن container خود استفاده می‌کنید، می‌توانید از دستورالعمل ENV برای تنظیم default values برای آن parameters ها و همچنین document کردن استفاده از آنها استفاده کنید:
    </p>
<pre><code class="language-bash">...
# The PROXY_PORT parameter indicates the port on localhost to redirect
# traffic to.
ENV PROXY_PORT 8000
...</code></pre>
<p>
        در نهایت، شما همیشه باید از دستورالعمل LABEL برای افزودن metadata برای image خود استفاده کنید؛ به عنوان مثال، آدرس ایمیل، web page و نسخه maintainer از image:
    </p>
<pre><code class="language-bash">...
LABEL "org.label-schema.vendor"="name@company.com"
LABEL "org.label.url"="http://images.company.com/my-cool-image"
LABEL "org.label-schema.version"="1.0.3"
...</code></pre>
<p>
        The names برای این labels از schema ایجاد شده توسط پروژه Label Schema گرفته شده است. این پروژه در حال کار بر روی ایجاد یک مجموعه مشترک از labels های شناخته شده است. با استفاده از یک taxonomy مشترک از image labels ها، چندین tool مختلف می‌توانند برای تجسم، نظارت و استفاده صحیح از یک application به همان meta information متکی باشند. با پذیرش terms های مشترک، می‌توانید از مجموعه toolsهای توسعه‌یافته در community بدون تغییر image خود استفاده کنید. البته، شما همچنین می‌توانید هر labels اضافی که در context image شما معنی دارد را اضافه کنید.
    </p>
<h4 style="font-weight: bold;">Summary</h4>
<p>
        در طول این فصل، ما sidecar pattern را برای ترکیب containers بر روی یک machine واحد معرفی کردیم. در sidecar pattern، یک sidecar container،
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0033</div>
            </div>
        </div>
        <!-- Page 0034 -->
        <div class="chapter" id="page-0034">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        و یک application container را گسترش می‌دهد تا functionality را اضافه کند. sidecars ها می‌توانند برای به‌روزرسانی applications legacy موجود، زمانی که تغییر application بیش از حد پرهزینه است، استفاده شوند. به همین ترتیب، می‌توان از آنها برای ایجاد containers های modular utility استفاده کرد که پیاده‌سازی‌های functionality رایج را استاندارد می‌کنند. این utility containers ها را می‌توان در تعداد زیادی از applications استفاده مجدد کرد، که consistency را افزایش داده و هزینه توسعه هر application را کاهش می‌دهد. فصل‌های بعدی، الگوهای single-node دیگری را معرفی می‌کنند که سایر موارد استفاده از containers های modular و reusable را نشان می‌دهند.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0034</div>
            </div>
        </div>
        <!-- Page 0035 -->
        <div class="chapter" id="page-0035">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3 style="font-weight: bold;">CHAPTER 3</h3>
<h3 style="font-weight: bold;">Ambassadors</h3>
<p>
        فصل قبل، sidecar pattern را معرفی کرد، جایی که یک container، یک container از پیش موجود را برای افزودن functionality افزایش می‌دهد. این فصل، ambassador pattern را معرفی می‌کند، جایی که یک ambassador container تعاملات بین application container و بقیه world را واسطه می‌کند. همانند سایر الگوهای single-node، دو containers در یک جفت همزیستی که به یک machine واحد برنامه‌ریزی شده‌اند، به‌شدت به هم متصل هستند.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 3-1. Generic ambassador pattern</em>
</p>
<p>
        value of the ambassador pattern دو جنبه دارد. اول، مانند سایر الگوهای single-node، ارزش ذاتی در ساخت containers های modular و reusable وجود دارد. separation of concerns، ساخت و نگهداری containersها را آسان‌تر می‌کند. به همین ترتیب، ambassador container می‌تواند با تعدادی application containers مختلف مورد استفاده مجدد قرار گیرد. این reuse، توسعه application را سرعت می‌بخشد زیرا کد container را می‌توان در تعدادی مکان مورد استفاده مجدد قرار داد. علاوه بر این، پیاده‌سازی هم consistentتر و هم با کیفیت بالاتر است زیرا یک‌بار ساخته شده و در بسیاری از contexts های مختلف استفاده می‌شود.
    </p>
<p>
        بقیه این فصل، تعدادی از examplesهای استفاده از ambassador pattern را برای پیاده‌سازی مجموعه‌ای از applicationsهای real-world ارائه می‌دهد.
    </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 35" src="page_0035/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0035</div>
            </div>
        </div>
        <!-- Page 0036 -->
        <div class="chapter" id="page-0036">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4 style="font-weight: bold;">Using an Ambassador to Shard a Service</h4>
<p>
        گاهی اوقات داده‌هایی که می‌خواهید در یک storage layer ذخیره کنید، برای اینکه یک machine واحد بتواند آنها را مدیریت کند، بیش از حد بزرگ می‌شوند. در چنین situation هایی، شما باید storage layer خود را shard کنید. Sharding، لایه را به قطعات مجزا و متعددی تقسیم می‌کند که هر کدام توسط یک machine جداگانه میزبانی می‌شوند. این فصل بر یک pattern single-node برای تطبیق یک service موجود برای صحبت با یک service sharded که در جایی از world وجود دارد، متمرکز است. این بحث نمی‌کند که چگونه service sharded به وجود آمده است. Sharding و یک multi-node sharded service design pattern با جزئیات زیاد در فصل 6 مورد بحث قرار گرفته است. نموداری از یک service sharded در شکل 3-2 نشان داده شده است.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 3-2. A generic sharded service</em>
</p>
<p>
        هنگام deploy کردن یک service sharded، یک سوال مطرح می‌شود این است که چگونه آن را با frontend یا middleware code که داده‌ها را ذخیره می‌کند، ادغام کنیم. واضح است که باید logic وجود داشته باشد که یک request خاص را به یک shard خاص هدایت کند، اما اغلب retrofit کردن چنین client sharded در source code موجود که انتظار دارد به یک storage backend واحد متصل شود، دشوار است. علاوه بر این، services sharded، اشتراک configuration را بین development environments (که اغلب فقط یک storage shard وجود دارد) و production environments (که اغلب storage shards زیادی وجود دارد) دشوار می‌کند.
    </p>
<p>
        یک رویکرد این است که تمام logic sharding را در خود service sharded بسازیم. در این رویکرد، service sharded همچنین دارای یک load balancer stateless است که ترافیک را به shard مناسب هدایت می‌کند. به‌طور مؤثر، این load balancer یک ambassador توزیع شده به‌عنوان یک service است. این کار، یک ambassador client-side را در ازای یک deployment پیچیده‌تر برای service sharded غیر ضروری می‌کند. جایگزین این است که یک ambassador single-node را در سمت client ادغام کنید تا ترافیک را به shard مناسب هدایت کنید.
    </p>
<p>
        این کار، deploy کردن client را تا حدودی پیچیده‌تر می‌کند، اما deployment service sharded را ساده می‌کند. همانطور که همیشه در مورد trade-offs ها صدق می‌کند، این به ویژگی‌های application خاص شما بستگی دارد که مشخص کند کدام رویکرد منطقی‌تر است. برخی از عواملی که باید در نظر گرفته شوند عبارتند از جایی که خطوط تیم در معماری شما قرار دارند، و همچنین جایی که شما کد می‌نویسید در مقابل deploy کردن نرم‌افزار off-the-shelf.
    </p>
<p>
        در نهایت، هر دو انتخاب معتبر هستند. بخش زیر نحوه استفاده از single-node ambassador pattern را برای sharding client-side توضیح می‌دهد.
    </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 36" src="page_0036/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0036</div>
            </div>
        </div>
        <!-- Page 0037 -->
        <div class="chapter" id="page-0037">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        هنگام تطبیق یک application موجود با یک sharded backend، می‌توانید یک ambassador container معرفی کنید که حاوی تمام logic مورد نیاز برای هدایت requests ها به storage shard مناسب باشد. بنابراین، frontend یا middleware application شما فقط به چیزی متصل می‌شود که به نظر می‌رسد یک storage backend واحد است که روی localhost در حال اجرا است. با این حال، این server در واقع یک sharding ambassador proxy است که تمام requests ها را از application code شما دریافت می‌کند، یک request را به storage shard مناسب ارسال می‌کند و سپس نتیجه را به application شما برمی‌گرداند. استفاده از این ambassador در شکل 3-3 نشان داده شده است.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 3-3. A generic sharded service</em>
</p>
<p>
        نتیجه خالص به‌کارگیری ambassador pattern برای services sharded، separation of concerns بین application container است که به سادگی می‌داند که باید با یک storage service صحبت کند و آن service را در localhost کشف می‌کند، و sharding ambassador proxy که فقط شامل code لازم برای انجام sharding مناسب است، می‌باشد.
    </p>
<p>
        همانند تمام الگوهای single-node خوب، این ambassador را می‌توان بین بسیاری از applications های مختلف استفاده مجدد کرد. یا، همانطور که در مثال hands-on زیر خواهیم دید، یک پیاده‌سازی open source off-the shelf را می‌توان برای ambassador استفاده کرد که توسعه system distributed کلی را سرعت می‌بخشد.
    </p>
<h4 style="font-weight: bold;">Hands On: Implementing a Sharded Redis</h4>
<p>
        Redis یک key-value store سریع است که می‌تواند به‌عنوان cache یا برای storage های پایدارتر استفاده شود. در این مثال، ما از آن به‌عنوان cache استفاده خواهیم کرد. ما با deploy کردن یک service sharded Redis به یک cluster Kubernetes شروع می‌کنیم. ما از شی API StatefulSet برای deploy کردن آن استفاده خواهیم کرد، زیرا این به ما نام‌های DNS منحصربه‌فردی را برای هر shard می‌دهد که می‌توانیم هنگام پیکربندی proxy از آنها استفاده کنیم.
    </p>
<p>
        StatefulSet برای Redis به این صورت است:
    </p>
<pre><code class="language-yaml">apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: sharded-redis
spec:
  serviceName: "redis"
  replicas: 3
  template:
    metadata:
      labels:
        app: redis
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: redis
        image: redis
        ports:
        - containerPort: 6379
          name: redis</code></pre>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0037</div>
            </div>
        </div>
        <!-- Page 0038 -->
        <div class="chapter" id="page-0038">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        این را در فایلی به نام redis-shards.yaml ذخیره کنید و می‌توانید این را با kubectl
        <pre><code class="language-bash">create -f redis-shards.yaml</code></pre>
        deploy کنید. این کار سه container را که redis را اجرا می‌کنند ایجاد می‌کند. شما می‌توانید این موارد را با اجرای
        <pre><code class="language-bash">kubectl get pods</code></pre>
        مشاهده کنید. شما باید sharded-redis-[0,1,2] را مشاهده کنید.
    </p>
<p>
        البته، فقط اجرای replicas کافی نیست. ما همچنین به نام‌هایی نیاز داریم که بتوانیم به replicasها ارجاع دهیم. در این مورد، ما از یک Kubernetes Service استفاده خواهیم کرد، که نام‌های DNS را برای replicas هایی که ایجاد کرده‌ایم، ایجاد می‌کند. Service به این صورت است:
    </p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis
spec:
  ports:
  - port: 6379
    name: redis
  clusterIP: None
  selector:
    app: redis</code></pre>
<p>
        این را در فایلی به نام redis-service.yaml ذخیره کنید و با
        <pre><code class="language-bash">kubectl create -f redis-service.yaml</code></pre>
        deploy کنید. اکنون باید ورودی‌های DNS را برای
        <pre><code>sharded-
        redis-0.redis, sharded-redis-1.redis, etc.</code></pre>
        داشته باشید. ما می‌توانیم از این نام‌ها برای پیکربندی twemproxy استفاده کنیم. twemproxy یک proxy سبک و با performance بالا برای memcached و Redis است که در اصل توسط Twitter توسعه یافته و open source است و در GitHub در دسترس است. ما می‌توانیم twemproxy را طوری پیکربندی کنیم که به replicasهایی که ایجاد کرده‌ایم، با استفاده از پیکربندی زیر اشاره کند:
    </p>
<pre><code class="language-yaml">redis:
  listen: 127.0.0.1:6379
  hash: fnv1a_64
  distribution: ketama
  auto_eject_hosts: true
  redis: true
  timeout: 400
  server_retry_timeout: 2000
  server_failure_limit: 1
  servers:
   - sharded-redis-0.redis:6379:1
   - sharded-redis-1.redis:6379:1
   - sharded-redis-2.redis:6379:1</code></pre>
<p>
        در این config، می‌توانید ببینید که ما در حال سرویس‌دهی protocol Redis بر روی localhost:6379 هستیم تا application container بتواند به ambassador دسترسی داشته باشد. ما این را با استفاده از یک شی Kubernetes ConfigMap که می‌توانیم با آن ایجاد کنیم، در ambassador pod خود deploy می‌کنیم:
    </p>
<pre><code class="language-bash">kubectl create configmap --from-file=nutcracker.yaml</code></pre>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0038</div>
            </div>
        </div>
        <!-- Page 0039 -->
        <div class="chapter" id="page-0039">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        در نهایت، همه preparations ها انجام شده است و ما می‌توانیم example ambassador خود را deploy کنیم.
        ما یک pod تعریف می‌کنیم که به این صورت است:
    </p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: ambassador-example
spec:
  containers:
    # This is where the application container would go, for example
    # - name: nginx
    #   image: nginx
    # This is the ambassador container
    - name: twemproxy
      image: ganomede/twemproxy
      command:
      - nutcracker
      - -c
      - /etc/config/nutcracker.yaml
      - -v
      - 7
      - -s
      - 6222
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: twem-config</code></pre>
<p>
        این pod، ambassador را تعریف می‌کند. سپس application container کاربر خاص را می‌توان تزریق کرد تا pod تکمیل شود.
    </p>
<h4 style="font-weight: bold;">Using an Ambassador for Service Brokering</h4>
<p>
        هنگام تلاش برای رندر کردن یک application قابل حمل در محیط‌های متعدد (به عنوان مثال، public cloud، physical datacenter یا private cloud)، یکی از چالش‌های اصلی، service discovery و configuration است. برای درک معنای این، یک frontend را تصور کنید که برای ذخیره داده‌های خود به یک database MySQL متکی است. در public cloud، این service MySQL ممکن است به‌عنوان software-as-a-service (SaaS) ارائه شود، در حالی که در یک private cloud ممکن است لازم باشد یک virtual machine یا container جدید را که MySQL را اجرا می‌کند، به طور dynamic spin up کنید.
    </p>
<p>
        در نتیجه، ساختن یک application قابل حمل مستلزم این است که application بداند چگونه environment خود را introspect کند و service MySQL مناسب برای اتصال به آن را پیدا کند.
        به این process، service discovery گفته می‌شود، و system که این discovery و linking را انجام می‌دهد، معمولاً یک service broker نامیده می‌شود. همانند examplesهای قبلی، ambassador pattern یک system را قادر می‌سازد تا logic application container را جدا کند.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0039</div>
            </div>
        </div>
        <!-- Page 0040 -->
        <div class="chapter" id="page-0040">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        از logic service broker ambassador. application به سادگی همیشه به یک instance از service (به عنوان مثال، MySQL) در حال اجرا بر روی localhost متصل می‌شود. این مسئولیت service broker ambassador است که environment خود را introspect کند و اتصال مناسب را برقرار کند. این process در شکل 3-3 نشان داده شده است.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 3-3. A service broker ambassador creating a MySQL service</em>
</p>
<h4 style="font-weight: bold;">Using an Ambassador to Do Experimentation or Request Splitting</h4>
<p>
        یک مثال نهایی از application pattern ambassador، انجام آزمایش یا سایر اشکال request splitting است. در بسیاری از systems های production، توانایی انجام request splitting مفید است، جایی که بخشی از تمام requests ها توسط main production service سرویس‌دهی نمی‌شوند، بلکه به یک پیاده‌سازی متفاوت از service هدایت می‌شوند. اغلب، از این برای انجام آزمایش‌ها با نسخه‌های جدید و بتا از service استفاده می‌شود تا مشخص شود آیا نسخه جدید نرم‌افزار قابل اعتماد است یا از نظر performance با نسخه فعلی deploy شده قابل مقایسه است.
    </p>
<p>
        علاوه بر این، request splitting گاهی اوقات برای tee یا split traffic استفاده می‌شود به طوری که تمام traffic به system production و همچنین یک نسخه جدیدتر و undeployed می‌رود.
        پاسخ‌ها از system production به user بازگردانده می‌شوند، در حالی که پاسخ‌ها از service tee-d نادیده گرفته می‌شوند. اغلب، از این شکل request splitting برای شبیه‌سازی load production بر روی نسخه جدید service بدون به خطر انداختن تأثیر بر usersهای production موجود استفاده می‌شود.
    </p>
<p>
        با توجه به examplesهای قبلی، دیدن اینکه چگونه یک request-splitting ambassador می‌تواند با یک application container تعامل داشته باشد تا request splitting را پیاده‌سازی کند، ساده است. مانند قبل، application container به سادگی به service در localhost متصل می‌شود، در حالی که ambassador container requestsها را دریافت می‌کند، پاسخ‌ها را به هر دو pro -
    </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 40" src="page_0040/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0040</div>
            </div>
        </div>
        <!-- Page 0041 -->
        <div class="chapter" id="page-0041">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        duction و experimental systems، و سپس پاسخ‌های production را برمی‌گرداند، گویی که کار را خودش انجام داده است.
    </p>
<p>
        این separation of concerns، کد را در هر container، slim و متمرکز نگه می‌دارد، و modular factoring application تضمین می‌کند که request-splitting ambassador را می‌توان برای انواع applications و تنظیمات مختلف، استفاده مجدد کرد.
    </p>
<h4 style="font-weight: bold;">Hands On: Implementing 10% Experiments</h4>
<p>
        برای پیاده‌سازی آزمایش request-splitting خود، ما قصد داریم از web server nginx استفاده کنیم. Nginx یک server open source قدرتمند و با ویژگی‌های غنی است. برای پیکربندی nginx به‌عنوان ambassador، ما از پیکربندی زیر استفاده خواهیم کرد (توجه داشته باشید که این برای HTTP است اما می‌توان آن را به راحتی برای HTTPS نیز تطبیق داد).
    </p>
<pre><code class="language-nginx">worker_processes  5;
error_log  error.log;
pid        nginx.pid;
worker_rlimit_nofile 8192;
events {
  worker_connections  1024;
}
http {
    upstream backend {
        ip_hash;
        server web weight=9;
        server experiment;
    }
    server {
        listen localhost:80;
        location / {
            proxy_pass http://backend;
        }
    }
}</code></pre>
<p>
        همانطور که در بحث قبلی services sharded اشاره شد، همچنین deploy کردن framework experiment به‌عنوان یک microservice جداگانه در جلوی application شما، به جای ادغام آن به‌عنوان بخشی از client pods، امکان‌پذیر است. البته، با انجام این کار شما یک service دیگر را معرفی می‌کنید که باید maintenance، scaled، monitored و غیره شود.
        اگر experimentation احتمالاً یک component طولانی‌مدت در architecture شما باشد، این ممکن است ارزشمند باشد. اگر بیشتر اوقات استفاده شود، یک ambassador client-side ممکن است منطقی‌تر باشد.
    </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 41" src="page_0041/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0041</div>
            </div>
        </div>
        <!-- Page 0042 -->
        <div class="chapter" id="page-0042">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        شما متوجه خواهید شد که من در این پیکربندی از IP hashing استفاده می‌کنم. این مهم است زیرا اطمینان می‌دهد که user، بین experiment و main site، back و forth نمی‌شود. این تضمین می‌کند که هر user، یک تجربه consistent با application دارد.
    </p>
<p>
        parameter weight برای ارسال 90٪ از ترافیک به main existing application استفاده می‌شود، در حالی که 10٪ از ترافیک به experiment هدایت می‌شود.
    </p>
<p>
        همانند examplesهای دیگر، ما این پیکربندی را به‌عنوان یک شی ConfigMap در Kubernetes deploy خواهیم کرد:
    </p>
<pre><code class="language-bash">kubectl create configmaps --from-file=nginx.conf</code></pre>
<p>
        البته، این فرض را دارد که شما هم یک service web و هم یک service experiment تعریف کرده‌اید. اگر این کار را نکرده‌اید، باید آنها را اکنون قبل از تلاش برای ایجاد ambassador con - تعریف کنید، زیرا nginx دوست ندارد اگر نتواند services هایی را که به آنها proxy می‌کند، پیدا کند، شروع به کار کند.
        در اینجا برخی از service configs های example وجود دارد:
    </p>
<pre><code class="language-yaml"># This is the 'experiment' service
apiVersion: v1
kind: Service
metadata:
  name: experiment
  labels:
    app: experiment
spec:
  ports:
  - port: 80
    name: web
  selector:
    # Change this selector to match your application's labels
    app: experiment
---
# This is the 'prod' service
apiVersion: v1
kind: Service
metadata:
  name: web
  labels:
    app: web
spec:
  ports:
  - port: 80
    name: web
  selector:
    # Change this selector to match your application's labels
    app: web</code></pre>
<p>
        و سپس ما خود nginx را به‌عنوان ambassador container در یک pod مستقر خواهیم کرد:
    </p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod</code></pre>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0042</div>
            </div>
        </div>
        <!-- Page 0043 -->
        <div class="chapter" id="page-0043">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre><code class="language-yaml">metadata:
  name: experiment-example
spec:
  containers:
    # This is where the application container would go, for example
    # - name: some-name
    #   image: some-image
    # This is the ambassador container
    - name: nginx
      image: nginx
      volumeMounts:
      - name: config-volume
        mountPath: /etc/nginx
  volumes:
    - name: config-volume
      configMap:
        name: experiment-config</code></pre>
<p>
        شما می‌توانید یک (یا دومین یا سومین یا چهارمین) container دوم را به pod اضافه کنید تا از ambassador استفاده کنید.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0043</div>
            </div>
        </div>
        <!-- Page 0045 -->
        <div class="chapter" id="page-0045">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3 style="font-weight: bold;">CHAPTER 4</h3>
<h3 style="font-weight: bold;">Adapters</h3>
<p>
        در فصل‌های قبل، دیدیم که چگونه sidecar pattern می‌تواند application containers موجود را توسعه داده و افزایش دهد. ما همچنین دیدیم که چگونه ambassadorsها می‌توانند نحوه ارتباط application container با دنیای بیرون را تغییر دهند و واسطه گری کنند. این فصل، pattern single-node نهایی را شرح می‌دهد: adapter pattern. در adapter pattern، از adapter container برای اصلاح interface application container استفاده می‌شود تا با برخی از interfaceهای از پیش تعریف‌شده که از همه applications ها انتظار می‌رود، مطابقت داشته باشد. به عنوان مثال، یک adapter ممکن است اطمینان حاصل کند که یک application یک interface نظارتی consistent را پیاده‌سازی می‌کند. یا ممکن است اطمینان حاصل کند که log files همیشه به stdout یا هر تعداد دیگری از conventions ها نوشته می‌شوند.
    </p>
<p>
        توسعه application real-world یک تمرین heterogeneous و hybrid است. برخی از بخش‌های application شما ممکن است توسط تیم شما از ابتدا نوشته شده باشد، برخی توسط vendorها ارائه شده باشد و برخی ممکن است کاملاً از نرم‌افزار open source یا proprietary off-the-shelf تشکیل شده باشد که شما آن را به‌عنوان binary از پیش کامپایل شده مصرف می‌کنید. تأثیر خالص این heterogeneity این است که هر application real-world که deploy می‌کنید، در انواع زبان‌ها، با انواع conventions ها برای logging، monitoring و سایر services های رایج نوشته شده است.
    </p>
<p>
        با این حال، برای نظارت و بهره‌برداری مؤثر از application خود، شما به interfaces های common نیاز دارید. هنگامی که هر application metrics را با استفاده از یک format و interface متفاوت ارائه می‌دهد، جمع‌آوری همه آن metrics ها در یک مکان واحد برای visualization و alerting بسیار دشوار است. اینجاست که adapter pattern مرتبط است. مانند سایر الگوهای single-node، adapter pattern از containers های modular تشکیل شده است. application containers های مختلف می‌توانند interfaces نظارتی متفاوتی را ارائه دهند در حالی که adapter container این heterogeneity را تطبیق می‌دهد تا یک interface consistent را ارائه دهد. این شما را قادر می‌سازد تا یک tool واحد را deploy کنید که انتظار این interface واحد را دارد. شکل 4-1 این pat - را نشان می‌دهد.
    </p>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 4-1. Generic adapter pattern</em>
</p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0045</div>
            </div>
        </div>
        <!-- Page 0046 -->
        <div class="chapter" id="page-0046">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p style="text-align: center;">
<em style="font-style: italic;">Figure 4-1. The generic adapter pattern</em>
</p>
<p>
        بقیه این فصل، چندین application مختلف از adapter pattern را ارائه می‌دهد.
    </p>
<h4 style="font-weight: bold;">Monitoring</h4>
<p>
        هنگام monitoring نرم‌افزار خود، شما به یک solution واحد نیاز دارید که بتواند به طور خودکار هر applicationی را که در environment شما deploy شده است، کشف و مانیتور کند. برای عملی کردن این، هر application باید یک interface نظارتی یکسان را پیاده‌سازی کند.
        examples های متعددی از interfaces نظارتی استاندارد، مانند syslog، event tracing بر روی ویندوز (etw)، JMX برای applications جاوا، و بسیاری از protocols ها و interfaces های دیگر وجود دارد. با این حال، هر یک از این موارد، هم در protocol برای ارتباطات و هم در سبک ارتباطات (push در مقابل pull) منحصربه‌فرد هستند.
    </p>
<p>
        متأسفانه، applications در system distributed شما احتمالاً گستره‌ای از code را که خودتان نوشته‌اید، تا components های open source off-the-shelf در بر می‌گیرد. در نتیجه، شما خود را با طیف گسترده‌ای از interfaces های نظارتی مختلف خواهید یافت که باید آنها را در یک system واحد و well-understood ادغام کنید.
    </p>
<p>
        خوشبختانه، اکثر solutions های monitoring درک می‌کنند که باید به طور گسترده قابل اجرا باشند، و از این رو، آنها انواع مختلفی از plugins ها را پیاده‌سازی کرده‌اند که می‌توانند یک format نظارتی را با یک interface common تطبیق دهند. با توجه به این مجموعه از tools ها، چگونه می‌توانیم applications های خود را به روشی agile و stable، deploy و manage کنیم؟ خوشبختانه، adapter pattern می‌تواند پاسخ‌ها را به ما ارائه دهد. با به‌کارگیری adapter pattern برای monitoring، ما می‌بینیم که application container به سادگی applicationی است که ما می‌خواهیم آن را monitor کنیم.
        adapter container، tools هایی را برای تبدیل interface نظارتی که توسط application container در معرض دید قرار دارد، به interface مورد انتظار توسط system monitoring general-purpose، شامل می‌شود.
    </p>
<p>
        Decoupling system به این روش، یک system قابل فهم‌تر و قابل نگهداری‌تر ایجاد می‌کند. ارائه نسخه‌های جدید application نیازی به rollout adapter نظارتی ندارد. علاوه بر این، container نظارتی می‌تواند با چندین application container مختلف مورد استفاده مجدد قرار گیرد. container نظارتی، ممکن است حتی توسط نگهدارندگان system monitoring، مستقل از developers application ارائه شده باشد. در نهایت، deploy کردن adapter نظارتی به‌عنوان یک container جداگانه تضمین می‌کند که هر container، resources های اختصاصی خود را هم از نظر CPU و هم از نظر memory دریافت می‌کند.
    </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 46" src="page_0046/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0046</div>
            </div>
        </div>
        <!-- Page 0047 -->
        <div class="chapter" id="page-0047">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        این اطمینان می‌دهد که یک adapter نظارتی بد رفتار نمی‌تواند باعث ایجاد مشکل در یک service با user-facing شود.
    </p>
<h4 style="font-weight: bold;">Hands On: Using Prometheus for Monitoring</h4>
<p>
        به عنوان مثال، نظارت بر containers خود را از طریق پروژه open source Prometheus در نظر بگیرید. Prometheus یک monitoring aggregator است که metrics ها را جمع‌آوری کرده و آنها را در یک database time-series واحد جمع‌آوری می‌کند. علاوه بر این database، Prometheus، visualization و query language را برای introspecting the collected metrics فراهم می‌کند. برای جمع‌آوری metrics ها از انواع systems های مختلف، Prometheus انتظار دارد که هر container یک metrics API خاص را در معرض دید قرار دهد. این Prometheus را قادر می‌سازد تا طیف گسترده‌ای از برنامه‌های مختلف را از طریق یک interface واحد، monitor کند.
    </p>
<p>
        با این حال، بسیاری از برنامه‌های محبوب، مانند key-value store Redis، metrics ها را در یک format که با Prometheus سازگار است، export نمی‌کنند. در نتیجه، adapter pattern برای گرفتن یک service موجود مانند Redis و تطبیق آن با interface جمع‌آوری metrics Prometheus، بسیار مفید است.
    </p>
<p>
        یک تعریف pod ساده Kubernetes را برای یک server Redis در نظر بگیرید:
    </p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: adapter-example
  namespace: default
spec:
  containers:
  - image: redis
    name: redis</code></pre>
<p>
        در این مرحله، این container قادر به monitor شدن توسط Prometheus نیست زیرا interface مناسب را export نمی‌کند. با این حال، اگر ما به سادگی یک adapter container (در این مورد، یک exporter Prometheus open source) اضافه کنیم، می‌توانیم این pod را تغییر دهیم تا interface صحیح را export کند و بنابراین آن را با انتظارات Prometheus تطبیق دهیم:
    </p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: adapter-example
  namespace: default
spec:
  containers:
  - image: redis
    name: redis
 # Provide an adapter that implements the Prometheus interface
  - image: oliver006/redis_exporter
    name: adapter</code></pre>
<p>
        این مثال، نه‌تنها value adapter pattern را برای اطمینان از یک interface consistent، بلکه value container patterns را به‌طور کلی برای con -
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0047</div>
            </div>
        </div>
        <!-- Page 0048 -->
        <div class="chapter" id="page-0048">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        tainer reuse. در این مورد، example نشان‌داده‌شده، یک container Redis موجود را با یک adapter Prometheus موجود ترکیب می‌کند. تأثیر خالص این یک server Redis قابل مانیتور است، که برای deploy کردن آن، کار کمی از طرف ما لازم است. در صورت عدم وجود adapter pattern، deployment یکسان، به کار سفارشی بسیار بیشتری نیاز داشت و به یک solution کمتر قابل بهره‌برداری منجر می‌شد، زیرا هرگونه به‌روزرسانی برای Redis یا adapter، نیاز به کار برای اعمال به‌روزرسانی داشت.
    </p>
<h4 style="font-weight: bold;">Logging</h4>
<p>
        مشابه monitoring، تنوع زیادی در نحوه logging داده‌ها به یک output stream وجود دارد. systemsها ممکن است logsهای خود را به سطوح مختلف (مانند debug، info، warning و error) تقسیم کنند که هر سطح به یک فایل متفاوت می‌رود. برخی ممکن است به سادگی به stdout و stderr log شوند. این امر به‌ویژه در دنیای containerized applications که انتظار کلی وجود دارد که containersهای شما به stdout log می‌شوند، مشکل‌ساز است، زیرا این چیزی است که از طریق دستوراتی مانند docker logs یا kubectl logs در دسترس است.
    </p>
<p>
        افزودن پیچیدگی بیشتر، اطلاعات logged به‌طور کلی دارای information ساختار یافته است (به عنوان مثال، تاریخ/زمان log)، اما این اطلاعات بین کتابخانه‌های logging مختلف (به عنوان مثال، logging داخلی جاوا در مقابل بسته glog برای Go) بسیار متفاوت است.
    </p>
<p>
        البته، هنگامی که شما در حال ذخیره و query کردن logs برای system distributed خود هستید، واقعاً به این تفاوت‌ها در format logging اهمیت نمی‌دهید. شما می‌خواهید اطمینان حاصل کنید که با وجود ساختارهای مختلف برای data، هر log در نهایت با timestamp مناسب به پایان می‌رسد.
    </p>
<p>
        خوشبختانه، همانطور که در مورد monitoring، adapter pattern می‌تواند به ارائه یک design modular و reusable برای هر دو این situation ها کمک کند. در حالی که application container ممکن است به یک فایل log شود، adapter container می‌تواند آن فایل را به stdout هدایت کند. application containers های مختلف می‌توانند اطلاعات را در formats های مختلف log کنند، اما adapter container می‌تواند آن داده‌ها را به یک representation ساختاریافته واحد تبدیل کند که می‌تواند توسط log aggregator شما مصرف شود. باز هم، adapter یک دنیای heterogeneous از applications را می‌گیرد و یک دنیای homogenous از interfaces های common ایجاد می‌کند.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0048</div>
            </div>
        </div>
        <!-- Page 0049 -->
        <div class="chapter" id="page-0049">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        یک سوال که اغلب هنگام consideration adapter patterns مطرح می‌شود این است: چرا به‌سادگی خود application container را تغییر ندهیم؟ اگر شما developer مسئول application container هستید، این ممکن است در واقع یک solution خوب باشد. تطبیق کد یا container خود برای پیاده‌سازی یک interface consistent می‌تواند به‌خوبی کار کند.
    </p>
<p>
        با این حال، در بسیاری از موارد ما در حال استفاده مجدد از یک container تولید شده توسط یک party دیگر هستیم. در چنین مواردی، مشتق کردن یک image کمی اصلاح شده که ما مجبور به نگهداری آن هستیم (patch، rebase و غیره) به‌طور قابل توجهی گران‌تر از توسعه یک adapter container است که می‌تواند در کنار image party دیگر اجرا شود. علاوه بر این، decoupling adapter در container خود، امکان اشتراک‌گذاری و reuse را فراهم می‌کند، که هنگام تغییر application container، امکان‌پذیر نیست.
    </p>
<h4 style="font-weight: bold;">Hands On: Normalizing Different Logging Formats with Fluentd</h4>
<p>
        یکی از task های common برای یک adapter، نرمال‌سازی log metrics ها به یک مجموعه استاندارد از events است. بسیاری از applications های مختلف دارای formats خروجی متفاوتی هستند، اما شما می‌توانید از یک tool logging استاندارد که به‌عنوان adapter deploy شده است، برای نرمال‌سازی همه آنها به یک format consistent استفاده کنید. در این مثال، ما از عامل مانیتورینگ fluentd و همچنین برخی از plugins های پشتیبانی شده توسط community برای به دست آوردن logs ها از منابع مختلف استفاده خواهیم کرد.
    </p>
<p>
        fluentd یکی از محبوب‌ترین عوامل logging open source موجود است. یکی از ویژگی‌های اصلی آن، مجموعه‌ای غنی از plugins های پشتیبانی شده توسط community است که انعطاف‌پذیری زیادی را در monitoring انواع applications فراهم می‌کند.
    </p>
<p>
        اولین application که ما monitor خواهیم کرد Redis است. Redis یک key-value store محبوب است. یکی از دستوراتی که ارائه می‌دهد، دستور SLOWLOG است. این دستور، queries های اخیر را فهرست می‌کند که از یک بازه زمانی خاص فراتر رفته‌اند. چنین اطلاعاتی در debugging performance application شما بسیار مفید است. متأسفانه، SLOWLOG تنها به‌عنوان یک دستور در server Redis در دسترس است، به این معنی که استفاده از آن به‌صورت retrospectively، اگر مشکلی در زمانی که کسی برای debugging server در دسترس نیست، اتفاق بیفتد، دشوار است. برای رفع این محدودیت، ما می‌توانیم از fluentd و adapter pattern برای افزودن slow-query logging به Redis استفاده کنیم.
    </p>
<p>
        برای انجام این کار، ما از adapter pattern با یک container redis به‌عنوان main application container و container fluentd به‌عنوان adapter container خود استفاده می‌کنیم. در این مورد، ما همچنین از fluent-plugin-redis-slowlog fluentd plugin برای گوش دادن به queries های slow استفاده خواهیم کرد. ما می‌توانیم این plugin را با استفاده از snippet زیر پیکربندی کنیم:
    </p>
<pre><code class="language-yaml">&lt;source&gt;
  type redis_slowlog
  host localhost
  port 6379
  tag redis.slowlog
&lt;/source&gt;</code></pre>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 49" src="page_0049/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0049</div>
            </div>
        </div>
        <!-- Page 0050 -->
        <div class="chapter" id="page-0050">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
        از آنجایی که ما از یک adapter استفاده می‌کنیم و containers هر دو یک network namespace را به اشتراک می‌گذارند، پیکربندی logging، به‌سادگی از localhost و port پیش‌فرض Redis (6379) استفاده می‌کند.
    </p>
<p>
        با توجه به این application از adapter pattern، logging همیشه در صورت تمایل به debug کردن queries های slow Redis، در دسترس خواهد بود.
    </p>
<p>
        یک تمرین مشابه را می‌توان برای monitor کردن logsها از system Apache Storm انجام داد. باز هم، Storm داده‌ها را از طریق یک RESTful API ارائه می‌دهد، که مفید است اما اگر در حال حاضر system را هنگام وقوع یک مشکل، monitor نکنیم، محدودیت‌هایی دارد. مانند Redis، ما می‌توانیم از یک adapter fluentd برای تبدیل process Storm به یک time series از logs های قابل query استفاده کنیم. برای انجام این کار، ما یک adapter fluentd را با plugin fluent-plugin-storm فعال deploy می‌کنیم.
    </p>
<p>
        ما می‌توانیم این plugin را با یک config fluentd که به localhost اشاره می‌کند، پیکربندی کنیم (زیرا دوباره، ما به‌عنوان یک container group با یک localhost مشترک در حال اجرا هستیم). config این plugin به این صورت است:
    </p>
<pre><code class="language-yaml">&lt;source&gt;
  type storm
  tag storm
  url http://localhost:8080
  window 600
  sys 0
&lt;/source&gt;</code></pre>
<h4 style="font-weight: bold;">Adding a Health Monitor</h4>
<p>
        یک مثال آخر از اعمال adapter pattern از monitoring health یک application container مشتق شده است. task monitoring health یک database container off-the-shelf را در نظر بگیرید. در این مورد، container برای database توسط project database ارائه می‌شود، و ما ترجیح می‌دهیم که آن container را برای افزودن health checks، تغییر ندهیم. البته، یک container orchestrator به ما امکان می‌دهد تا health checks های ساده‌ای را اضافه کنیم تا اطمینان حاصل شود که process در حال اجرا است و به یک port خاص گوش می‌دهد، اما اگر بخواهیم health checks های غنی‌تری را اضافه کنیم که در واقع queries ها را در برابر database اجرا می‌کنند، چه؟
    </p>
<p>
        systemsهای container orchestration مانند Kubernetes ما را قادر می‌سازند تا از shell scripts به‌عنوان health checks نیز استفاده کنیم. با توجه به این قابلیت، ما می‌توانیم یک shell script غنی بنویسیم که تعدادی از queries های diagnostic مختلف را در برابر database اجرا می‌کند تا health آن را تعیین کند.
        اما ما می‌توانیم چنین script را در کجا ذخیره کنیم و چگونه می‌توانیم آن را version کنیم؟
    </p>
<p>
        پاسخ به این مشکلات اکنون باید آسان باشد: ما می‌توانیم از یک adapter container استفاده کنیم. database در application container اجرا می‌شود و یک network interface را با adapter container به اشتراک می‌گذارد. adapter container، یک container ساده است که فقط حاوی shell script برای تعیین health database است. سپس این script می‌تواند به‌عنوان health check برای database container تنظیم شود و می‌تواند هر health checks ای را که application ما نیاز دارد، انجام دهد. اگر این checks ها تا به حال شکست بخورند، database به‌طور خودکار راه‌اندازی مجدد می‌شود.
    </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0050</div>
            </div>
        </div>
        <!-- Page 0051 -->
        <div class="chapter" id="page-0051">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Hands On: Adding Rich Health Monitoring for MySQL</h3>
<p>فرض کنید شما می‌خواهید یک نظارت عمیق را به یک database از نوع MySQL اضافه کنید که در آن، شما واقعاً یک query را اجرا می‌کنید که نمایانگر workload شما است. در این مورد، یک گزینه این است که MySQL container را به‌روزرسانی کنید تا شامل یک health check باشد که مختص به application شما است. با این حال، این ایده به‌طور کلی جذاب نیست، زیرا مستلزم آن است که شما هم image پایه MySQL موجود را اصلاح کنید و هم آن image را هنگام انتشار images جدید MySQL به‌روزرسانی کنید.</p>
<p>استفاده از <strong>adapter pattern</strong> یک رویکرد بسیار جذاب‌تر برای افزودن health checks به database container شما است. به‌جای اصلاح MySQL container موجود، می‌توانید یک adapter container اضافی به MySQL container از قبل موجود اضافه کنید، که query مناسب را برای تست health database اجرا می‌کند. با توجه به اینکه این adapter container، HTTP health check مورد انتظار را پیاده‌سازی می‌کند، به سادگی تعریف health check از process مربوط به database MySQL، برحسب interface ارائه‌شده توسط این adapter database، است.</p>
<p>
   source code این adapter نسبتاً ساده است و در Go به این شکل است (اگرچه مشخص است که پیاده‌سازی‌های زبان‌های دیگر نیز امکان‌پذیر است):
  </p>
<pre><code class="language-go">
package main

import (
 "database/sql"
 "flag"
 "fmt"
 "net/http"
 _ "github.com/go-sql-driver/mysql"
)

var (
 user   = flag.String("user", "", "The database user name")
 passwd = flag.String("password", "", "The database password")
 db     = flag.String("database", "", "The database to connect to")
 query  = flag.String("query", "", "The test query")
 addr   = flag.String("address", "localhost:8080",
                    "The address to listen on")
)

// Basic usage:
//   db-check --query="SELECT * from my-cool-table" \
//            --user=bdburns \
//            --passwd="you wish"
//
func main() {
 flag.Parse()
 db, err := sql.Open("localhost",
                    fmt.Sprintf("%s:%s@/%s", *user, *passwd, *db))
 if err != nil {
  Adding a Health Monitor 
 | 
 37
 </code></pre>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0051</div>
            </div>
        </div>
        <!-- Page 0052 -->
        <div class="chapter" id="page-0052">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<pre><code class="language-go">
fmt.Printf("Error opening database: %v", err)
 
}
 // Simple web handler that runs the query
 
http.HandleFunc("", func(res http.ResponseWriter, req *http.Request) {
 
 
_, err := db.Exec(*query)
 
 
if err != nil {
 
 
 
res.WriteHeader(http.StatusInternalServerError)
 
 
 
res.Write([]byte(err.Error()))
 
 
 
return
 
 
}
 
 
res.WriteHeader(http.StatusOK)
 
 
res.Write([]byte("OK"))
 
 
return
 
})
 // Startup the server
 
http.ListenAndServe(*addr, nil)
}
 </code></pre>
</p>
<p>
   سپس می‌توانیم این را به یک container image بسازیم و آن را به یک pod که شبیه این است، بکشیم:
  </p>
<pre><code class="language-yaml">
apiVersion: v1
kind: Pod
metadata:
  name: adapter-example-health
  namespace: default
spec:
  containers:
  - image: mysql
    name: mysql
  - image: brendanburns/mysql-adapter
    name: adapter
 </code></pre>
<p>
   به این ترتیب، <em>mysql container</em> بدون تغییر باقی می‌ماند، اما بازخورد دلخواه درباره health از <em>mysql server</em> همچنان می‌تواند از <em>adapter container</em> به‌دست آید.
  </p>
<p>
   هنگامی که به این application از <strong>adapter pattern</strong> نگاه می‌کنیم، ممکن است به نظر برسد که اعمال این pattern غیرضروری است. بدیهی است که می‌توانستیم image سفارشی خودمان را بسازیم که می‌دانست چگونه health check را برای <em>mysql instance</em> خود انجام دهد.
  </p>
<p>
   در حالی که این درست است، این متد مزایای قوی‌ای را که از modularity ناشی می‌شود، نادیده می‌گیرد. اگر هر developer، container خاص خود را با health checking داخلی پیاده‌سازی کند، هیچ فرصتی برای استفاده مجدد یا اشتراک‌گذاری وجود ندارد.
  </p>
<p>
   در مقابل، اگر از الگوهایی مانند <strong>adapter</strong> برای توسعه راه‌حل‌های modular که شامل چندین container هستند استفاده کنیم، کار به‌طور ذاتی از هم جدا شده و به‌راحتی به اشتراک گذاشته می‌شود. یک adapter که برای health check از <em>mysql</em> توسعه داده شده است، یک module است که می‌تواند توسط افراد مختلف به اشتراک گذاشته و استفاده مجدد شود. علاوه‌براین، افراد می‌توانند <strong>adapter pattern</strong> را با استفاده از این health-checking container مشترک، بدون داشتن دانش عمیق در مورد چگونگی health check یک <em>mysql database</em>، اعمال کنند. بنابراین modularity و <strong>adapter pattern</strong> خدمت می‌کنند
  </p>
<p>
<strong>Chapter 4: Adapters</strong>
</p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0052</div>
            </div>
        </div>
        <!-- Page 0053 -->
        <div class="chapter" id="page-0053">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   نه تنها برای تسهیل اشتراک‌گذاری، بلکه برای توانمندسازی افراد برای بهره‌برداری از دانش دیگران.
  </p>
<p>
   گاهی اوقات <strong>design patterns</strong> فقط برای developersای که آن‌ها را اعمال می‌کنند نیستند، بلکه منجر به توسعه جوامعی می‌شوند که می‌توانند راه‌حل‌ها را بین اعضای community و همچنین اکوسیستم گسترده‌تر developer به اشتراک بگذارند و با یکدیگر همکاری کنند.
  </p>
<p>
   Adding a Health Monitor
   <br/>
   |
   <br/>
   39
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0053</div>
            </div>
        </div>
        <!-- Page 0055 -->
        <div class="chapter" id="page-0055">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h2>PART II</h2>
<h3>Serving Patterns</h3>
<p>
   فصل قبل، الگوهایی را برای grouping مجموعه‌ای از containers که روی یک machine زمان‌بندی شده‌اند، شرح داد. این گروه‌ها، سیستم‌های tightly coupled و همزیستی هستند. آن‌ها به منابع محلی و مشترک مانند دیسک، network interface یا ارتباطات بین فرآیندی وابسته هستند. چنین مجموعه‌ای از containers الگوهای مهمی هستند، اما آن‌ها همچنین building blocks برای سیستم‌های بزرگ‌تر نیز هستند.
   <strong>Reliability</strong>، <strong>scalability</strong> و <strong>separation of concerns</strong> حکم می‌کند که سیستم‌های دنیای واقعی از اجزای مختلفی ساخته شده‌اند که در سراسر machines متعدد پراکنده شده‌اند. در مقابل الگوهای تک‌گره، الگوهای توزیع‌شده multi-node بیشتر loosely coupled هستند. در حالی که الگوها، الگوهای ارتباطی بین اجزا را دیکته می‌کنند، این ارتباط بر اساس network calls است. علاوه‌براین، بسیاری از callها به‌صورت موازی صادر می‌شوند و سیستم‌ها از طریق synchronization شل (loose) و نه محدودیت‌های سخت (tight) هماهنگ می‌شوند.
  </p>
<h4>
   Introduction to Microservices
  </h4>
<p>
   اخیراً، اصطلاح <strong>microservices</strong> به یک buzzword برای توصیف معماری‌های نرم‌افزاری توزیع‌شده multi-node تبدیل شده است. <strong>Microservices</strong> سیستمی را توصیف می‌کند که از اجزای مختلفی تشکیل شده است که در فرآیندهای مختلف در حال اجرا هستند و از طریق <strong>APIs</strong> تعریف شده با یکدیگر ارتباط برقرار می‌کنند. <strong>Microservices</strong> در مقابل سیستم‌های <strong>monolithic</strong> قرار دارند، که تمایل دارند تمام functionality یک service را در یک application واحد و tightly coordinated قرار دهند. این دو رویکرد معماری متفاوت در شکل II-1 و II-2 نشان داده شده‌اند.
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0055</div>
            </div>
        </div>
        <!-- Page 0056 -->
        <div class="chapter" id="page-0056">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure II-1. A monolithic service with all functions in a single container</h3>
<p>Figure II-2. A microservice architecture with each function broken out as a separate 
  <strong>microservice</strong></p>
<p>مزایای متعددی برای رویکرد <strong>microservices</strong> وجود دارد، که بیشتر آنها حول محور قابلیت اطمینان و چابکی متمرکز شده‌اند. 
   <strong>Microservices</strong> یک application را به قطعات کوچک تقسیم می‌کند که هر کدام بر ارائه یک <strong>service</strong> واحد متمرکز شده‌اند. 
   این محدوده کاهش یافته به هر <strong>service</strong> این امکان را می‌دهد که توسط یک تیم "دو پیتزایی" واحد ساخته و نگهداری شود. 
   کاهش اندازه تیم همچنین سربار مرتبط با متمرکز نگه داشتن یک تیم و حرکت در یک جهت را کاهش می‌دهد.</p>
<p>علاوه بر این، معرفی <strong>APIs</strong> رسمی در بین <strong>microservices</strong> مختلف، تیم‌ها را از یکدیگر جدا می‌کند و یک قرارداد قابل اعتماد بین <strong>services</strong> مختلف ارائه می‌دهد. 
   این قرارداد رسمی، نیاز به هماهنگی تنگاتنگ بین تیم‌ها را کاهش می‌دهد، زیرا تیمی که <strong>API</strong> را ارائه می‌کند، سطح پایدار مورد نیاز را درک می‌کند و تیمی که از <strong>API</strong> استفاده می‌کند، می‌تواند بدون نگرانی در مورد جزئیات آن، به یک <strong>service</strong> پایدار تکیه کند. 
   این <strong>decoupling</strong> به تیم‌ها این امکان را می‌دهد که به طور مستقل 
   کد و برنامه‌های انتشار خود را مدیریت کنند، که به نوبه خود توانایی هر تیم را برای تکرار و بهبود کد خود افزایش می‌دهد.</p>
<p>در نهایت، <strong>decoupling</strong> از <strong>microservices</strong>، مقیاس‌پذیری بهتری را امکان‌پذیر می‌کند. 
   از آنجایی که هر <strong>component</strong> به <strong>service</strong> خود تقسیم شده است، می‌تواند به طور مستقل مقیاس‌بندی شود. 
   به ندرت اتفاق می‌افتد که هر <strong>service</strong> در یک application بزرگتر با همان سرعت رشد کند یا داشته باشد</p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 56" src="page_0056/image_1.png"/></div>
<div class="page-image"><img alt="Image from page 56" src="page_0056/image_2.png"/></div>
</div>
                <div class="page-number">صفحه 0056</div>
            </div>
        </div>
        <!-- Page 0057 -->
        <div class="chapter" id="page-0057">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   همان روش <strong>scaling</strong> را دارند. برخی از <strong>systems</strong> ها 
   <strong>stateless</strong> هستند و می‌توانند به سادگی به صورت 
   <strong>horizontally</strong> مقیاس شوند، در حالی که سایر 
   <strong>systems</strong> ها <strong>state</strong> را حفظ می‌کنند و به <strong>sharding</strong> یا رویکردهای دیگری برای 
   <strong>scale</strong> نیاز دارند. با جدا کردن هر <strong>service</strong>، هر <strong>service</strong> می‌تواند از رویکرد 
   <strong>scaling</strong> که برایش مناسب است، استفاده کند. این امر زمانی که همه <strong>services</strong> بخشی از یک 
   <strong>monolith</strong> واحد هستند، امکان‌پذیر نیست.
  </p>
<p>
   اما البته معایبی نیز برای رویکرد <strong>microservices</strong> در طراحی <strong>system</strong> وجود دارد. 
   دو عیب اصلی این است که به دلیل اینکه <strong>system</strong>
<strong>loosely coupled</strong> شده است، 
   <strong>debugging</strong> سیستم در هنگام وقوع خرابی، به طور قابل توجهی دشوارتر می‌شود. 
   شما دیگر نمی‌توانید به سادگی یک application واحد را در یک 
   <strong>debugger</strong> بارگذاری کنید و تعیین کنید چه چیزی اشتباه رخ داده است. هر گونه خطا، محصولات جانبی تعداد زیادی <strong>system</strong> است که اغلب بر روی ماشین‌های مختلف اجرا می‌شوند. 
   این محیط، بازتولید در یک <strong>debugger</strong> را کاملاً چالش برانگیز می‌کند. 
   به عنوان یک نتیجه، سیستم‌های مبتنی بر <strong>microservices</strong> نیز دشوار برای طراحی و معماری هستند. 
   یک <strong>system</strong> مبتنی بر <strong>microservices</strong> از روش‌های متعددی برای برقراری ارتباط بین 
   <strong>services</strong> استفاده می‌کند. الگوهای مختلف (به عنوان مثال، <strong>synchronous, asynchronous, message-passing</strong> و غیره)؛ و الگوهای مختلف 
   <strong>coordination</strong> و <strong>control</strong> بین <strong>services</strong>.
  </p>
<p>
   این چالش‌ها، انگیزه برای <strong>distributed patterns</strong> هستند. اگر یک معماری <strong>microservices</strong> از الگوهای شناخته شده تشکیل شده باشد، طراحی آن آسان‌تر است، زیرا بسیاری از 
   <strong>design practices</strong> توسط الگوها مشخص شده‌اند. 
   علاوه بر این، الگوها، <strong>systems</strong> را برای 
   <strong>debug</strong> آسان‌تر می‌کنند، زیرا به 
   <strong>developers</strong> این امکان را می‌دهند که درس‌های آموخته شده را در تعدادی از <strong>systems</strong> مختلف که از الگوهای یکسان استفاده می‌کنند، اعمال کنند.
  </p>
<p>
   با در نظر گرفتن این موضوع، این بخش تعدادی از الگوهای چند 
   <strong>node</strong> را برای ساخت <strong>systems</strong> توزیع شده معرفی می‌کند. این الگوها منحصراً متقابل نیستند. هر <strong>system</strong> دنیای واقعی از مجموعه‌ای از این الگوها ساخته خواهد شد که با هم کار می‌کنند تا یک application سطح بالاتر را تولید کنند.
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0057</div>
            </div>
        </div>
        <!-- Page 0059 -->
        <div class="chapter" id="page-0059">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>CHAPTER 5</h3>
<h4>Replicated Load-Balanced Services</h4>
<p>
   ساده‌ترین <strong>distributed pattern</strong>، و الگویی که اکثر افراد با آن آشنا هستند، یک <strong>replicated load-balanced service</strong> است. در چنین 
   <strong>service</strong>، هر <strong>server</strong> با هر 
   <strong>server</strong> دیگری یکسان است و همه قادر به پشتیبانی از ترافیک هستند. این <strong>pattern</strong> از تعدادی <strong>scalable</strong> از 
   <strong>servers</strong> با یک <strong>load balancer</strong> در جلوی آنها تشکیل شده است. 
   <strong>load balancer</strong> معمولاً کاملاً 
   <strong>round-robin</strong> است یا از نوعی <strong>session stickiness</strong> استفاده می‌کند. این فصل یک مثال ملموس از نحوه استقرار چنین 
   <strong>service</strong> در 
   <strong>Kubernetes</strong> ارائه می‌دهد.
  </p>
<p>
<strong>Stateless Services</strong>
</p>
<p>
<strong>Stateless services</strong>، <strong>services</strong> هایی هستند که برای عملکرد صحیح نیازی به <strong>state</strong> ذخیره شده ندارند. در ساده‌ترین 
   <strong>stateless applications</strong>، حتی 
   <strong>requests</strong> فردی ممکن است به نمونه‌های جداگانه 
   <strong>service</strong> هدایت شوند (به شکل 5-1 مراجعه کنید). نمونه‌هایی از 
   <strong>stateless services</strong> شامل مواردی مانند 
   <strong>static content servers</strong> و 
   <strong>middleware systems</strong> پیچیده است که پاسخ‌ها را از تعداد زیادی <strong>backend systems</strong> مختلف دریافت و تجمیع می‌کنند.
  </p>
<p>
   Figure 5-1. Basic replicated stateless service
  </p>
<p>45</p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 59" src="page_0059/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0059</div>
            </div>
        </div>
        <!-- Page 0060 -->
        <div class="chapter" id="page-0060">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<strong>Stateless systems</strong> برای ارائه 
   <strong>redundancy</strong> و 
   <strong>scale</strong>، <strong>replicated</strong> می‌شوند. 
   صرف نظر از اینکه <strong>service</strong> شما چقدر کوچک است، برای ارائه یک 
   <strong>service</strong> با یک توافقنامه سطح <strong>service</strong> (SLA) با قابلیت اطمینان بالا، حداقل به دو 
   <strong>replica</strong> نیاز دارید. برای درک دلیل این امر، تلاش برای ارائه سه نه (99.9٪ <strong>availability</strong>) را در نظر بگیرید. در یک 
   <strong>service</strong> سه نه، 1.4 دقیقه <strong>downtime</strong> در روز دارید (24 × 60 × 0.001). با فرض اینکه 
   <strong>service</strong> دارید که هرگز 
   <strong>crashes</strong> نمی‌شود، هنوز هم به این معنی است که برای رسیدن به 
   <strong>SLA</strong> خود با یک 
   <strong>instance</strong> واحد، باید بتوانید یک 
   <strong>software upgrade</strong> را در کمتر از 1.4 دقیقه انجام دهید. و این فرض بر این است که شما 
   <strong>daily software rollouts</strong> را انجام می‌دهید. اگر تیم شما واقعاً 
   <strong>continuous delivery</strong> را در آغوش گرفته است و شما هر ساعت یک نسخه جدید از <strong>software</strong> را منتشر می‌کنید، برای دستیابی به 
   <strong>SLA</strong> 99.9٪ 
   <strong>uptime</strong> خود با یک 
   <strong>instance</strong> واحد، باید بتوانید یک <strong>software rollout</strong> را در 3.6 ثانیه انجام دهید. اگر زمان بیشتری طول بکشد، 
   <strong>downtime</strong> شما بیش از 0.01٪ خواهد بود.
  </p>
<p>
   البته، به جای انجام تمام این کارها، می‌توانید فقط دو 
   <strong>replica</strong> از 
   <strong>service</strong> خود داشته باشید که یک 
   <strong>load balancer</strong> در جلوی آنها قرار دارد. به این ترتیب، در حالی که در حال انجام یک 
   <strong>rollout</strong> هستید، یا در رویدادی بعید -من مطمئن هستم - که <strong>software</strong> شما 
   <strong>crashes</strong> می‌شود، <strong>users</strong> شما توسط 
   <strong>replica</strong> دیگر 
   <strong>service</strong> ارائه می‌شوند و هرگز متوجه نمی‌شوند که چه اتفاقی افتاده است.
  </p>
<p>
   همانطور که <strong>services</strong> بزرگتر می‌شوند، برای پشتیبانی از <strong>users</strong> اضافی نیز 
   <strong>replicated</strong> می‌شوند. <strong>Systems</strong> با قابلیت <strong>horizontal scaling</strong> با افزودن 
   <strong>replicas</strong> بیشتر، 
   <strong>users</strong> بیشتری را مدیریت می‌کنند. به شکل 5-2 مراجعه کنید. آنها این کار را با 
   <strong>load-balanced replicated serving pattern</strong> انجام می‌دهند.
  </p>
<p>
   Figure 5-2. Horizontal scaling of a replicated stateless application
  </p>
<p>
<strong>Readiness Probes for Load Balancing</strong>
</p>
<p>
   البته، به سادگی <strong>replicating</strong>
<strong>service</strong> و افزودن یک 
   <strong>load balancer</strong> تنها بخشی از یک <strong>pattern</strong> کامل برای 
   <strong>stateless replicated serving</strong> است. هنگام طراحی یک 
   <strong>replicated service</strong>، ساخت و استقرار یک 
   <strong>readiness probe</strong> برای اطلاع رسانی به 
   <strong>load balancer</strong> به همان اندازه مهم است. ما در مورد چگونگی استفاده از 
   <strong>health probes</strong> توسط یک سیستم 
   <strong>container orchestration</strong> برای تعیین زمان راه‌اندازی مجدد یک application بحث کرده‌ایم. در مقابل، یک 
   <strong>readiness probe</strong> تعیین می‌کند که چه زمانی یک application آماده سرویس دهی به 
   <strong>user requests</strong> است. دلیل این تمایز این است که بسیاری از applications برای آماده شدن برای سرویس دهی، به زمان نیاز دارند. آنها ممکن است نیاز داشته باشند که به <strong>databases</strong> متصل شوند، 
   <strong>plugins</strong> را بارگیری کنند، یا فایل‌های 
   <strong>serving</strong> را از <strong>network</strong> دانلود کنند. در همه این موارد، 
   <strong>containers</strong> زنده هستند، اما آماده نیستند. هنگام ساخت یک application برای یک 
   <strong>replicated service pattern</strong>، مطمئن شوید که یک <strong>URL</strong> ویژه شامل این 
   <strong>readiness check</strong> است.
  </p>
<p>46 | Chapter 5: Replicated Load-Balanced Services</p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 60" src="page_0060/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0060</div>
            </div>
        </div>
        <!-- Page 0061 -->
        <div class="chapter" id="page-0061">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>Hands On: Creating a Replicated Service in Kubernetes</strong></h3>
<p>
   دستورالعمل‌های زیر یک مثال مشخص از نحوه استقرار یک service بدون حالت (state-less) و replicated را در پشت یک load balancer ارائه می‌دهند. این دستورالعمل‌ها از container orchestrator به نام Kubernetes استفاده می‌کنند، اما این الگو می‌تواند بر روی تعدادی از container orchestrators مختلف پیاده‌سازی شود.
  </p>
<p>
   برای شروع، ما یک application کوچک NodeJS ایجاد می‌کنیم که تعاریف کلمات را از فرهنگ لغت ارائه می‌دهد.
  </p>
<p>
   برای امتحان کردن این service، می‌توانید آن را با استفاده از یک image از نوع container اجرا کنید:
  </p>
<pre>
   <code class="language-bash">docker run -p 8080:8080 brendanburns/dictionary-server
   </code>
  </pre>
<p>
   این دستور یک dictionary server ساده را بر روی machine محلی شما اجرا می‌کند. به عنوان مثال، می‌توانید به آدرس http://localhost:8080/dog مراجعه کنید تا تعریف کلمه dog را ببینید.
  </p>
<p>
   اگر به logs مربوط به container نگاه کنید، خواهید دید که بلافاصله شروع به ارائه service می‌کند، اما تنها پس از دانلود شدن dictionary (که تقریباً 8 MB است) از طریق network، readiness را گزارش می‌دهد.
  </p>
<p>
   برای استقرار این service در Kubernetes، یک Deployment ایجاد می‌کنید:
  </p>
<pre>
   <code class="language-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: dictionary-server
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: dictionary-server
    spec:
      containers:
      - name: server
        image: brendanburns/dictionary-server
        ports:
        - containerPort: 8080
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
   </code>
  </pre>
<p>
   شما می‌توانید این service بدون حالت (stateless) و replicated را با دستور زیر ایجاد کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f dictionary-deploy.yaml
   </code>
  </pre>
<p>
   اکنون که تعدادی replica دارید، به یک load balancer نیاز دارید تا requests ها را به replicas شما برساند. load balancer برای توزیع بار (load) و همچنین ارائه... سرویس‌های بدون حالت (Stateless Services)
   47
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0061</div>
            </div>
        </div>
        <!-- Page 0062 -->
        <div class="chapter" id="page-0062">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>abstraction برای جدا کردن service replicated از مصرف‌کنندگان service</strong></h3>
<p>
   load balancer همچنین یک نام قابل‌resolve ارائه می‌دهد که مستقل از هر یک از replicas های خاص است.
  </p>
<p>
   با Kubernetes، شما می‌توانید این load balancer را با یک object از نوع Service ایجاد کنید:
  </p>
<pre>
   <code class="language-yaml">kind: Service
apiVersion: v1
metadata:
  name: dictionary-server-service
spec:
  selector:
    app: dictionary-server
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
   </code>
  </pre>
<p>
   هنگامی که فایل configuration را دارید، می‌توانید service مربوط به dictionary را با دستور زیر ایجاد کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f dictionary-service.yaml
   </code>
  </pre>
<h4><strong>Session Tracked Services</strong></h4>
<p>
   مثال‌های قبلی از الگوی replicated stateless، requests ها را از همه users به همه replicas های یک service هدایت می‌کرد. در حالی که این امر توزیع بار (load) و fault tolerance را تضمین می‌کند، همیشه راه‌حل ترجیحی نیست. اغلب دلایلی وجود دارد که بخواهید اطمینان حاصل کنید که requests های یک user خاص همیشه به همان machine ختم می‌شوند. گاهی اوقات این به این دلیل است که شما data های آن user را در حافظه cache می‌کنید، بنابراین قرار گرفتن روی همان machine، نرخ hit cache بالاتری را تضمین می‌کند. گاهی اوقات به این دلیل است که تعامل، ماهیت طولانی مدتی دارد، بنابراین مقداری state بین requests حفظ می‌شود. صرف نظر از دلیل، یک تطابق از الگوی service replicated stateless استفاده از session tracked services است، که اطمینان حاصل می‌کند که همه requests ها برای یک user واحد به همان replica نگاشت می‌شوند، همانطور که در Figure 5-3 نشان داده شده است.
  </p>
<p>
<em>Figure 5-3. A session tracked service where all requests for a specific user are routed to a single instance</em>
</p>
<p>
   48
   | Chapter 5: Replicated Load-Balanced Services
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 62" src="page_0062/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0062</div>
            </div>
        </div>
        <!-- Page 0063 -->
        <div class="chapter" id="page-0063">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   به طور کلی، این session tracking با hashing کردن source و destination IP addresses انجام می‌شود و از آن key برای شناسایی server ای که باید requests ها را service دهد، استفاده می‌شود. تا زمانی که source و destination IP addresses ثابت بمانند، همه requests ها به همان replica ارسال می‌شوند.
  </p>
<p>
   IP-based session tracking در داخل یک cluster (IP های داخلی) کار می‌کند اما به طور کلی با IP addresses های خارجی به دلیل network address translation (NAT) به خوبی کار نمی‌کند. برای external session tracking، application-level tracking (به عنوان مثال، از طریق cookies) ترجیح داده می‌شود.
  </p>
<p>
   اغلب، session tracking از طریق یک consistent hashing function انجام می‌شود. مزیت یک consistent hashing function زمانی آشکار می‌شود که service به بالا یا پایین scale می‌شود. بدیهی است، هنگامی که تعداد replicas تغییر می‌کند، mapping یک user خاص به یک replica ممکن است تغییر کند. Consistent hashing functions تعداد users هایی را که واقعاً replica ای که به آن map شده‌اند را تغییر می‌دهند، به حداقل می‌رسانند، که تأثیر scaling بر application شما را کاهش می‌دهد.
  </p>
<h4><strong>Application-Layer Replicated Services</strong></h4>
<p>
   در تمام مثال‌های قبلی، replication و load balancing در لایه network service انجام می‌شود. load balancing مستقل از protocol واقعی است که بر روی network صحبت می‌شود، فراتر از TCP/IP. با این حال، بسیاری از applications از HTTP به عنوان protocol برای صحبت با یکدیگر استفاده می‌کنند، و دانش protocol application که در حال صحبت است، اصلاحات بیشتری را برای الگوی serving stateless replicated برای قابلیت‌های اضافی امکان‌پذیر می‌کند.
  </p>
<h4><strong>Introducing a Caching Layer</strong></h4>
<p>
   گاهی اوقات code در service stateless شما با وجود stateless بودن همچنان expensive است. ممکن است برای service requests ها، query هایی را به یک database انجام دهد یا مقدار قابل توجهی rendering یا data mixing را برای service request انجام دهد. در چنین دنیایی، یک caching layer می‌تواند حس بسیار خوبی داشته باشد. یک cache بین application stateless شما و request end-user وجود دارد. ساده‌ترین شکل caching برای web applications، یک caching web proxy است. caching proxy به سادگی یک HTTP server است که requests های user را در state حافظه نگه می‌دارد. اگر دو user یک web page یکسان را request کنند، فقط یک request به backend شما می‌رود. دیگری از حافظه cache service می‌شود. این در Figure 5-4 نشان داده شده است.
  </p>
<p>
   Application-Layer Replicated Services
   | 49
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 63" src="page_0063/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0063</div>
            </div>
        </div>
        <!-- Page 0064 -->
        <div class="chapter" id="page-0064">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 5-4. The operation of a cache server</em>
</p>
<p>
   برای اهداف ما، ما از Varnish، یک web cache متن‌باز (open source) استفاده خواهیم کرد.
  </p>
<h4><strong>Deploying Your Cache</strong></h4>
<p>
   ساده‌ترین راه برای استقرار web cache، در کنار هر instance از web server شما با استفاده از الگو sidecar است (به Figure 5-5 مراجعه کنید).
  </p>
<p>
<em>Figure 5-5. Adding the web cache server as a sidecar</em>
</p>
<p>
   اگرچه این رویکرد ساده است، اما دارای برخی معایب است، یعنی اینکه شما باید cache خود را در همان مقیاس web servers خود scale کنید. این اغلب رویکردی نیست که شما می‌خواهید. برای cache خود، شما می‌خواهید تا حد امکان replica های کمی با منابع زیاد برای هر replica داشته باشید (به عنوان مثال، به جای 10 replica با 1 GB از RAM در هر کدام، شما می‌خواهید دو replica با 5 GB از RAM در هر کدام). برای درک دلیل این که چرا این ترجیح داده می‌شود، در نظر بگیرید که هر صفحه در هر replica ذخیره می‌شود. با 10 replicas، شما هر صفحه را 10 بار ذخیره می‌کنید، که مجموعه کلی صفحات را که می‌توانید در حافظه cache نگه دارید، کاهش می‌دهد. این باعث کاهش hit rate می‌شود، کسری از زمان که یک request می‌تواند از cache service شود، که به نوبه خود، utility cache را کاهش می‌دهد.
  </p>
<p>
   اگرچه شما می‌خواهید چند cache بزرگ داشته باشید، ممکن است تعداد زیادی replica کوچک از web servers خود نیز بخواهید. بسیاری از زبان‌ها (به عنوان مثال، NodeJS) واقعاً فقط می‌توانند از یک core استفاده کنند، و بنابراین شما می‌خواهید replicas های زیادی داشته باشید تا بتوانید از چندین core استفاده کنید، حتی در همان machine. بنابراین، منطقی‌ترین کار این است که caching layer خود را به عنوان یک tier service stateless replicated دوم در بالای tier web-serving خود پیکربندی کنید، همانطور که در Figure 5-6 نشان داده شده است.
  </p>
<p>
   50
   | Chapter 5: Replicated Load-Balanced Services
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 64" src="page_0064/image_1.png"/></div>
<div class="page-image"><img alt="Image from page 64" src="page_0064/image_2.png"/></div>
</div>
                <div class="page-number">صفحه 0064</div>
            </div>
        </div>
        <!-- Page 0065 -->
        <div class="chapter" id="page-0065">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 5-6. Adding the caching layer to our replicated service</em>
</p>
<p>
   مگر اینکه مراقب باشید، caching می‌تواند session tracking را خراب کند. دلیل این امر این است که اگر شما از affinity آدرس IP پیش‌فرض و load balancing استفاده کنید، تمام requests ها از آدرس‌های IP cache ارسال می‌شوند، نه از end user service شما. اگر شما از توصیه‌های قبلی پیروی کرده‌اید و چند cache بزرگ مستقر کرده‌اید، affinity مبتنی بر آدرس IP شما ممکن است در واقع به این معنی باشد که برخی از replicas های لایه web شما هیچ ترافیکی را نمی‌بینند. در عوض، شما باید از چیزی مانند cookie یا HTTP header برای session tracking استفاده کنید.
  </p>
<h4><strong>Hands On: Deploying the Caching Layer</strong></h4>
<p>
   service dictionary-server که قبلاً ساختیم، ترافیک را به dictionary server توزیع می‌کند و به عنوان نام DNS dictionary-server-service قابل شناسایی است. این pattern در Figure 5-7 نشان داده شده است.
  </p>
<p>
   Introducing a Caching Layer
   | 51
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 65" src="page_0065/image_1.png"/></div>
<div class="page-image"><img alt="Image from page 65" src="page_0065/image_2.png"/></div>
</div>
                <div class="page-number">صفحه 0065</div>
            </div>
        </div>
        <!-- Page 0066 -->
        <div class="chapter" id="page-0066">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 5-7. Adding a caching layer to the dictionary server</em>
</p>
<p>
   ما می‌توانیم این را با configuration Varnish cache زیر شروع به ساختن کنیم:
  </p>
<pre>
   <code class="language-vcl">vcl 4.0;
backend default {
  .host = "dictionary-server-service";
  .port = "8080";
}
   </code>
  </pre>
<p>
   یک object از نوع ConfigMap ایجاد کنید تا این configuration را نگه دارید:
  </p>
<pre>
   <code class="language-bash">kubectl create configmap varnish-config --from-file=default.vcl
   </code>
  </pre>
<p>
   اکنون می‌توانیم cache Varnish replicated را مستقر کنیم، که این configuration را بارگذاری می‌کند:
  </p>
<pre>
   <code class="language-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: varnish-cache
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: varnish-cache
    spec:
      containers:
      - name: cache
        resources:
          requests:
            # We'll use two gigabytes for each varnish cache
            memory: 2Gi
   </code>
  </pre>
<p>
   52
   | Chapter 5: Replicated Load-Balanced Services
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 66" src="page_0066/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0066</div>
            </div>
        </div>
        <!-- Page 0067 -->
        <div class="chapter" id="page-0067">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre>
   <code class="language-yaml">          image: brendanburns/varnish
        command:
        - varnishd
        - -F
        - -f
        - /etc/varnish-config/default.vcl
        - -a
        - 0.0.0.0:8080
        - -s
        # This memory allocation should match the memory request above
        - malloc,2G
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: varnish
          mountPath: /etc/varnish-config
      volumes:
      - name: varnish
        configMap:
          name: varnish-config
   </code>
  </pre>
<p>
   شما می‌توانید servers های Varnish replicated را با دستور زیر مستقر کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f varnish-deploy.yaml
   </code>
  </pre>
<p>
   و در نهایت یک load balancer برای این Varnish cache مستقر کنید:
  </p>
<pre>
   <code class="language-yaml">kind: Service
apiVersion: v1
metadata:
  name: varnish-service
spec:
  selector:
    app: varnish-cache
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
   </code>
  </pre>
<p>
   که می‌توانید با دستور زیر ایجاد کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f varnish-service.yaml
   </code>
  </pre>
<h4><strong>Expanding the Caching Layer</strong></h4>
<p>
   اکنون که ما یک caching layer را به service stateless و replicated خود وارد کرده‌ایم، بیایید نگاهی به این بیندازیم که این layer می‌تواند فراتر از caching استاندارد ارائه دهد. HTTP reverse proxies مانند Varnish به طور کلی pluggable هستند و می‌توانند تعدادی از ویژگی‌های پیشرفته را ارائه دهند که فراتر از caching مفید هستند.
  </p>
<p>
   Expanding the Caching Layer
   | 53
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0067</div>
            </div>
        </div>
        <!-- Page 0068 -->
        <div class="chapter" id="page-0068">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4><strong>Rate Limiting and Denial-of-Service Defense</strong></h4>
<p>
   تعداد کمی از ما سایت‌هایی را با این انتظار می‌سازیم که با یک denial-of-service attack مواجه شویم. اما با ساخت API های بیشتر و بیشتر، یک denial of service می‌تواند به سادگی از یک developer که یک client را به درستی پیکربندی نکرده یا یک site-reliability engineer که به طور تصادفی یک load test را در برابر یک نصب production اجرا می‌کند، ناشی شود. بنابراین، منطقی است که دفاع عمومی denial-of-service را از طریق rate limiting به caching layer اضافه کنیم. اکثر HTTP reverse proxies ها مانند Varnish دارای قابلیت‌هایی در این راستا هستند. به طور خاص، Varnish یک module به نام throttle دارد که می‌تواند برای ارائه throttling بر اساس آدرس IP و request path و همچنین اینکه آیا یک user وارد سیستم شده است یا خیر، پیکربندی شود.
  </p>
<p>
   اگر در حال استقرار یک API هستید، به طور کلی بهترین روش این است که یک rate limit نسبتاً کوچک برای دسترسی anonymous داشته باشید و سپس users را مجبور کنید تا وارد سیستم شوند تا یک rate limit بالاتر دریافت کنند. نیاز به login حسابرسی را فراهم می‌کند تا مشخص شود چه کسی مسئول load پیش‌بینی نشده است، و همچنین یک مانع برای مهاجمان احتمالی ارائه می‌دهد که نیاز به دریافت چندین identity برای راه‌اندازی یک attack موفق دارند.
  </p>
<p>
   هنگامی که یک user به rate limit می‌رسد، server، HTTP status code 429 را برمی‌گرداند که نشان می‌دهد requests های زیادی صادر شده است. با این حال، بسیاری از users می‌خواهند بدانند که قبل از رسیدن به آن limit، چند request دیگر دارند. برای این منظور، احتمالاً می‌خواهید یک HTTP header را با اطلاعات remaining-calls پر کنید. اگرچه یک header استاندارد برای برگرداندن این data وجود ندارد، بسیاری از APIs مقداری variation از X-RateLimit-Remaining را برمی‌گردانند.
  </p>
<h4><strong>SSL Termination</strong></h4>
<p>
   علاوه بر انجام caching برای performance، یکی دیگر از وظایف رایج انجام شده توسط edge layer، SSL termination است. حتی اگر قصد دارید از SSL برای ارتباط بین لایه‌ها در cluster خود استفاده کنید، همچنان باید از certificates های مختلف برای edge و services های داخلی خود استفاده کنید. در واقع، هر service internal باید از certificate خود استفاده کند تا اطمینان حاصل شود که هر layer می‌تواند به طور مستقل ارائه شود. متأسفانه، web cache از نوع Varnish نمی‌تواند برای SSL termination استفاده شود، اما خوشبختانه، application از نوع nginx می‌تواند. بنابراین ما می‌خواهیم یک لایه سوم به الگوی application stateless خود اضافه کنیم، که یک لایه replicated از servers های nginx خواهد بود که SSL termination را برای ترافیک HTTPS مدیریت کرده و ترافیک را به cache Varnish ما ارسال می‌کند.
  </p>
<p>
   ترافیک HTTP همچنان به web cache از نوع Varnish منتقل می‌شود و Varnish ترافیک را به application web ما ارسال می‌کند، همانطور که در Figure 5-8 نشان داده شده است.
  </p>
<p>
   54
   | Chapter 5: Replicated Load-Balanced Services
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0068</div>
            </div>
        </div>
        <!-- Page 0069 -->
        <div class="chapter" id="page-0069">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 5-8. Complete replicated stateless serving example</em>
</p>
<h4><strong>Hands On: Deploying nginx and SSL Termination</strong></h4>
<p>
   دستورالعمل‌های زیر نحوه اضافه کردن یک nginx با SSL terminating replicated را به service و cache replicated که قبلاً مستقر کرده‌ایم، شرح می‌دهند.
  </p>
<p>
   این دستورالعمل‌ها فرض می‌کنند که شما یک certificate دارید. اگر نیاز به دریافت یک certificate دارید، ساده‌ترین راه برای انجام این کار از طریق ابزارهای Let’s Encrypt است. متناوباً، می‌توانید از ابزار openssl برای ایجاد آن‌ها استفاده کنید. دستورالعمل‌های زیر فرض می‌کنند که شما آنها را server.crt (public certificate) و server.key (private key on the server) نامیده‌اید. چنین certificates های خودامضا، باعث ایجاد security alerts در web browsers های مدرن می‌شوند و هرگز نباید برای production استفاده شوند.
  </p>
<p>
   اولین قدم این است که certificate خود را به عنوان یک secret به Kubernetes آپلود کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create secret tls ssl --cert=server.crt --key=server.key
   </code>
  </pre>
<p>
   هنگامی که certificate خود را به عنوان یک secret آپلود کردید، باید یک configuration از نوع nginx برای ارائه SSL ایجاد کنید:
  </p>
<pre>
   <code class="language-nginx">events {
  worker_connections  1024;
}
http {
  server {
    listen 443 ssl;
    server_name my-domain.com www.my-domain.com;
   </code>
  </pre>
<p>
   Expanding the Caching Layer
   | 55
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 69" src="page_0069/image_1.png"/></div>
<div class="page-image"><img alt="Image from page 69" src="page_0069/image_2.png"/></div>
</div>
                <div class="page-number">صفحه 0069</div>
            </div>
        </div>
        <!-- Page 0070 -->
        <div class="chapter" id="page-0070">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre>
   <code class="language-nginx">       ssl on;
    ssl_certificate         /etc/certs/tls.crt;
    ssl_certificate_key     /etc/certs/tls.key;
    location / {
        proxy_pass http://varnish-service:80;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Real-IP $remote_addr;
    }
  }
}
   </code>
  </pre>
<p>
   همانند Varnish، شما نیاز دارید این را به یک object از نوع ConfigMap تبدیل کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create configmap nginx-conf --from-file=nginx.conf
   </code>
  </pre>
<p>
   اکنون که شما یک secret و یک configuration از نوع nginx دارید، زمان آن فرا رسیده است که لایه nginx replicated و stateless را ایجاد کنید:
  </p>
<pre>
   <code class="language-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ssl
spec:
  replicas: 4
  template:
    metadata:
      labels:
        app: nginx-ssl
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 443
        volumeMounts:
        - name: conf
          mountPath: /etc/nginx
        - name: certs
          mountPath: /etc/certs
      volumes:
      - name: conf
        configMap:
          # This is the ConfigMap for nginx we created previously
          name: nginx-conf
      - name: certs
        secret:
          # This is the secret we created above
          secretName: ssl
   </code>
  </pre>
<p>
   56
   | Chapter 5: Replicated Load-Balanced Services
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0070</div>
            </div>
        </div>
        <!-- Page 0071 -->
        <div class="chapter" id="page-0071">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   برای ایجاد servers های nginx replicated، شما از دستور زیر استفاده می‌کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f nginx-deploy.yaml
   </code>
  </pre>
<p>
   در نهایت، شما می‌توانید این server nginx SSL را با یک service در معرض دید قرار دهید:
  </p>
<pre>
   <code class="language-yaml">kind: Service
apiVersion: v1
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx-ssl
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 443
      targetPort: 443
   </code>
  </pre>
<p>
   برای ایجاد این service load-balancing، دستور زیر را اجرا کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f nginx-service.yaml
   </code>
  </pre>
<p>
   اگر شما این service را در یک cluster از نوع Kubernetes ایجاد کنید که از external load balancers پشتیبانی می‌کند، این service یک service public externalized ایجاد می‌کند که ترافیک را در یک IP address public service می‌دهد.
  </p>
<p>
   برای دریافت این IP address، شما می‌توانید دستور زیر را اجرا کنید:
  </p>
<pre>
   <code class="language-bash">kubectl get services
   </code>
  </pre>
<p>
   سپس باید بتوانید با web browser خود به service دسترسی پیدا کنید.
  </p>
<h4><strong>Summary</strong></h4>
<p>
   این فصل با یک pattern ساده برای service stateless replicated شروع شد. سپس دیدیم که چگونه این pattern با دو لایه load-balanced replicated اضافی برای ارائه caching برای performance و SSL termination برای secure web serving، رشد می‌کند. این pattern کامل برای serving stateless replicated در Figure 5-8 نشان داده شده است.
  </p>
<p>
   این pattern کامل می‌تواند با استفاده از سه Deployment و load balancers Service برای اتصال لایه‌های نشان داده شده در Figure 5-8 در Kubernetes مستقر شود. کد منبع کامل برای این مثال‌ها را می‌توان در آدرس https://github.com/brendandburns/ designing-distributed-systems یافت.
  </p>
<p>
   Summary
   | 57
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0071</div>
            </div>
        </div>
        <!-- Page 0073 -->
        <div class="chapter" id="page-0073">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>CHAPTER 6</strong></h3>
<h4><strong>Sharded Services</strong></h4>
<p>
   در فصل قبل، ما ارزش replicating service stateless را برای reliabil و redundancy و scaling دیدیم. این فصل، service های sharded را در نظر می‌گیرد. با service های replicated که ما در فصل قبل معرفی کردیم، هر replica کاملاً homogeneous بود و قادر به service دادن به هر request ای بود. در مقابل service های replicated، با service های sharded، هر replica یا shard، تنها قادر به service دادن به یک subset از همه requests ها است. یک node load-balancing یا root، مسئول بررسی هر request و توزیع هر request به shard یا shards مناسب برای پردازش است. تفاوت بین service های replicated و sharded در Figure 6-1 نشان داده شده است.
  </p>
<p>
<em>Figure 6-1. Replicated service versus sharded service</em>
</p>
<p>
   service های Replicated به طور کلی برای ساختن service های stateless استفاده می‌شوند، در حالی که service های sharded به طور کلی برای ساختن service های stateful استفاده می‌شوند. دلیل اصلی برای sharding data این است که اندازه state برای اینکه توسط یک machine واحد service داده شود، بسیار بزرگ است. Sharding به شما این امکان را می‌دهد که یک service را در پاسخ به اندازه state که باید service داده شود، scale کنید.
  </p>
<h4><strong>Sharded Caching</strong></h4>
<p>
   برای نشان دادن کامل طراحی یک system sharded، این بخش یک deep dive را به طراحی یک system sharded caching ارائه می‌دهد. یک sharded cache، یک cache است که در
   59
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 73" src="page_0073/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0073</div>
            </div>
        </div>
        <!-- Page 0074 -->
        <div class="chapter" id="page-0074">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   بین requests های user و پیاده‌سازی frontend. یک نمودار high-level از system در Figure 6-2 نشان داده شده است.
  </p>
<p>
<em>Figure 6-2. A sharded cache</em>
</p>
<p>
   در فصل 3، ما در مورد چگونگی استفاده از یک ambassador برای توزیع data به یک service sharded بحث کردیم. این بخش در مورد نحوه ساختن آن service بحث می‌کند. هنگام طراحی یک sharded cache، تعدادی جنبه طراحی وجود دارد که باید در نظر گرفت:
  </p>
<ul>
<li>چرا ممکن است به یک sharded cache نیاز داشته باشید</li>
<li>نقش cache در architecture شما</li>
<li>cache های Replicated و sharded</li>
<li>sharding function</li>
</ul>
<h4><strong>Why You Might Need a Sharded Cache</strong></h4>
<p>
   همانطور که در مقدمه ذکر شد، دلیل اصلی برای sharding هر service، افزایش اندازه data ای است که در service ذخیره می‌شود. برای درک اینکه چگونه این به یک system caching کمک می‌کند، system زیر را تصور کنید: هر cache دارای 10 GB از RAM است که برای ذخیره results ها در دسترس است و می‌تواند 100 requests در ثانیه (RPS) را service دهد. سپس فرض کنید که service ما در مجموع 200 GB results احتمالی دارد که می‌توانند برگردانده شوند و 1000 RPS مورد انتظار است. بدیهی است، ما به 10 replica از cache نیاز داریم تا 1000 RPS را برآورده کنیم (10 replicas × 100 requests در ثانیه در هر replica). ساده‌ترین راه برای استقرار این service، به عنوان یک service replicated است، همانطور که در فصل قبل توضیح داده شد. اما با استقرار به این روش، cache توزیع‌شده می‌تواند حداکثر 5٪ (10 GB/200 GB) از کل data set را که ما در حال service دادن آن هستیم، نگه دارد. این به این دلیل است که هر cache replica مستقل است و بنابراین هر cache replica تقریباً دقیقاً data یکسان را در cache ذخیره می‌کند. این برای redundancy عالی است، اما برای به حداکثر رساندن memory utilization بسیار وحشتناک است. اگر در عوض، ما یک cache sharded 10-way را مستقر کنیم، همچنان می‌توانیم مناسب را service دهیم.
  </p>
<p>
   60
   | Chapter 6: Sharded Services
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 74" src="page_0074/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0074</div>
            </div>
        </div>
        <!-- Page 0075 -->
        <div class="chapter" id="page-0075">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   ate number از نوع RPS (10 × 100 همچنان 1000 است)، اما از آنجایی که هر cache یک مجموعه داده کاملاً منحصربه‌فرد را service می‌دهد، ما می‌توانیم 50٪ (10 × 10 GB/200 GB) از کل data set را ذخیره کنیم. این افزایش ده برابری در cache storage به این معنی است که حافظه cache بسیار بهتر مورد استفاده قرار می‌گیرد، زیرا هر key فقط در یک cache وجود دارد.
  </p>
<h4><strong>The Role of the Cache in System Performance</strong></h4>
<p>
   در فصل 5 ما در مورد چگونگی استفاده از cache ها برای بهینه‌سازی end-user performance و latency بحث کردیم، اما یک چیز که پوشش داده نشد، critical بودن cache برای performance، reliability و stability application شما بود.
  </p>
<p>
   به بیان ساده، سوال مهمی که باید در نظر بگیرید این است: اگر cache از کار بیفتد، تأثیر آن برای users و service شما چه خواهد بود؟
  </p>
<p>
   وقتی در مورد cache replicated بحث کردیم، این سوال کمتر مرتبط بود زیرا خود cache به صورت افقی قابل scale بود و failures های replica های خاص فقط منجر به transient failures می‌شد. به همین ترتیب، cache می‌تواند در پاسخ به افزایش load بدون تأثیر بر end user، به صورت افقی scale شود.
  </p>
<p>
   این موضوع زمانی تغییر می‌کند که شما cache های sharded را در نظر می‌گیرید. از آنجایی که یک user یا request خاص همیشه به همان shard map می‌شود، اگر آن shard از کار بیفتد، آن user یا request همیشه cache را از دست می‌دهد تا زمانی که shard بازیابی شود. با توجه به ماهیت cache به عنوان data transient، این miss ذاتاً یک مشکل نیست و system شما باید بداند چگونه data را دوباره محاسبه کند. با این حال، این recalculation ذاتا کندتر از استفاده مستقیم از cache است و بنابراین، implications performance را برای end users شما دارد.
  </p>
<p>
   performance cache شما بر حسب hit rate آن تعریف می‌شود. hit rate درصد زمانی است که cache شما شامل data برای یک user request است. در نهایت، hit rate، ظرفیت کلی system توزیع‌شده شما را تعیین می‌کند و بر ظرفیت و performance کلی system شما تأثیر می‌گذارد.
  </p>
<p>
   تصور کنید، اگر بخواهید، یک لایه request-serving دارید که می‌تواند 1000 RPS را مدیریت کند. پس از 1000 RPS، system شروع به برگرداندن خطاهای HTTP 500 به users می‌کند. اگر شما یک cache با hit rate 50٪ را در جلوی این لایه request-serving قرار دهید، اضافه کردن این cache، حداکثر RPS شما را از 1000 RPS به 2000 RPS افزایش می‌دهد. برای درک اینکه چرا این درست است، می‌توانید ببینید که از 2000 requests ورودی، 1000 (50٪) را می‌توان توسط cache service داد و 1000 requests باقی می‌ماند تا توسط serving layer شما service داده شود. در این مورد، cache کاملاً برای service شما critical است، زیرا اگر cache از کار بیفتد، serving layer overload می‌شود و نیمی از تمام user requests های شما با شکست مواجه می‌شوند. با توجه به این موضوع، احتمالاً منطقی است که service خود را در حداکثر 1500 RPS rate کنید تا کل 2000 RPS. اگر این کار را انجام دهید، می‌توانید یک failure از نیمی از cache replicas های خود را حفظ کنید و همچنان service خود را پایدار نگه دارید.
  </p>
<p>
   اما performance system شما فقط بر حسب تعداد requests هایی که می‌تواند پردازش کند، تعریف نمی‌شود. performance end-user system شما بر حسب latency requests نیز تعریف می‌شود. یک result از یک cache به طور کلی بسیار سریعتر است
  </p>
<p>
   Sharded Caching
   | 61
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0075</div>
            </div>
        </div>
        <!-- Page 0076 -->
        <div class="chapter" id="page-0076">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   تا محاسبه آن result از ابتدا. در نتیجه، یک cache می‌تواند سرعت requests ها و همچنین تعداد کل requests های پردازش شده را بهبود بخشد. برای اینکه بدانید چرا این درست است، تصور کنید که system شما می‌تواند یک request را از یک user در 100 میلی‌ثانیه service دهد. شما یک cache با hit rate 25٪ اضافه می‌کنید که می‌تواند یک result را در 10 میلی‌ثانیه برگرداند. بنابراین، میانگین latency برای یک request در system شما اکنون 77.5 میلی‌ثانیه است. بر خلاف حداکثر requests در ثانیه، cache به سادگی requests های شما را سریع‌تر می‌کند، بنابراین نیاز کمتری به نگرانی در مورد این واقعیت وجود دارد که requests ها در صورت failure cache یا ارتقا کند می‌شوند. با این حال، در برخی موارد، impact performance می‌تواند باعث شود که requests های user زیادی در request queues جمع شوند و در نهایت time out شوند. همیشه توصیه می‌شود که system خود را هم با و هم بدون cache ها load test کنید تا impact cache را بر performance کلی system خود درک کنید.
  </p>
<p>
   در نهایت، این فقط failures نیست که شما باید در مورد آن فکر کنید. اگر شما نیاز به ارتقا یا redeploy یک sharded cache دارید، شما نمی‌توانید فقط یک replica جدید را مستقر کنید و فرض کنید که load را بر عهده می‌گیرد. استقرار یک version جدید از یک sharded cache به طور کلی منجر به از دست دادن موقت مقداری capacity می‌شود. یک گزینه پیشرفته‌تر دیگر، replicate کردن shards های شما است.
  </p>
<h4><strong>Replicated, Sharded Caches</strong></h4>
<p>
   گاهی اوقات system شما آنقدر به یک cache برای latency یا load وابسته است که از دست دادن یک cache shard کامل، قابل قبول نیست، چه failure یا شما در حال انجام rollout هستید. از طرف دیگر، ممکن است شما load زیادی بر روی یک cache shard خاص داشته باشید که نیاز به scale کردن آن برای مدیریت load دارید. به همین دلایل، شما ممکن است انتخاب کنید که یک service شارد شده و replicated را مستقر کنید. یک service sharded و replicated، pattern service replicated که در فصل قبل توضیح داده شد را با pattern sharded که در بخش‌های قبلی توضیح داده شد، ترکیب می‌کند. به طور خلاصه، به جای داشتن یک server واحد برای پیاده‌سازی هر shard در cache، از یک service replicated برای پیاده‌سازی هر cache shard استفاده می‌شود.
  </p>
<p>
   این طراحی بدیهی است که پیاده‌سازی و استقرار آن پیچیده‌تر است، اما چندین مزیت نسبت به یک service sharded ساده دارد. مهمتر از همه، با جایگزینی یک server واحد با یک service replicated، هر cache shard در برابر failures مقاوم است و همیشه در طول failures وجود دارد. به جای طراحی system خود به گونه‌ای که تحمل تخریب performance ناشی از cache shard failures را داشته باشد، می‌توانید به بهبود performance که cache ارائه می‌دهد، تکیه کنید. با فرض اینکه شما مایل به over-provision capacity shard هستید، این بدان معنی است که شما می‌توانید با خیال راحت یک cache rollout را در طول peak traffic انجام دهید، به جای انتظار برای یک دوره آرام برای service خود.
  </p>
<p>
   علاوه بر این، از آنجایی که هر cache shard replicated، یک service replicated مستقل است، شما می‌توانید هر cache shard را در پاسخ به load آن scale کنید. این نوع "hot sharding" در پایان این فصل مورد بحث قرار می‌گیرد.
  </p>
<p>
   62
   | Chapter 6: Sharded Services
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0076</div>
            </div>
        </div>
        <!-- Page 0077 -->
        <div class="chapter" id="page-0077">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4><strong>Hands On: Deploying an Ambassador and Memcache for a Sharded Cache</strong></h4>
<p>
   در فصل 3 ما دیدیم که چگونه یک service sharded از نوع Redis را مستقر کنیم. استقرار یک memcache sharded مشابه است.
  </p>
<p>
   ابتدا، ما memcache را به عنوان یک Kubernetes StatefulSet مستقر خواهیم کرد:
  </p>
<pre>
   <code class="language-yaml">apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: sharded-memcache
spec:
  serviceName: "memcache"
  replicas: 3
  template:
    metadata:
      labels:
        app: memcache
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: memcache
        image: memcached
        ports:
        - containerPort: 11211
          name: memcache
   </code>
  </pre>
<p>
   این را در فایلی با نام memcached-shards.yaml ذخیره کنید و شما می‌توانید این را با دستور زیر مستقر کنید
   <pre>
    <code class="language-bash">kubectl create -f memecached-shards.yaml
    </code>
   </pre>
   این دستور سه container را که memcached را اجرا می‌کنند، ایجاد می‌کند.
  </p>
<p>
   همانند مثال Redis sharded، ما همچنین نیاز داریم که یک Kubernetes Service ایجاد کنیم که نام‌های DNS را برای replicas هایی که ایجاد کرده‌ایم، ایجاد می‌کند. service به این صورت است:
  </p>
<pre>
   <code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: memcache
  labels:
    app: memcache
spec:
  ports:
  - port: 11211
    name: memcache
  clusterIP: None
  selector:
    app: memecache
   </code>
  </pre>
<p>
   این را در فایلی با نام memcached-service.yaml ذخیره کنید و آن را با دستور زیر مستقر کنید
   <pre>
    <code class="language-bash">kubectl create -f memcached-service.yaml. 
   </code>
  </pre>
<p>
   You 
   should 
   now 
   have 
   DNS 
   entries 
   for
  </p>
<p>
   Sharded Caching
   | 63
  </p>
</p></div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0077</div>
            </div>
        </div>
        <!-- Page 0078 -->
        <div class="chapter" id="page-0078">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   memecache-0.memecache, memecache-1.memcache, etc. مانند Redis، ما می‌توانیم از این نام‌ها برای پیکربندی twemproxy استفاده کنیم.
  </p>
<pre>
   <code class="language-yaml">memcache:
  listen: 127.0.0.1:11211
  hash: fnv1a_64
  distribution: ketama
  auto_eject_hosts: true
  timeout: 400
  server_retry_timeout: 2000
  server_failure_limit: 1
  servers:
   - memcache-0.memcache:11211:1
   - memcache-1.memcache:11211:1
   - memcache-2.memcache:11211:1
   </code>
  </pre>
<p>
   در این config، شما می‌توانید ببینید که ما در حال service دادن protocol memcache بر روی localhost:11211 هستیم تا container application بتواند به ambassador دسترسی داشته باشد. ما این را با استفاده از یک object از نوع Kubernetes ConfigMap که می‌توانیم با دستور زیر ایجاد کنیم، در pod ambassador خود مستقر می‌کنیم:
   <pre>
    <code class="language-bash">kubectl create configmap --from-file=nutcracker.yaml twem-config.
    </code>
   </pre>
</p>
<p>
   در نهایت، تمام آماده‌سازی‌ها انجام شده است و ما می‌توانیم مثال ambassador خود را مستقر کنیم.
   ما یک pod را تعریف می‌کنیم که به این صورت است:
  </p>
<pre>
   <code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: sharded-memcache-ambassador
spec:
  containers:
    # This is where the application container would go, for example
    # - name: nginx
    #   image: nginx
    # This is the ambassador container
    - name: twemproxy
      image: ganomede/twemproxy
      command:
      - nutcracker
      - -c
      - /etc/config/nutcracker.yaml
      - -v
      - 7
      - -s
      - 6222
      volumeMounts:
      - name: config-volume
        mountPath: /etc/config
  volumes:
    - name: config-volume
      configMap:
        name: twem-config
   </code>
  </pre>
<p>
   64
   | Chapter 6: Sharded Services
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0078</div>
            </div>
        </div>
        <!-- Page 0079 -->
        <div class="chapter" id="page-0079">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   شما می‌توانید این را در فایلی با نام memcached-ambassador-pod.yaml ذخیره کنید، و سپس آن را با دستور زیر مستقر کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f memcached-ambassador-pod.yaml
   </code>
  </pre>
<p>
   البته، اگر نخواهیم، مجبور نیستیم از pattern ambassador استفاده کنیم. یک جایگزین، استقرار یک service shard router replicated است. بین استفاده از یک ambassador در مقابل استفاده از یک service shard routing، trade-offs وجود دارد. ارزش service کاهش پیچیدگی است. شما مجبور نیستید ambassador را با هر pod ای که می‌خواهد به service memcache sharded دسترسی داشته باشد، مستقر کنید، می‌توان از طریق یک service نام‌گذاری شده و load-balanced به آن دسترسی داشت. نقطه ضعف یک service shared دو چندان است. اول، از آنجایی که این یک service shared است، شما باید آن را با افزایش load تقاضا، scale کنید. دوم، استفاده از service shared، یک network hop اضافی را معرفی می‌کند که مقداری latency به requests ها اضافه می‌کند و به bandwith network در system توزیع‌شده کلی کمک می‌کند.
  </p>
<p>
   برای استقرار یک service shared routing، شما باید configuration twemproxy را کمی تغییر دهید تا به جای localhost:11211، به همه interfaces گوش دهد:
  </p>
<pre>
   <code class="language-yaml">memcache:
  listen: 0.0.0.0:11211
  hash: fnv1a_64
  distribution: ketama
  auto_eject_hosts: true
  timeout: 400
  server_retry_timeout: 2000
  server_failure_limit: 1
  servers:
   - memcache-0.memcache:11211:1
   - memcache-1.memcache:11211:1
   - memcache-2.memcache:11211:1
   </code>
  </pre>
<p>
   شما می‌توانید این را در فایلی با نام shared-nutcracker.yaml ذخیره کنید، و سپس با استفاده از kubectl، یک ConfigMap مربوطه ایجاد کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create configmap --from-file=shared-nutcracker.yaml shared-twem-config
   </code>
  </pre>
<p>
   سپس شما می‌توانید service routing shard replicated را به عنوان یک Deployment بالا بیاورید:
  </p>
<pre>
   <code class="language-yaml">apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: shared-twemproxy
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: shared-twemproxy
    spec:
      containers:
      - name: twemproxy
        image: ganomede/twemproxy
   </code>
  </pre>
<p>
   Sharded Caching
   | 65
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0079</div>
            </div>
        </div>
        <!-- Page 0080 -->
        <div class="chapter" id="page-0080">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre>
   <code class="language-yaml">      command:
        - nutcracker
        - -c
        - /etc/config/shared-nutcracker.yaml
        - -v
        - 7
        - -s
        - 6222
        volumeMounts:
        - name: config-volume
          mountPath: /etc/config
      volumes:
      - name: config-volume
        configMap:
          name: shared-twem-config
   </code>
  </pre>
<p>
   اگر شما این را در shared-twemproxy-deploy.yaml ذخیره کنید، شما می‌توانید shard router replicated را با استفاده از kubectl ایجاد کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f shared-twemproxy-deploy.yaml
   </code>
  </pre>
<p>
   برای تکمیل shard router، ما باید یک load balancer برای پردازش requests ها declare کنیم:
  </p>
<pre>
   <code class="language-yaml">kind: Service
apiVersion: v1
metadata:
  name: shard-router-service
spec:
  selector:
    app: shared-twemproxy
  ports:
    - protocol: TCP
      port: 11211
      targetPort: 11211
   </code>
  </pre>
<p>
   این load balancer را می‌توان با استفاده از دستور زیر ایجاد کرد: kubectl create -f shard-router- service.yaml.
  </p>
<h4><strong>An Examination of Sharding Functions</strong></h4>
<p>
   تا کنون ما در مورد طراحی و استقرار cache های sharded و replicated sharded ساده بحث کرده‌ایم، اما زمان زیادی را به بررسی نحوه هدایت ترافیک به shards های مختلف اختصاص نداده‌ایم. service sharded ای را در نظر بگیرید که در آن شما 10 shard مستقل دارید. با توجه به request خاص user Req، چگونه شما تعیین می‌کنید که کدام shard S در محدوده صفر تا نه باید برای request استفاده شود؟ این mapping، مسئولیت sharding function است. یک sharding function بسیار شبیه به یک hashing function است، که ممکن است هنگام یادگیری در مورد data structures های hashtable با آن مواجه شده باشید. در واقع، یک hashtable مبتنی بر bucket می‌تواند نمونه‌ای از یک service sharded در نظر گرفته شود. با توجه به Req و Shard، سپس نقش sharding function، مرتبط کردن آنها با یکدیگر است، به طور خاص:
  </p>
<p>
   66
   | Chapter 6: Sharded Services
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0080</div>
            </div>
        </div>
        <!-- Page 0081 -->
        <div class="chapter" id="page-0081">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   Shard = ShardingFunction(Req)
  </p>
<p>
   به طور معمول، sharding function با استفاده از یک hashing function و عملگر modulo (%) تعریف می‌شود. Hashing functions، توابعی هستند که یک object دلخواه را به یک integer hash تبدیل می‌کنند. hash function دو ویژگی مهم برای sharding ما دارد:
  </p>
<p>
<strong>Determinism</strong>
   خروجی باید همیشه برای یک ورودی منحصر به فرد یکسان باشد.
  </p>
<p>
<strong>Uniformity</strong>
   توزیع خروجی‌ها در فضای خروجی باید برابر باشد.
  </p>
<p>
   برای service sharded ما، determinism و uniformity مهمترین characteristics هستند. Determinism مهم است زیرا تضمین می‌کند که یک request خاص R همیشه به همان shard در service می‌رود. Uniformity مهم است زیرا تضمین می‌کند که load به طور مساوی بین shards های مختلف پخش می‌شود.
  </p>
<p>
   خوشبختانه برای ما، زبان‌های برنامه‌نویسی مدرن شامل انواع گسترده‌ای از hash functions با کیفیت بالا هستند. با این حال، خروجی‌های این hash functions اغلب به طور قابل توجهی بزرگتر از تعداد shards ها در یک service sharded هستند. در نتیجه، ما از عملگر modulo (%) برای کاهش یک hash function به محدوده مناسب استفاده می‌کنیم. با بازگشت به service sharded ما با 10 shards، ما می‌توانیم ببینیم که می‌توانیم sharding function خود را به صورت زیر تعریف کنیم:
  </p>
<p>
   Shard = hash(Req) % 10
  </p>
<p>
   اگر خروجی hash function دارای properties مناسب از نظر determinism و uniformity باشد، آن properties ها توسط عملگر modulo حفظ می‌شوند.
  </p>
<h4><strong>Selecting a Key</strong></h4>
<p>
   با توجه به این sharding function، ممکن است وسوسه‌انگیز باشد که به سادگی از hash function استفاده کنید که در زبان برنامه‌نویسی ساخته شده است، کل object را hash کنید و آن را یک روز بنامید.
   با این حال، نتیجه این کار یک sharding function بسیار خوب نخواهد بود.
  </p>
<p>
   برای درک این موضوع، یک request HTTP ساده را در نظر بگیرید که شامل سه مورد است:
  </p>
<ul>
<li>زمان request</li>
<li>آدرس IP منبع از client</li>
<li>request path HTTP (به عنوان مثال، /some/page.html)</li>
</ul>
<p>
   اگر ما از یک hash function مبتنی بر object ساده، shard(request)، استفاده کنیم، پس مشخص است که {12:00, 1.2.3.4, /some/file.html} یک shard value متفاوت از {12:01 دارد،
  </p>
<p>
   An Examination of Sharding Functions
   | 67
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0081</div>
            </div>
        </div>
        <!-- Page 0082 -->
        <div class="chapter" id="page-0082">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   5.6.7.8, /some/file.html}. خروجی sharding function متفاوت است زیرا آدرس IP client و زمان request بین دو request متفاوت است. اما البته، در بیشتر موارد، آدرس IP client و زمان request بر response به request HTTP تأثیری ندارند. در نتیجه، به جای hashing کل object request، یک sharding function بسیار بهتر خواهد بود
   shard(request.path). هنگامی که ما از request.path به عنوان shard key استفاده می‌کنیم، سپس هر دو request را به همان shard نگاشت می‌کنیم، و بنابراین response به یک request می‌تواند از cache برای service دیگری استفاده شود.
  </p>
<p>
   البته، گاهی اوقات IP client برای response ای که از frontend برگردانده می‌شود، مهم است. به عنوان مثال، IP client ممکن است برای جستجوی منطقه جغرافیایی که user در آن قرار دارد، استفاده شود، و محتوای متفاوت (به عنوان مثال، زبان‌های مختلف) ممکن است به آدرس‌های IP مختلف برگردانده شود. در چنین مواردی، sharding function قبلی
   shard(request.path) در واقع منجر به errors می‌شود، زیرا یک cache request از یک آدرس IP فرانسوی ممکن است یک صفحه result از cache را به زبان انگلیسی service دهد. در چنین مواردی، cache function بیش از حد general است، زیرا requests هایی را که responses یکسان ندارند، با هم گروه‌بندی می‌کند.
  </p>
<p>
   با توجه به این مشکل، تعریف sharding function ما به این صورت وسوسه‌انگیز خواهد بود
   shard(request.ip, request.path)، اما این sharding function نیز مشکلاتی دارد.
   این باعث می‌شود که دو آدرس IP فرانسوی مختلف به shards های مختلف map شوند، در نتیجه sharding نا‌کارآمدی ایجاد می‌شود. این shard function بیش از حد specific است، زیرا در گروه‌بندی requests هایی که یکسان هستند، شکست می‌خورد. یک sharding function بهتر برای این وضعیت خواهد بود:
  </p>
<p>
   shard(country(request.ip), request.path)
  </p>
<p>
   این ابتدا کشور را از آدرس IP تعیین می‌کند، و سپس از آن کشور به عنوان بخشی از key برای sharding function استفاده می‌کند. بنابراین چندین request از فرانسه به یک shard هدایت می‌شود، در حالی که requests ها از ایالات متحده به یک shard متفاوت هدایت می‌شوند.
  </p>
<p>
   تعیین key مناسب برای sharding function شما برای طراحی خوب system sharded شما حیاتی است. تعیین shard key صحیح نیاز به درک requests هایی دارد که انتظار دارید ببینید.
  </p>
<h4><strong>Consistent Hashing Functions</strong></h4>
<p>
   تنظیم initial shards برای یک service جدید نسبتاً ساده است: شما shards و roots مناسب را برای انجام sharding تنظیم می‌کنید و از مسابقه خارج می‌شوید. با این حال، چه اتفاقی می‌افتد وقتی نیاز دارید تعداد shards ها را در service sharded خود تغییر دهید؟ چنین "re-sharding" اغلب یک فرآیند پیچیده است.
  </p>
<p>
   برای درک اینکه چرا این درست است، cache sharded را که قبلاً بررسی شد، در نظر بگیرید. مطمئناً، scale کردن cache از 10 به 11 replicas، انجام آن با یک
  </p>
<p>
   68
   | Chapter 6: Sharded Services
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0082</div>
            </div>
        </div>
        <!-- Page 0083 -->
        <div class="chapter" id="page-0083">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   tainer orchestrator، اما تأثیر تغییر scaling function از
   hash(Req) % 10 به hash(Req) % 11 را در نظر بگیرید. هنگامی که شما این scaling function جدید را مستقر می‌کنید، تعداد زیادی از requests ها به یک shard متفاوت از آنهایی که قبلاً به آن map شده بودند، map می‌شوند. در یک cache sharded، این امر به طور چشمگیری hit rate شما را افزایش می‌دهد تا زمانی که cache با responses برای requests های جدیدی که توسط sharding function جدید به آن cache shard map شده‌اند، دوباره پر شود. در بدترین حالت، rolling out یک sharding function جدید برای cache sharded شما معادل یک failure کامل cache خواهد بود.
  </p>
<p>
   برای حل این نوع مشکلات، بسیاری از sharding functions از consistent hashing functions استفاده می‌کنند. Consistent hashing functions، hash functions خاصی هستند که تضمین می‌کنند فقط # keys / # shards، هنگام تغییر اندازه به # shards، remapping می‌شوند. به عنوان مثال، اگر ما از یک consistent hashing function برای cache sharded خود استفاده کنیم، حرکت از 10 به 11 shards، فقط منجر به remapping &lt; 10% (K / 11) keys می‌شود. این به طور چشمگیری بهتر از از دست دادن کل service sharded است.
  </p>
<h4><strong>Hands On: Building a Consistent HTTP Sharding Proxy</strong></h4>
<p>
   برای shard کردن requests های HTTP، اولین سوالی که باید به آن پاسخ دهید این است که چه چیزی را به عنوان key برای sharding function استفاده کنید. اگرچه چندین گزینه وجود دارد، یک key خوب با هدف عمومی، request path و همچنین fragment و query parameters است (به عنوان مثال، همه چیزهایی که request را unique می‌کند). توجه داشته باشید که این شامل cookies ها از user یا زبان/مکان (به عنوان مثال، EN_US) نمی‌شود. اگر service شما سفارشی‌سازی گسترده‌ای را برای users یا location آنها ارائه می‌دهد، شما باید آنها را نیز در hash key لحاظ کنید.
  </p>
<p>
   ما می‌توانیم از web server HTTP از نوع nginx برای sharding proxy خود استفاده کنیم.
  </p>
<pre>
   <code class="language-nginx">worker_processes  5;
error_log  error.log;
pid        nginx.pid;
worker_rlimit_nofile 8192;
events {
  worker_connections  1024;
}
http {
    # define a named 'backend' that we can use in the proxy directive
    # below.
    upstream backend {
        # Has the full URI of the request and use a consistent hash
        hash $request_uri consistent
        server web-shard-1.web;
        server web-shard-2.web;
        server web-shard-3.web;
    }
    server {
   </code>
  </pre>
<p>
   An Examination of Sharding Functions
   | 69
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0083</div>
            </div>
        </div>
        <!-- Page 0084 -->
        <div class="chapter" id="page-0084">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre>
   <code class="language-nginx">       listen localhost:80;
        location / {
            proxy_pass http://backend;
        }
    }
}
   </code>
  </pre>
<p>
   توجه داشته باشید که ما انتخاب کردیم از request URI کامل به عنوان key برای hash استفاده کنیم و از کلمه key consistent استفاده کنیم تا نشان دهیم که ما می‌خواهیم از یک consistent hashing function استفاده کنیم.
  </p>
<h4><strong>Sharded, Replicated Serving</strong></h4>
<p>
   بیشتر مثال‌های این فصل تا کنون، sharding را بر حسب cache serving توصیف کرده‌اند. اما، البته، cache ها تنها نوع service هایی نیستند که می‌توانند از sharding بهره‌مند شوند. Sharding زمانی مفید است که هر نوع service را در نظر بگیرید که در آن data بیشتری نسبت به آنچه می‌تواند بر روی یک machine واحد قرار گیرد، وجود دارد. در مقابل مثال‌های قبلی، key و sharding function بخشی از request HTTP نیستند، بلکه مقداری context برای user هستند.
  </p>
<p>
   به عنوان مثال، پیاده‌سازی یک بازی چند نفره در مقیاس بزرگ را در نظر بگیرید. چنین world بازی احتمالاً بسیار بزرگتر از آن است که در یک machine واحد قرار گیرد. با این حال، بازیکنانی که از یکدیگر در این world مجازی دور هستند، بعید است با هم تعامل داشته باشند. در نتیجه، world بازی می‌تواند در بسیاری از machine های مختلف sharded شود. sharding function از مکان player key می‌گیرد تا همه players در یک مکان خاص در همان مجموعه از servers قرار گیرند.
  </p>
<h4><strong>Hot Sharding Systems</strong></h4>
<p>
   در حالت ایده‌آل، load بر روی یک cache sharded کاملاً یکنواخت خواهد بود، اما در بسیاری از موارد این درست نیست و "hot shards" ظاهر می‌شوند زیرا الگوهای load organic، ترافیک بیشتری را به یک shard خاص هدایت می‌کنند.
  </p>
<p>
   به عنوان مثالی از این، یک cache sharded برای photos یک user را در نظر بگیرید. هنگامی که یک photo خاص به صورت viral منتشر می‌شود و ناگهان مقدار نامتناسبی از ترافیک را دریافت می‌کند، cache shard شامل آن photo، "hot" می‌شود. وقتی این اتفاق می‌افتد، با یک cache sharded و replicated، شما می‌توانید cache shard را برای پاسخ به افزایش load، scale کنید.
  </p>
<p>
   در واقع، اگر شما autoscaling را برای هر cache shard تنظیم کنید، می‌توانید به طور پویا هر shard replicated را با تغییر ترافیک organic به service خود، رشد و کوچک کنید. یک تصویر از این فرآیند در Figure 6-3 نشان داده شده است. در ابتدا service sharded، ترافیک مساوی را به هر سه shard دریافت می‌کند. سپس ترافیک تغییر می‌کند به طوری که Shard A چهار برابر ترافیک بیشتر از Shard B و Shard C دریافت می‌کند. system hot sharding، Shard B را به همان machine Shard C منتقل می‌کند و Shard A را به یک machine دوم replicated می‌کند. ترافیک اکنون، دوباره به طور مساوی بین replicas به اشتراک گذاشته شده است.
  </p>
<p>
   70
   | Chapter 6: Sharded Services
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0084</div>
            </div>
        </div>
        <!-- Page 0085 -->
        <div class="chapter" id="page-0085">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 6-3. An example of a hot sharded system: initially the shards are evenly dis‐ tributed, but when extra traffic comes to shard A, it is replicated to two machines, and shards B and C are combined on a single machine</em>
</p>
<p>
   Hot Sharding Systems
   | 71
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 85" src="page_0085/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0085</div>
            </div>
        </div>
        <!-- Page 0087 -->
        <div class="chapter" id="page-0087">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>CHAPTER 7</strong></h3>
<h4><strong>Scatter/Gather</strong></h4>
<p>
   تا کنون ما system هایی را بررسی کرده‌ایم که برای scalability از نظر تعداد requests های پردازش شده در هر ثانیه (الگوی replicated stateless) و همچنین scalability برای اندازه data (الگوی sharded data) replicated می‌کنند. در این فصل ما pattern scatter/gather را معرفی می‌کنیم، که از replication برای scalability از نظر زمان استفاده می‌کند. به طور خاص، pattern scatter/gather به شما این امکان را می‌دهد که در service دادن به requests ها، به موازات عمل کنید، و به شما امکان می‌دهد آنها را به طور قابل توجهی سریعتر از زمانی که مجبور بودید آنها را به صورت متوالی service دهید، service دهید.
  </p>
<p>
   مانند systems های replicated و sharded، pattern scatter/gather یک pattern درختی با یک root است که requests ها را توزیع می‌کند و leaves هایی که آن requests ها را پردازش می‌کنند. با این حال، در مقابل systems های replicated و sharded، با scatter/gather، requests ها به طور همزمان به تمام replicas ها در system داده می‌شوند. هر replica مقدار کمی پردازش انجام می‌دهد و سپس بخشی از result را به root برمی‌گرداند. سپس server root، نتایج جزئی مختلف را با هم ترکیب می‌کند تا یک response کامل واحد به request ایجاد کند و سپس این request را به client برمی‌گرداند. pattern scatter/gather در Figure 7-1 نشان داده شده است.
  </p>
<p>
   Scatter/gather زمانی بسیار مفید است که شما مقدار زیادی از processing را دارید که بیشتر مستقل است و برای رسیدگی به یک request خاص مورد نیاز است. Scatter/gather را می‌توان به عنوان sharding محاسبات لازم برای service دادن به request، به جای sharding data (اگرچه data sharding نیز ممکن است بخشی از آن باشد) مشاهده کرد.
  </p>
<p>
   73
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0087</div>
            </div>
        </div>
        <!-- Page 0088 -->
        <div class="chapter" id="page-0088">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 7-1. A scatter/gather pattern</em>
</p>
<h4><strong>Scatter/Gather with Root Distribution</strong></h4>
<p>
   ساده‌ترین شکل scatter/gather، حالتی است که در آن هر leaf کاملاً homogenous است اما کار به تعدادی از leaves های مختلف توزیع می‌شود تا performance request را بهبود بخشد. این pattern معادل حل یک مشکل "embarrassingly parallel" است. این مشکل می‌تواند به قطعات مختلفی تقسیم شود و هر قطعه را می‌توان با تمام قطعات دیگر در کنار هم قرار داد تا یک پاسخ کامل را تشکیل دهد.
  </p>
<p>
   برای درک این موضوع به صورت ملموس‌تر، تصور کنید که شما نیاز دارید یک user request R را service دهید و service دادن به این request توسط یک core واحد یک دقیقه طول می‌کشد تا پاسخ A را تولید کند. اگر ما یک application multi-threaded را برنامه‌نویسی کنیم، می‌توانیم این request را بر روی یک machine واحد با استفاده از چندین core موازی کنیم. با توجه به این رویکرد و یک processor 30 core (بله، معمولاً این یک processor 32 core خواهد بود، اما 30 محاسبات را تمیزتر می‌کند)، ما می‌توانیم زمان لازم برای پردازش یک request واحد را به 2 ثانیه کاهش دهیم (60 ثانیه محاسبه تقسیم شده در 30 thread برای محاسبه برابر با 2 ثانیه). اما حتی دو ثانیه برای service دادن به web request یک user بسیار کند است. علاوه بر این، دستیابی به یک سرعت کاملاً موازی بر روی یک process واحد، دشوار خواهد بود زیرا چیزهایی مانند حافظه، network یا پهنای باند دیسک شروع به تبدیل شدن به bottleneck می‌کنند. به جای parallelizing یک application در سراسر cores ها بر روی یک machine واحد، ما می‌توانیم از pattern scatter/gather برای parallelize کردن requests ها در سراسر چندین process بر روی machine های مختلف استفاده کنیم. به این ترتیب، ما می‌توانیم latency requests کلی خود را بهبود بخشیم، زیرا دیگر به تعداد cores هایی که می‌توانیم بر روی یک machine واحد داشته باشیم، محدود نیستیم، و همچنین اطمینان حاصل می‌کنیم که bottleneck در process ما همچنان CPU است، زیرا پهنای باند حافظه، network و دیسک همگی در تعدادی از machine های مختلف پخش می‌شوند. علاوه بر این، از آنجایی که هر machine در درخت scatter/gather قادر به رسیدگی به هر request است، root درخت می‌تواند به صورت پویا load را به گره‌های مختلف در زمان‌های مختلف بسته به responsiveness آنها، dispatch کند. اگر به هر دلیلی، یک leaf node خاص کندتر از machine های دیگر پاسخ می‌دهد (به عنوان مثال، دارای
   74
   | Chapter 7: Scatter/Gather
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 88" src="page_0088/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0088</div>
            </div>
        </div>
        <!-- Page 0089 -->
        <div class="chapter" id="page-0089">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   neighbor process که با resources تداخل دارد)، root می‌تواند به صورت پویا load را دوباره توزیع کند تا پاسخ سریعی را تضمین کند.
  </p>
<h4><strong>Hands On: Distributed Document Search</strong></h4>
<p>
   برای دیدن نمونه‌ای از scatter/gather در عمل، task جستجو در یک database بزرگ از documents برای تمام documents هایی که حاوی کلمات "cat" و "dog" هستند، را در نظر بگیرید.
  </p>
<p>
   یک راه برای انجام این جستجو این است که تمام documents ها را باز کنید، کل مجموعه را بخوانید، کلمات را در هر document جستجو کنید و سپس مجموعه documents هایی را که شامل هر دو کلمه هستند، به user برگردانید.
  </p>
<p>
   همانطور که ممکن است تصور کنید، این یک process بسیار کند است زیرا برای هر request نیاز به باز کردن و خواندن تعداد زیادی از فایل‌ها دارد. برای سریعتر کردن پردازش request، شما می‌توانید یک index بسازید. index در واقع یک hashtable است، که در آن keys کلمات جداگانه هستند (به عنوان مثال، "cat") و values یک لیست از documents هایی هستند که حاوی آن کلمه هستند.
  </p>
<p>
   اکنون، به جای جستجو در هر document، یافتن documents هایی که با هر کلمه مطابقت دارند، به آسانی انجام یک lookup در این hashtable است. با این حال، ما یک توانایی مهم را از دست داده‌ایم. به یاد داشته باشید که ما به دنبال تمام documents هایی بودیم که حاوی "cat" و "dog" بودند. از آنجایی که index فقط کلمات واحد دارد، نه conjunctions کلمات، ما هنوز هم نیاز داریم که documents هایی را که شامل هر دو کلمه هستند، پیدا کنیم. خوشبختانه، این فقط یک intersection از مجموعه documents هایی است که برای هر کلمه برگردانده می‌شود.
  </p>
<p>
   با توجه به این رویکرد، ما می‌توانیم این جستجوی document را به عنوان نمونه‌ای از pattern scatter/gather پیاده‌سازی کنیم. هنگامی که یک request به root جستجوی document می‌رسد، آن request را parse می‌کند و دو leaf machine (یکی برای کلمه "cat" و دیگری برای کلمه "dog") را توزیع می‌کند. هر یک از این machines، یک لیست از documents هایی را که با یکی از کلمات مطابقت دارند، برمی‌گرداند، و node root، لیست documents هایی را که شامل هر دو "cat" و "dog" هستند، برمی‌گرداند.
  </p>
<p>
   یک نمودار از این process در Figure 7-2 نشان داده شده است: leaf {doc1, doc2, doc4} را برای "cat" و {doc1, doc3, doc4} را برای "dog" برمی‌گرداند، بنابراین root، intersection را پیدا می‌کند و {doc1, doc4} را برمی‌گرداند.
  </p>
<p>
   Scatter/Gather with Root Distribution
   | 75
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0089</div>
            </div>
        </div>
        <!-- Page 0090 -->
        <div class="chapter" id="page-0090">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 7-2. Example of a term-sharded scatter/gather system</em>
</p>
<h4><strong>Scatter/Gather with Leaf Sharding</strong></h4>
<p>
   در حالی که اعمال pattern scatter/gather data replicated به شما امکان می‌دهد زمان پردازش مورد نیاز برای handling user requests را کاهش دهید، به شما این امکان را نمی‌دهد که از میزان data ای که می‌تواند در حافظه یا دیسک یک machine واحد نگه داشته شود، scale کنید. درست مانند pattern serving replicated که قبلاً توضیح داده شد، ساختن یک system scatter/gather replicated ساده است. اما در یک اندازه data خاص، لازم است که sharding را برای ساختن یک system که می‌تواند data بیشتری نسبت به آنچه در یک machine واحد ذخیره می‌شود، معرفی کنید.
  </p>
<p>
   قبلاً، هنگامی که sharding برای scale کردن systems های replicated معرفی شد، sharding در سطح per-request انجام شد. بخشی از request برای تعیین جایی که request ارسال می‌شد، استفاده می‌شد. سپس آن replica تمام پردازش را برای request مدیریت کرد و response به user بازگردانده شد. در عوض، با sharding scatter/gather، request به تمام leaf nodes (یا shards) در system ارسال می‌شود. هر leaf node، request را با استفاده از data ای که در shard خود بارگذاری کرده است، پردازش می‌کند. سپس این response جزئی به root node که data را درخواست کرده است، بازگردانده می‌شود و آن root node، همه responses ها را با هم ادغام می‌کند تا یک response جامع برای user ایجاد شود.
  </p>
<p>
   به عنوان یک مثال مشخص از این نوع architecture، پیاده‌سازی جستجو در یک مجموعه document بسیار بزرگ (به عنوان مثال، تمام patent ها در جهان) را در نظر بگیرید. در چنین مواردی، data بیش از آن است که در حافظه یک machine واحد جا شود، بنابراین در عوض data در چندین replica شارد می‌شود. به عنوان مثال، patent های 0-100,000 ممکن است در اولین machine، 100,001-200,000 در machine بعدی و غیره باشند. (توجه داشته باشید که این در واقع یک طرح sharding خوب نیست زیرا ما را به طور مداوم مجبور به اضافه کردن shards جدید می‌کند زیرا patent های جدید ثبت می‌شوند. در عمل، ما احتمالاً از شماره patent modulo تعداد کل shards استفاده می‌کنیم.) هنگامی که یک user یک request را برای یافتن یک کلمه خاص (به عنوان مثال، "rockets") در تمام patent ها در index ارسال می‌کند، آن request به هر
   76
   | Chapter 7: Scatter/Gather
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 90" src="page_0090/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0090</div>
            </div>
        </div>
        <!-- Page 0091 -->
        <div class="chapter" id="page-0091">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   shard، که در shard patent خود به دنبال patent هایی می‌گردد که با کلمه موجود در query مطابقت دارند. هر match ای که پیدا می‌شود، در پاسخ به request shard به root node برگردانده می‌شود. سپس root node، همه این responses ها را با هم جمع می‌کند تا یک response واحد ایجاد کند که شامل تمام patent هایی است که با کلمه خاص مطابقت دارند. عملکرد این search index در Figure 7-3 نشان داده شده است.
  </p>
<h4><strong>Hands On: Sharded Document Search</strong></h4>
<p>
   مثال قبلی، requests های term های مختلف را در سراسر cluster پخش کرد، اما این فقط در صورتی کار می‌کند که تمام documents ها در تمام machine های موجود در درخت scatter/gather وجود داشته باشند. اگر فضای کافی برای همه documents ها در تمام leaves های موجود در درخت وجود نداشته باشد، پس sharding باید برای قرار دادن مجموعه‌های مختلفی از documents ها بر روی leaves های مختلف استفاده شود.
  </p>
<p>
   این بدان معنی است که وقتی یک user درخواستی را برای تمام documents هایی که با کلمات "cat" و "dog" مطابقت دارند، انجام می‌دهد، درخواست در واقع به هر leaf در system scatter/gather ارسال می‌شود. هر leaf node، مجموعه documents هایی را که در مورد آنها می‌داند که با "cat" و "dog" مطابقت دارند، برمی‌گرداند. قبلاً، root node مسئول انجام intersection از دو مجموعه documents بود که برای دو کلمه مختلف برگردانده شد. در مورد sharded، root node مسئول ایجاد union از تمام documents هایی است که توسط تمام shards های مختلف برگردانده می‌شود و این مجموعه کامل از documents را به user برمی‌گرداند.
  </p>
<p>
   در Figure 7-3، اولین leaf، documents 1 تا 10 را service می‌دهد و {doc1, doc5} را برمی‌گرداند. دومین leaf، documents 11 تا 20 را service می‌دهد و {doc15} را برمی‌گرداند. سومین leaf، documents 21 تا 30 را service می‌دهد و {doc22, doc28} را برمی‌گرداند. root، همه این responses ها را با هم ترکیب می‌کند تا یک response واحد ایجاد کند و {doc1, doc5, doc15, doc22, doc28} را برمی‌گرداند.
  </p>
<p>
<em>Figure 7-3. Conjunctive query executing in a scatter/gather search system</em>
</p>
<p>
   Scatter/Gather with Leaf Sharding
   | 77
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 91" src="page_0091/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0091</div>
            </div>
        </div>
        <!-- Page 0092 -->
        <div class="chapter" id="page-0092">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4><strong>Choosing the Right Number of Leaves</strong></h4>
<p>
   ممکن است به نظر برسد که در pattern scatter/gather، replication کردن به تعداد بسیار زیادی از leaves همیشه یک ایده خوب خواهد بود. شما محاسبات خود را parallelize می‌کنید و در نتیجه زمان ساعت (clock time) مورد نیاز برای پردازش هر request خاص را کاهش می‌دهید. با این حال، افزایش parallelization هزینه‌بر است، و بنابراین انتخاب تعداد مناسب leaf nodes در pattern scatter/gather برای طراحی یک system توزیع‌شده با performance بالا، بسیار مهم است.
  </p>
<p>
   برای درک چگونگی وقوع این اتفاق، ارزش دارد که دو چیز را در نظر بگیرید. اولین مورد این است که پردازش هر request خاص دارای مقدار مشخصی از overhead است. این زمانی است که صرف parsing یک request، ارسال HTTP از طریق wire و غیره می‌شود. به طور کلی، overhead ناشی از رسیدگی به request system ثابت است و به طور قابل توجهی کمتر از زمان صرف شده در پردازش user code request است. در نتیجه، این overhead را می‌توان به طور کلی هنگام ارزیابی performance pattern scatter/gather نادیده گرفت.
  </p>
<p>
   با این حال، درک این نکته مهم است که هزینه این overhead با تعداد leaf nodes در pattern scatter/gather مقیاس می‌شود. بنابراین، حتی اگر هزینه کمی دارد، با ادامه parallelization، این overhead در نهایت بر هزینه محاسباتی business logic شما غلبه می‌کند. این بدان معنی است که gains parallelization، asymptotic هستند.
  </p>
<p>
   علاوه بر این واقعیت که اضافه کردن leaf nodes های بیشتر ممکن است در واقع سرعت پردازش را افزایش ندهد، systems های scatter/gather نیز از مشکل "straggler" رنج می‌برند. برای درک چگونگی عملکرد این، مهم است به یاد داشته باشید که در یک system scatter/gather، root node منتظر است تا requests ها از تمام leaf nodes برگردند قبل از اینکه یک response را به end user ارسال کند. از آنجایی که data از هر leaf node مورد نیاز است، زمان کلی لازم برای پردازش یک user request توسط کندترین leaf node که یک response ارسال می‌کند، تعریف می‌شود. برای درک impact این موضوع، تصور کنید که ما یک service داریم که دارای 99th percentile latency از 2 ثانیه است. این بدان معنی است که به طور متوسط ​​از هر 100 request، یک request دارای latency 2 ثانیه است، یا به عبارت دیگر، 1٪ احتمال وجود دارد که یک request 2 ثانیه طول بکشد. این ممکن است در ابتدا کاملاً قابل قبول باشد: یک user واحد از 100 نفر دارای یک request کند است. با این حال، در نظر بگیرید که این در واقع چگونه در یک system scatter/gather کار می‌کند. از آنجایی که زمان user request توسط کندترین response تعریف می‌شود، ما باید نه یک request واحد، بلکه تمام requests های پخش شده به leaf nodes های مختلف را در نظر بگیریم.
  </p>
<p>
   بیایید ببینیم وقتی به پنج leaf node پخش می‌کنیم، چه اتفاقی می‌افتد. در این وضعیت، 5٪ احتمال وجود دارد که یکی از این پنج scatter requests دارای latency 2 ثانیه باشد (0.99 × 0.99 × 0.99 × 0.99 × 0.99 == 0.95). این بدان معنی است که 99th percentile latency ما برای individual requests به 95th percentile latency برای system scatter/gather کامل ما تبدیل می‌شود. و از آنجا بدتر می‌شود: اگر ما به 100 leaves پخش کنیم، پس کم و بیش تضمین می‌کنیم که latency کلی ما برای همه requests ها 2 ثانیه خواهد بود.
  </p>
<p>
   78
   | Chapter 7: Scatter/Gather
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0092</div>
            </div>
        </div>
        <!-- Page 0093 -->
        <div class="chapter" id="page-0093">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   با هم، این complications های systems scatter/gather ما را به سمت چند نتیجه سوق می‌دهند:
  </p>
<ul>
<li>parallelism افزایش یافته همیشه سرعت را افزایش نمی‌دهد زیرا overhead بر روی هر node وجود دارد.</li>
<li>parallelism افزایش یافته همیشه سرعت را افزایش نمی‌دهد زیرا مشکل straggler وجود دارد.</li>
<li>performance از 99th percentile در مقایسه با systems های دیگر مهمتر است زیرا هر user request در واقع به requests های متعددی به service تبدیل می‌شود.</li>
</ul>
<p>
   همان مشکل straggler برای availability نیز اعمال می‌شود. اگر شما یک request را به 100 leaf node صادر کنید، و احتمال اینکه هر leaf node با شکست مواجه شود 1 در 100 باشد، شما دوباره عملاً تضمین می‌کنید که هر user request با شکست مواجه می‌شود.
  </p>
<h4><strong>Scaling Scatter/Gather for Reliability and Scale</strong></h4>
<p>
   البته، درست مانند یک system sharded، داشتن یک replica واحد از یک system scatter/gather sharded احتمالاً انتخاب طراحی مطلوبی نیست. یک replica واحد به این معنی است که اگر آن با شکست مواجه شود، تمام scatter/gather requests ها برای مدت زمانی که shard در دسترس نیست، با شکست مواجه می‌شوند زیرا تمام requests ها باید توسط تمام leaf nodes ها در pattern scatter/gather پردازش شوند. به همین ترتیب، upgrades ها، درصدی از shards شما را از بین می‌برند، بنابراین یک upgrade در حالی که تحت user-facing load قرار دارد، دیگر امکان‌پذیر نیست. در نهایت، scale محاسباتی system شما با load ای محدود می‌شود که هر node واحد قادر به دستیابی به آن است.
  </p>
<p>
   در نهایت، این امر scale شما را محدود می‌کند، و همانطور که در بخش‌های قبلی دیده‌ایم، شما نمی‌توانید به سادگی تعداد shards ها را برای بهبود قدرت محاسباتی یک pattern scatter/gather افزایش دهید.
  </p>
<p>
   با توجه به این چالش‌های reliability و scale، رویکرد صحیح این است که هر یک از shards های individual را replicated کنید تا به جای یک instance واحد در هر leaf node، یک service replicated وجود داشته باشد که هر leaf shard را پیاده‌سازی می‌کند. این pattern scatter/gather sharded و replicated در Figure 7-4 نشان داده شده است.
  </p>
<p>
   Scaling Scatter/Gather for Reliability and Scale
   | 79
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0093</div>
            </div>
        </div>
        <!-- Page 0094 -->
        <div class="chapter" id="page-0094">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 7-4. A sharded, replicated scatter/gatther system</em>
</p>
<p>
   به این روش ساخته شده است، هر leaf request از root در واقع در سراسر تمام replicas های سالم shard load balanced است. این بدان معنی است که اگر هر گونه failure وجود داشته باشد، آنها منجر به یک outage قابل مشاهده برای user system شما نمی‌شوند. به همین ترتیب، شما می‌توانید به طور ایمن یک upgrade را تحت load انجام دهید، زیرا هر shard replicated را می‌توان یک replica در یک زمان upgrade کرد. در واقع، شما می‌توانید upgrade را در سراسر چندین shard به طور همزمان انجام دهید، بسته به اینکه چقدر سریع می‌خواهید upgrade را انجام دهید.
  </p>
<p>
   80
   | Chapter 7: Scatter/Gather
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 94" src="page_0094/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0094</div>
            </div>
        </div>
        <!-- Page 0095 -->
        <div class="chapter" id="page-0095">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>CHAPTER 8</strong></h3>
<h4><strong>Functions and Event-Driven Processing</strong></h4>
<p>
   تا کنون، ما طراحی system هایی با محاسبات طولانی مدت را بررسی کرده‌ایم. servers هایی که requests های user را مدیریت می‌کنند، همیشه در حال اجرا هستند. این pattern برای بسیاری از applications که تحت بار سنگین قرار دارند، مقدار زیادی data را در حافظه نگه می‌دارند یا به نوعی background processing نیاز دارند، مناسب است. با این حال، یک کلاس از applications وجود دارد که ممکن است فقط به طور موقت برای رسیدگی به یک request واحد یا به سادگی نیاز به پاسخگویی به یک event خاص داشته باشد. این سبک از طراحی application مبتنی بر request یا event، اخیراً با توسعه محصولات function-as-a-service (FaaS) توسط ارائه دهندگان بزرگ cloud public، رونق یافته است. اخیراً، پیاده‌سازی‌های FaaS نیز بر روی cluster orchestrators در cloud private یا محیط‌های فیزیکی ظاهر شده‌اند. این فصل architecture های در حال ظهور را برای این سبک جدید از computing توصیف می‌کند. در بسیاری از موارد، FaaS یک component در یک architecture گسترده‌تر است تا یک راه‌حل کامل.
  </p>
<p>
   اغلب، FaaS به عنوان serverless computing نامیده می‌شود. و در حالی که این درست است (شما servers را در FaaS نمی‌بینید)، ارزش دارد که بین FaaS مبتنی بر event و مفهوم گسترده‌تر serverless computing تمایز قائل شوید. در واقع، serverless computing می‌تواند برای طیف گسترده‌ای از service های computing اعمال شود. به عنوان مثال، یک container orchestrator چند مستأجری (container-as-a-service) serverless است اما event-driven نیست. برعکس، یک FaaS متن‌باز (open source) که بر روی یک cluster از machine های فیزیکی که شما مالک و اداره‌کننده آن هستید، اجرا می‌شود، event-driven است اما serverless نیست. درک این تمایز به شما این امکان را می‌دهد که تعیین کنید چه زمانی event-driven، serverless یا هر دو، انتخاب مناسبی برای application شما است.
  </p>
<p>
   81
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 95" src="page_0095/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0095</div>
            </div>
        </div>
        <!-- Page 0096 -->
        <div class="chapter" id="page-0096">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4><strong>Determining When FaaS Makes Sense</strong></h4>
<p>
   مانند بسیاری از ابزارها برای توسعه یک system توزیع‌شده، دیدن یک راه‌حل خاص مانند event-driven processing به عنوان یک چکش جهانی، وسوسه‌انگیز است. با این حال، حقیقت این است که این ابزار برای مجموعه خاصی از مشکلات مناسب است. در یک context خاص، این یک ابزار قدرتمند است، اما کشیدن آن برای متناسب کردن همه applications یا systems ها منجر به طرح‌های بیش از حد پیچیده و شکننده می‌شود. به‌ویژه از آنجایی که FaaS یک ابزار computing جدید است، قبل از بحث در مورد الگوهای طراحی خاص، ارزش دارد که در مورد مزایا، محدودیت‌ها و موقعیت‌های بهینه برای استفاده از event-driven computing بحث کنیم.
  </p>
<h4><strong>The Benefits of FaaS</strong></h4>
<p>
   مزایای FaaS عمدتاً برای developer است. این امر فاصله از code تا running service را به طور چشمگیری ساده می‌کند. از آنجایی که هیچ artifact ای برای ایجاد یا push کردن فراتر از خود source code وجود ندارد، FaaS، رفتن از code روی یک laptop یا web browser به code در حال اجرا در cloud را ساده می‌کند.
  </p>
<p>
   به همین ترتیب، code ای که مستقر می‌شود، به طور خودکار مدیریت و scale می‌شود. با بارگذاری ترافیک بیشتر بر روی service، نمونه‌های بیشتری از function ایجاد می‌شود تا این افزایش ترافیک را مدیریت کند. اگر یک function به دلیل failures application یا machine از کار بیفتد، به طور خودکار بر روی machine دیگری راه‌اندازی می‌شود.
  </p>
<p>
   در نهایت، درست مانند containers ها، functions ها یک building block حتی granular تر برای طراحی systems توزیع‌شده هستند. Functions ها stateless هستند و بنابراین هر system ای که شما بر روی functions ها می‌سازید، ذاتاً modular تر و decoupled تر از یک system مشابه است که در یک باینری واحد ساخته شده است. اما، البته، این نیز چالش توسعه systems ها در FaaS است. decoupling هم یک نقطه قوت و هم یک نقطه ضعف است. بخش زیر برخی از چالش‌هایی را که از توسعه systems ها با استفاده از FaaS ناشی می‌شود، شرح می‌دهد.
  </p>
<h4><strong>The Challenges of FaaS</strong></h4>
<p>
   همانطور که در بخش قبل توضیح داده شد، توسعه systems ها با استفاده از FaaS شما را مجبور می‌کند که هر قطعه از service خود را به شدت decoupled کنید. هر function کاملاً مستقل است. تنها communication در سراسر network است، و هر instance function نمی‌تواند memory محلی داشته باشد، که نیازمند ذخیره تمام states ها در یک storage service است. این decoupling اجباری می‌تواند چابکی و سرعت را که با آن می‌توانید service ها را توسعه دهید، بهبود بخشد، اما همچنین می‌تواند عملیات همان service را به طور قابل توجهی پیچیده کند.
  </p>
<p>
   به طور خاص، اغلب بسیار دشوار است که یک view جامع از service خود بدست آورید، تعیین کنید که چگونه functions های مختلف با یکدیگر ادغام می‌شوند، و درک کنید که چه زمانی مشکلی پیش می‌آید و چرا این مشکلات پیش می‌آیند. علاوه بر این، ماهیت request-based و serverless از functions ها به این معنی است که تشخیص مشکلات خاص، بسیار دشوار است. به عنوان مثال، functions های زیر را در نظر بگیرید:
  </p>
<p>
   82
   | Chapter 8: Functions and Event-Driven Processing
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0096</div>
            </div>
        </div>
        <!-- Page 0097 -->
        <div class="chapter" id="page-0097">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   • functionA() که functionB() را فراخوانی می‌کند
  </p>
<p>
   • functionB() که functionC() را فراخوانی می‌کند
  </p>
<p>
   • functionC() که به functionA() بازمی‌گردد
  </p>
<p>
   اکنون در نظر بگیرید که چه اتفاقی می‌افتد وقتی یک request وارد هر یک از این functions ها می‌شود: این یک حلقه بی‌نهایت را شروع می‌کند که فقط زمانی خاتمه می‌یابد که زمان request اصلی تمام شود (و احتمالاً حتی در آن صورت هم نه) یا زمانی که پول شما برای پرداخت requests ها در system تمام شود. بدیهی است، مثال بالا کاملاً ساختگی است، اما در واقع تشخیص آن در code شما بسیار دشوار است. از آنجایی که هر function به طور رادیکال از سایر functions ها جدا شده است، هیچ representation واقعی از dependencies یا تعاملات بین functions های مختلف وجود ندارد. این مشکلات غیرقابل حل نیستند، و من انتظار دارم که با بالغ شدن FaaS ها، ابزارهای تجزیه و تحلیل و اشکال‌زدایی بیشتری یک تجربه غنی‌تر را برای درک چگونگی و چرایی عملکرد یک application متشکل از FaaS، ارائه دهند.
  </p>
<p>
   در حال حاضر، هنگام اتخاذ FaaS، شما باید هوشیار باشید تا monitoring و alerting دقیق را برای نحوه عملکرد system خود اتخاذ کنید تا بتوانید موقعیت‌ها را شناسایی کرده و قبل از اینکه به مشکلات مهمی تبدیل شوند، آنها را اصلاح کنید. البته، پیچیدگی معرفی شده توسط monitoring تا حدودی در مقابل سادگی استقرار به FaaS است، که اصطکاکی است که developers شما باید بر آن غلبه کنند.
  </p>
<h4><strong>The Need for Background Processing</strong></h4>
<p>
   FaaS ذاتاً یک مدل application مبتنی بر event است. Functions ها در پاسخ به events های مجزایی که رخ می‌دهند و اجرای functions ها را trigger می‌کنند، اجرا می‌شوند. علاوه بر این، به دلیل ماهیت serverless پیاده‌سازی این services ها، زمان اجرا (runtime) هر instance function خاص، عموماً زمان‌بندی شده است. این بدان معنی است که FaaS معمولاً برای موقعیت‌هایی که نیاز به processing دارند، مناسب نیست. نمونه‌هایی از این background processing ممکن است شامل transcoding یک ویدیو، فشرده‌سازی log files یا انواع دیگری از محاسبات کم‌اولویت و طولانی‌مدت باشد. در بسیاری از موارد، می‌توان یک trigger زمان‌بندی‌شده تنظیم کرد که به طور مصنوعی events را در functions های شما در یک زمان‌بندی خاص تولید می‌کند. اگرچه این برای پاسخگویی به events زمانی (به عنوان مثال، راه‌اندازی یک هشدار پیام متنی برای بیدار کردن کسی) مناسب است، اما هنوز هم زیرساخت کافی برای background processing عمومی نیست. برای دستیابی به آن، شما باید code خود را در یک محیطی راه‌اندازی کنید که از processes های طولانی‌مدت پشتیبانی می‌کند. و این به طور کلی به معنای تغییر به یک مدل pay-per-consumption به جای pay-per-request برای بخش‌هایی از application شما است که background processing را انجام می‌دهند.
  </p>
<h4><strong>The Need to Hold Data in Memory</strong></h4>
<p>
   علاوه بر چالش‌های عملیاتی، برخی از محدودیت‌های architecture وجود دارد که FaaS را برای برخی از انواع applications نامناسب می‌کند. اولین مورد از این محدودیت‌ها، نیاز به داشتن مقدار قابل توجهی data است که برای پردازش در حافظه بارگذاری شده است.
  </p>
<p>
   Determining When FaaS Makes Sense
   | 83
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0097</div>
            </div>
        </div>
        <!-- Page 0098 -->
        <div class="chapter" id="page-0098">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   user requests ها. انواع مختلفی از services ها (به عنوان مثال، service دادن به یک search index از documents) وجود دارد که برای service دادن به user requests ها، نیاز به مقدار زیادی data دارند که در حافظه بارگذاری شود. حتی با یک لایه storage نسبتاً سریع، بارگذاری چنین data هایی می‌تواند به طور قابل توجهی بیشتر از زمان مورد نظر برای service دادن به یک user request طول بکشد. از آنجایی که با FaaS، خود function ممکن است به صورت پویا در پاسخ به یک user request راه‌اندازی شود در حالی که user در حال انتظار است، نیاز به بارگذاری مقدار زیادی جزئیات ممکن است به طور قابل توجهی بر latency که user هنگام تعامل با service شما درک می‌کند، تأثیر بگذارد. البته، هنگامی که FaaS شما ایجاد شد، ممکن است تعداد زیادی از requests ها را مدیریت کند، بنابراین این هزینه بارگذاری را می‌توان در تعداد زیادی از requests ها amortized کرد. اما اگر شما تعداد کافی از requests ها را دارید تا یک function را فعال نگه دارید، پس احتمالاً شما بیش از حد برای requests هایی که در حال پردازش آنها هستید، پرداخت می‌کنید.
  </p>
<h4><strong>The Costs of Sustained Request-Based Processing</strong></h4>
<p>
   مدل هزینه FaaS cloud public بر اساس قیمت‌گذاری per-request است. این رویکرد عالی است اگر شما فقط چند request در دقیقه یا ساعت دارید. در چنین شرایطی، شما بیشتر اوقات idle هستید، و با توجه به یک مدل pay-per-request، شما فقط برای زمانی که service شما فعالانه requests ها را service می‌دهد، پرداخت می‌کنید. در مقابل، اگر شما requests ها را از طریق یک service طولانی مدت در یک container یا یک virtual machine service می‌دهید، پس شما همیشه برای چرخه‌های پردازنده که تا حد زیادی در انتظار یک user request هستند، پرداخت می‌کنید.
  </p>
<p>
   با این حال، با رشد یک service، تعداد requests هایی که شما در حال service دادن آنها هستید به نقطه‌ای می‌رسد که می‌توانید یک processor را به طور مداوم فعال نگه دارید و به user requests ها service دهید.
  </p>
<p>
   در این مرحله، economics یک مدل pay-per-request شروع به بد شدن می‌کند، و فقط بدتر می‌شود زیرا هزینه virtual machines cloud به طور کلی با اضافه کردن cores های بیشتر کاهش می‌یابد (و همچنین از طریق منابع متعهد شده مانند reservations یا sustained use discounts)، در حالی که هزینه per-request تا حد زیادی با تعداد requests ها به صورت خطی افزایش می‌یابد.
  </p>
<p>
   در نتیجه، با رشد و تکامل service شما، بسیار محتمل است که استفاده شما از FaaS نیز تکامل یابد. یک راه ایده‌آل برای scale کردن FaaS، اجرای یک FaaS متن‌باز (open source) است که بر روی یک container orchestrator مانند Kubernetes اجرا می‌شود. به این ترتیب، شما همچنان می‌توانید از مزایای developer از FaaS استفاده کنید، در حالی که از مدل‌های قیمت‌گذاری virtual machines بهره می‌برید.
  </p>
<h4><strong>Patterns for FaaS</strong></h4>
<p>
   علاوه بر درک trade-offs در استقرار architecture های event-driven یا FaaS به عنوان بخشی از system توزیع‌شده شما، درک بهترین راه‌ها برای استقرار FaaS برای طراحی یک system موفق، بسیار مهم است. این بخش برخی از canonical patterns ها را برای گنجاندن FaaS شرح می‌دهد.
  </p>
<p>
   84
   | Chapter 8: Functions and Event-Driven Processing
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0098</div>
            </div>
        </div>
        <!-- Page 0099 -->
        <div class="chapter" id="page-0099">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4><strong>The Decorator Pattern: Request or Response Transformation</strong></h4>
<p>
   FaaS برای استقرار simple functions هایی که می‌توانند یک ورودی را دریافت کنند، آن را به یک خروجی تبدیل کنند، و سپس آن را به یک service متفاوت منتقل کنند، ایده‌آل است. این pattern عمومی می‌تواند برای افزایش یا decorate کردن requests های HTTP به یا از یک service متفاوت استفاده شود. یک تصویر پایه از این pattern در Figure 8-1 نشان داده شده است.
  </p>
<p>
<em>Figure 8-1. The decorator pattern applied to HTTP APIs</em>
</p>
<p>
   جالب اینجاست که چندین analogy به این pattern در زبان‌های برنامه‌نویسی وجود دارد. به طور خاص، the decorator pattern از Python یک analogue نزدیک برای service هایی است که یک request یا response decorator می‌تواند انجام دهد. از آنجایی که transformation های decoration به طور کلی stateless هستند، و همچنین به این دلیل که آنها اغلب بعد از این واقعیت به code موجود اضافه می‌شوند زیرا service تکامل می‌یابد، آنها service های ایده‌آلی برای پیاده‌سازی از طریق FaaS هستند. علاوه بر این، سبکی FaaS به این معنی است که شما می‌توانید قبل از اینکه در نهایت یکی را اتخاذ کرده و آن را به طور کامل‌تر در پیاده‌سازی service خود قرار دهید، با انواع مختلفی از decorators ها آزمایش کنید.
  </p>
<p>
   یک مثال عالی از ارزش the decorator pattern، اضافه کردن defaults ها به ورودی یک API HTTP RESTFul است. در بسیاری از موارد در API، fields هایی وجود دارند که مقادیر آنها در صورت خالی بودن، باید دارای sane defaults باشند. به عنوان مثال، شما ممکن است بخواهید یک field به true پیش‌فرض شود، اما انجام این کار در JSON کلاسیک دشوار است، زیرا default value برای یک field null است، که به طور کلی به عنوان false درک می‌شود. برای حل این مشکل، ما می‌توانیم منطق defaulting را یا در جلوی server API یا در خود code application اضافه کنیم (به عنوان مثال، if (field == null) field = true). با این حال، هر دوی این راه‌حل‌ها تا حدودی ناخوشایند هستند، زیرا مکانیزم defaulting از نظر مفهومی از handling request نسبتاً مستقل است. در عوض، ما می‌توانیم از pattern decorator FaaS برای transform کردن request بین user و پیاده‌سازی service استفاده کنیم.
  </p>
<p>
   با توجه به بحث قبلی در مورد the adapter pattern در بخش single-node، ممکن است از خود بپرسید که چرا ما به سادگی این defaulting را به عنوان یک adapter con-
  </p>
<p>
   Patterns for FaaS
   | 85
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 99" src="page_0099/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0099</div>
            </div>
        </div>
        <!-- Page 0100 -->
        <div class="chapter" id="page-0100">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   tainer. و این یک رویکرد کاملاً منطقی است، اما به این معنی است که ما مقیاس service defaulting را با خود service API جفت می‌کنیم. defaulting در واقع یک operation سبک است، و ما احتمالاً به موارد بسیار کمتری از آن نسبت به خود service برای مدیریت load نیاز داریم.
  </p>
<p>
   برای مثال‌های این فصل، ما از فریم‌ورک kubeless FaaS استفاده خواهیم کرد. Kubeless بر روی service container orchestration از نوع Kubernetes مستقر شده است. با فرض اینکه شما یک cluster از نوع Kubernetes را تهیه کرده‌اید، می‌توانید Kubeless را از صفحه releases آن نصب کنید. هنگامی که شما binary kubeless را نصب کردید، می‌توانید آن را با دستورات زیر در cluster خود نصب کنید: kubeless install.
  </p>
<p>
   Kubeless خود را به عنوان یک API third-party از نوع Kubernetes بومی نصب می‌کند. این بدان معنی است که پس از نصب، شما می‌توانید از ابزار command-line native kubectl استفاده کنید. به عنوان مثال، شما می‌توانید functions های مستقر شده را با استفاده از kubectl get functions مشاهده کنید. در حال حاضر، شما نباید هیچ function ای را مستقر کرده باشید.
  </p>
<h4><strong>Hands On: Adding Request Defaulting Prior to Request Processing</strong></h4>
<p>
   برای نشان دادن utility از the decorator pattern برای FaaS، task اضافه کردن default values ها به یک call function RESTful را در نظر بگیرید اگر values ها وجود نداشته باشند. انجام این کار با استفاده از FaaS کاملاً ساده است. ما function defaulting را با استفاده از زبان برنامه‌نویسی Python خواهیم نوشت:
  </p>
<pre>
   <code class="language-python"># Simple handler function for adding default values
def handler(context):
  # Get the input value
  obj = context.json
  # If the 'name' field is not present, set it randomly
  if obj.get("name", None) is None:
    obj["name"] = random_name()
  # If the 'color' field is not present, set it to 'blue'
  if obj.get("color", None) is None:
    obj["color"] = "blue"
  # Call the actual API, potentially with the new default
  # values, and return the result
  return call_my_api(obj)
   </code>
  </pre>
<p>
   این function را در فایلی با نام defaults.py ذخیره کنید. شما بدیهی است که می‌خواهید code call_my_api را به‌روزرسانی کنید تا به API واقعی که می‌خواهید فراخوانی کنید، اشاره کند. هنگامی که شما نوشتن code را به پایان رساندید، این function defaulting را می‌توان به عنوان یک function kubeless با استفاده از دستور زیر نصب کرد:
  </p>
<pre>
   <code class="language-bash">kubeless function deploy add-defaults \
    --runtime python27 \
    --handler defaults.handler \
   </code>
  </pre>
<p>
   86
   | Chapter 8: Functions and Event-Driven Processing
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 100" src="page_0100/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0100</div>
            </div>
        </div>
        <!-- Page 0101 -->
        <div class="chapter" id="page-0101">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre>
   <code class="language-bash">--from-file defaults.py \
    --trigger-http
   </code>
  </pre>
<p>
   اگر شما می‌خواهید handling این function را آزمایش کنید، شما همچنین می‌توانید از ابزار kubeless استفاده کنید:
  </p>
<pre>
   <code class="language-bash">kubeless function call add-defaults --data '{"name": "foo"}'
   </code>
  </pre>
<p>
   the decorator pattern نشان می‌دهد که چقدر آسان است که APIs موجود را با ویژگی‌های اضافی مانند اعتبار‌سنجی یا defaulting، تطبیق داده و گسترش دهید.
  </p>
<h4><strong>Handling Events</strong></h4>
<p>
   در حالی که اکثر systems ها request driven هستند، handling یک جریان ثابت از user و API requests ها، بسیاری از systems های دیگر ذاتاً event-driven هستند. تمایز، حداقل در نظر من، بین یک request و یک event ارتباط با مفهوم session است. Requests ها بخشی از یک سری تعاملات یا sessions بزرگتر هستند. به طور کلی هر user request بخشی از یک تعامل بزرگتر با یک web application یا API کامل است. Events ها، همانطور که من آنها را می‌بینم، تمایل دارند که به جای آن، single-instance و asynchronous باشند. Events ها مهم هستند و باید به درستی مدیریت شوند، اما آنها از یک تعامل اصلی fire می‌شوند و مدتی بعد به آنها پاسخ داده می‌شود. نمونه‌هایی از events ها شامل ثبت نام user برای یک service جدید (که ممکن است یک ایمیل خوش‌آمدگویی را trigger کند، بارگذاری یک فایل توسط شخص در یک پوشه shared (که ممکن است اعلان‌هایی را به همه کسانی که به پوشه دسترسی دارند، ارسال کند)، یا حتی یک machine که قرار است reboot شود (که ممکن است یک operator یا system خودکار را مطلع کند تا اقدام مناسب را انجام دهد).
  </p>
<p>
   از آنجایی که این events ها تمایل دارند که ذاتاً مستقل و stateless باشند، و از آنجایی که rate of events می‌تواند بسیار متغیر باشد، آنها کاندیداهای ایده‌آلی برای architecture های event-driven و FaaS هستند. در این نقش، آنها اغلب در کنار یک application server تولیدی به عنوان augmentation به main user experience یا برای handling نوعی reactive، background processing، مستقر می‌شوند. علاوه بر این، از آنجایی که events های جدید اغلب به طور پویا به service اضافه می‌شوند، ماهیت سبک استقرار functions ها یک match خوب برای تعریف event handlers های جدید است. به همین ترتیب، از آنجایی که هر event از نظر مفهومی مستقل است، decoupling اجباری یک system مبتنی بر functions ها در واقع با فعال کردن یک developer برای تمرکز بر مراحل مورد نیاز برای handling فقط یک نوع event، به کاهش پیچیدگی مفهومی کمک می‌کند.
  </p>
<p>
   یک مثال مشخص از ادغام یک component مبتنی بر event به یک service موجود، پیاده‌سازی two-factor authentication است. در این مورد، event، وارد شدن user به یک service است. service می‌تواند یک event برای این action تولید کند، آن را به یک handler مبتنی بر function fire کند که code و اطلاعات تماس user را می‌گیرد، و کد two-factor را از طریق پیام متنی ارسال می‌کند.
  </p>
<h4><strong>Hands On: Implementing Two-Factor Authentication</strong></h4>
<p>
   Two-factor authentication نیاز دارد که user هم چیزی داشته باشد که می‌داند (به عنوان مثال، یک رمز عبور) و همچنین چیزی که دارد (به عنوان مثال، یک تلفن) تا بتواند
  </p>
<p>
   Patterns for FaaS
   | 87
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0101</div>
            </div>
        </div>
        <!-- Page 0102 -->
        <div class="chapter" id="page-0102">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   log in به system شود. Two-factor authentication به طور قابل توجهی امن‌تر از passwords به تنهایی است زیرا به دو compromise امنیتی مختلف (دزدیدن password شما و دزدیدن تلفن شما) نیاز دارد تا یک مشکل امنیتی واقعی ایجاد شود.
  </p>
<p>
   هنگام در نظر گرفتن نحوه پیاده‌سازی two-factor authentication، یکی از چالش‌ها این است که چگونه درخواست را برای تولید یک code تصادفی و ثبت آن در service login و همچنین ارسال پیام متنی، مدیریت کنیم. افزودن این code به main login web server امکان‌پذیر است. اما این پیچیده و monolithic است، و عمل ارسال یک پیام متنی، که می‌تواند مقداری latency داشته باشد را به همراه code ای قرار می‌دهد که صفحه web login را ارائه می‌دهد. این latency یک user experience زیر استاندارد را تولید می‌کند.
  </p>
<p>
   یک گزینه بهتر این است که یک FaaS را ثبت کنید تا به صورت asynchronous، number تصادفی را تولید کند، آن را در service login ثبت کند و number را به تلفن user ارسال کند. به این ترتیب، server login می‌تواند به سادگی یک request web-hook asynchronous را به یک FaaS fire کند، و آن FaaS می‌تواند task تا حدودی slow و asynchronous از ثبت code two-factor و ارسال پیام متنی را مدیریت کند.
  </p>
<p>
   برای دیدن نحوه عملکرد این در عمل، code زیر را در نظر بگیرید:
  </p>
<pre>
   <code class="language-python">def two_factor(context):
  # Generate a random six digit code
  code = random.randint(100000, 999999)
  # Register the code with the login service
  user = context.json["user"]
  register_code_with_login_service(user, code)
  # Use the twillio library to send texts
  account = "my-account-sid"
  token = "my-token"
  client = twilio.rest.Client(account, token)
  user_number = context.json["phoneNumber"]
  msg = "Hello {} your authentication code is: {}.".format(user, code)
  message = client.api.account.messages.create(to=user_number,
                                               from_="+12065251212",
                                               body=msg)
  return {"status": "ok"}
   </code>
  </pre>
<p>
   سپس ما می‌توانیم این FaaS را با kubeless ثبت کنیم:
  </p>
<pre>
   <code class="language-bash">kubeless function deploy add-two-factor \
    --runtime python27 \
    --handler two_factor.two_factor \
    --from-file two_factor.py \
    --trigger-http
   </code>
  </pre>
<p>
   این function را می‌توان به صورت asynchronous از client-side JavaScript هنگامی که user با موفقیت password خود را ارائه می‌دهد، ایجاد کرد. سپس web UX می‌تواند بلافاصله صفحه‌ای را برای وارد کردن code نمایش دهد، و user (پس از دریافت code به عنوان یک text mes-
  </p>
<p>
   Patterns for FaaS
   | 87
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0102</div>
            </div>
        </div>
        <!-- Page 0103 -->
        <div class="chapter" id="page-0103">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   sage) می‌تواند آن را به service ارائه دهد، جایی که code قبلاً از طریق FaaS ما ثبت شده است.
  </p>
<p>
   باز هم، توسعه یک service ساده، asynchronous و مبتنی بر event که هر زمان که یک user وارد سیستم می‌شود، trigger می‌شود، با استفاده از FaaS به طور چشمگیری ساده‌تر می‌شود.
  </p>
<h4><strong>Event-Based Pipelines</strong></h4>
<p>
   برخی از applications ها وجود دارند که ذاتاً در terms یک pipeline از events های decoupled، راحت‌تر می‌توان در مورد آنها فکر کرد. این event pipelines ها اغلب شبیه فلوچارت‌های قدیمی هستند. آنها را می‌توان به عنوان یک نمودار directed از event syncs های متصل نشان داد. در pattern event pipeline، هر node یک function یا webhook متفاوت است، و edges هایی که نمودار را به هم متصل می‌کنند، HTTP یا سایر network calls ها به function/webhook هستند.
  </p>
<p>
   به طور کلی، هیچ state مشترکی بین قطعات مختلف pipeline وجود ندارد، اما ممکن است یک context یا نقطه مرجع دیگری وجود داشته باشد که بتوان از آن برای lookup اطلاعات در shared storage استفاده کرد.
  </p>
<p>
   بنابراین تفاوت بین این نوع pipeline و یک architecture از نوع "microservices" چیست؟ دو تفاوت اصلی وجود دارد. اولین مورد، تفاوت اصلی بین functions ها به طور کلی و service های طولانی‌مدت است، که یک event-based pipeline ذاتاً event-driven است. برعکس، یک architecture microservices دارای مجموعه‌ای از service های طولانی‌مدت است. علاوه بر این، event-driven pipelines ها ممکن است در مواردی که با هم ارتباط دارند، بسیار asynchronous و متنوع باشند. به عنوان مثال، در حالی که دیدن اینکه چگونه یک انسان که یک ticket را در یک system ticketing مانند Jira تأیید می‌کند، می‌تواند در یک application microservices ادغام شود، دیدن اینکه چگونه آن event می‌تواند در یک event-driven pipeline گنجانده شود، بسیار آسان است.
  </p>
<p>
   به عنوان نمونه‌ای از این، یک pipeline را تصور کنید که در آن event اصلی، code است که به یک system source control ارسال می‌شود. سپس این event، یک build را trigger می‌کند. build ممکن است چندین دقیقه طول بکشد تا تکمیل شود، و وقتی این کار را انجام داد، یک event را به یک build analysis function fire می‌کند. این function اقدامات مختلفی را انجام می‌دهد اگر build موفقیت‌آمیز باشد یا با شکست مواجه شود. اگر build موفقیت‌آمیز بود، یک ticket برای یک انسان ایجاد می‌شود تا آن را تأیید کند تا به production push شود. هنگامی که ticket بسته شد، عمل بستن یک event است که push واقعی به production را trigger می‌کند. اگر build با شکست مواجه شد، یک bug در مورد failure ثبت می‌شود، و event pipeline خاتمه می‌یابد.
  </p>
<h4><strong>Hands On: Implementing a Pipeline for New-User Signup</strong></h4>
<p>
   وظیفه پیاده‌سازی یک جریان signup user جدید را در نظر بگیرید. هنگامی که یک حساب کاربری جدید ایجاد می‌شود، کارهای خاصی همیشه انجام می‌شود، مانند ارسال یک ایمیل خوش‌آمدگویی. و کارهایی وجود دارد که به صورت اختیاری انجام می‌شوند، مانند ثبت نام user برای دریافت updates محصول (که گاهی اوقات به عنوان "spam" از طریق ایمیل آنها شناخته می‌شود).
  </p>
<p>
   یک رویکرد برای پیاده‌سازی این منطق، قرار دادن همه چیز در یک server user-creation monolithic واحد است. با این حال، این factoring به این معنی است که یک تیم واحد
  </p>
<p>
   Patterns for FaaS
   | 89
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0103</div>
            </div>
        </div>
        <!-- Page 0104 -->
        <div class="chapter" id="page-0104">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   صاحب تمامیت service user-creation است، و اینکه کل تجربه به عنوان یک service واحد مستقر شده است. هر دوی اینها به این معنی است که انجام آزمایش‌ها یا ایجاد تغییرات در user experience، دشوارتر است.
  </p>
<p>
   در عوض، پیاده‌سازی user login experience را به عنوان یک event pipeline با مجموعه‌ای از FaaS ها در نظر بگیرید. در این factoring، function user-creation در واقع از جزئیات آنچه در زمان login user اتفاق می‌افتد، بی‌اطلاع است. در عوض، main user-creation service به سادگی دارای دو لیست است:
  </p>
<ul>
<li>یک لیست از اقدامات مورد نیاز (به عنوان مثال، ارسال یک ایمیل خوش‌آمدگویی)</li>
<li>یک لیست از اقدامات اختیاری (به عنوان مثال، ثبت نام user برای دریافت updates محصول (که گاهی اوقات به عنوان "spam" شناخته می‌شود)</li>
</ul>
<p>
   هر یک از این actions ها نیز به عنوان یک FaaS پیاده‌سازی می‌شوند، و لیست actions ها در واقع فقط یک لیست از webhooks ها است. در نتیجه، main user creation function به این صورت است:
  </p>
<pre>
   <code class="language-python">def create_user(context):
  # For required event handlers, call them universally
  for key, value in required.items():
    call_function(value.webhook, context.json)
  # For optional event handlers, check and call them
  # conditionally
  for key, value in optional.items():
    if context.json.get(key, None) is not None:
      call_function(value.webhook, context.json)
   </code>
  </pre>
<p>
   اکنون ما همچنین می‌توانیم از FaaS برای پیاده‌سازی هر یک از این handlers ها استفاده کنیم:
  </p>
<pre>
   <code class="language-python">def email_user(context):
  # Get the user name
  user = context.json['username']
  msg = 'Hello {} thanks for joining my awesome service!".format(user)
  send_email(msg, contex.json['email])
def subscribe_user(context):
  # Get the user name
  email = context.json['email']
  subscribe_user(email)
   </code>
  </pre>
<p>
   به این ترتیب، هر FaaS ساده است، فقط شامل چند خط code است و بر پیاده‌سازی یک قطعه عملکرد خاص متمرکز است. این رویکرد مبتنی بر microservices، نوشتن آن ساده است، اما اگر ما واقعاً مجبور باشیم سه microservices مختلف را مستقر و مدیریت کنیم، ممکن است منجر به پیچیدگی شود. اینجاست که FaaS می‌تواند بدرخشد، زیرا میزبانی این code snippets های کوچک را به آسانی انجام می‌دهد. علاوه بر این، با تجسم جریان user-creation ما به عنوان یک event-driven pipeline، داشتن یک
  </p>
<p>
   Patterns for FaaS
   | 89
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0104</div>
            </div>
        </div>
        <!-- Page 0105 -->
        <div class="chapter" id="page-0105">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   درک high-level از اینکه دقیقاً چه اتفاقی در زمان login user رخ می‌دهد، به سادگی با دنبال کردن جریان context از طریق functions های مختلف در pipeline.
  </p>
<p>
   Patterns for FaaS
   | 91
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0105</div>
            </div>
        </div>
        <!-- Page 0107 -->
        <div class="chapter" id="page-0107">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>CHAPTER 9</strong></h3>
<h4><strong>Ownership Election</strong></h4>
<p>
   الگوهای قبلی که ما دیده‌ایم، در مورد توزیع requests ها برای scale کردن requests ها در ثانیه، state در حال service داده شدن، یا زمان پردازش یک request بوده‌اند. این فصل پایانی در مورد الگوهای serving چند node، در مورد نحوه scale کردن assignment است. در بسیاری از systems های مختلف، یک مفهوم از ownership وجود دارد که در آن یک process خاص، یک task خاص را بر عهده دارد. ما قبلاً این را در context از systems های sharded و hot-sharded دیده‌ایم که در آن instances های خاص، بخش‌های خاصی از space key sharded را در اختیار داشتند.
  </p>
<p>
   در context یک server واحد، ownership به طور کلی ساده است زیرا فقط یک application واحد وجود دارد که ownership را ایجاد می‌کند، و می‌تواند از locks های in-process تثبیت شده برای اطمینان از اینکه فقط یک actor واحد، یک shard یا context خاص را در اختیار دارد، استفاده کند. با این حال، محدود کردن ownership به یک application واحد، scalability را محدود می‌کند، زیرا task نمی‌تواند replicated شود، و reliability را نیز محدود می‌کند، زیرا اگر task با شکست مواجه شود، برای مدتی در دسترس نخواهد بود. در نتیجه، هنگامی که ownership در system شما مورد نیاز است، شما نیاز به توسعه یک system توزیع‌شده برای ایجاد ownership دارید.
  </p>
<p>
   یک نمودار عمومی از distributed ownership در Figure 9-1 نشان داده شده است. در نمودار، سه replica وجود دارد که می‌تواند owner یا master باشد. در ابتدا، اولین replica master است. سپس آن replica با شکست مواجه می‌شود، و replica شماره سه، master می‌شود. در نهایت، replica شماره یک بازیابی می‌شود و به گروه بازمی‌گردد، اما replica سه به عنوان master/owner باقی می‌ماند.
  </p>
<p>
   93
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0107</div>
            </div>
        </div>
        <!-- Page 0108 -->
        <div class="chapter" id="page-0108">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 9-1. A master election protocol in operation: initially the first master is selected, but when it fails, the third master takes over</em>
</p>
<p>
   اغلب، ایجاد distributed ownership هم پیچیده‌ترین و هم مهمترین بخش طراحی یک system توزیع‌شده قابل اعتماد است.
  </p>
<h4><strong>Determining If You Even Need Master Election</strong></h4>
<p>
   ساده‌ترین شکل از ownership، فقط داشتن یک replica از service است. از آنجایی که فقط یک instance در هر زمان در حال اجرا است، آن instance به طور ضمنی همه چیز را بدون نیاز به election در اختیار دارد. این امر مزایایی در ساده‌سازی application و استقرار شما دارد، اما از نظر downtime و reliability معایبی دارد. با این حال، برای بسیاری از applications ها، سادگی این pattern singleton ممکن است ارزش trade-off reliability را داشته باشد. بیایید این موضوع را بیشتر بررسی کنیم.
  </p>
<p>
   با فرض اینکه شما singleton خود را در یک system container orchestration مانند Kubernetes اجرا می‌کنید، شما تضمین‌های زیر را دارید:
  </p>
<ul>
<li>اگر container از کار بیفتد، به طور خودکار راه‌اندازی مجدد می‌شود</li>
</ul>
<p>
   94
   | Chapter 9: Ownership Election
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 108" src="page_0108/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0108</div>
            </div>
        </div>
        <!-- Page 0109 -->
        <div class="chapter" id="page-0109">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<ul>
<li>If the container hangs, and you implement a health check, it will automatically be restarted</li>
<li>If the machine fails, the container will be moved to a different machine</li>
</ul>
<p>
   به دلیل این تضمین‌ها، یک singleton از یک service که در یک container orchestrator در حال اجرا است، uptime نسبتاً خوبی دارد. برای اینکه تعریف "pretty good" را کمی بیشتر توضیح دهیم، بیایید بررسی کنیم که در هر یک از این failure modes ها چه اتفاقی می‌افتد. اگر process container با شکست مواجه شود یا container hang شود، application شما در عرض چند ثانیه راه‌اندازی مجدد می‌شود. با فرض اینکه container شما یک بار در روز crash می‌کند، این تقریباً سه تا چهار نه (nines) uptime است (2 ثانیه downtime / day ~= 99.99% uptime). اگر container شما کمتر crash می‌کند، حتی از این هم بهتر است. اگر machine شما با شکست مواجه شود، مدتی طول می‌کشد تا Kubernetes تصمیم بگیرد که machine با شکست مواجه شده است و آن را به یک machine متفاوت منتقل کند. بیایید فرض کنیم که این حدود 5 دقیقه طول می‌کشد. با توجه به این، اگر هر machine در cluster شما هر روز با شکست مواجه شود، service شما دو نه uptime خواهد داشت. و صادقانه بگویم، اگر هر machine در cluster شما هر روز با شکست مواجه شود، شما مشکلات بسیار بدتری نسبت به uptime service master-elected خود دارید.
  </p>
<p>
   البته، ارزش دارد که در نظر بگیرید که دلایل بیشتری برای downtime وجود دارد تا فقط failures ها. هنگامی که شما در حال rolling out نرم‌افزار جدید هستید، زمان لازم است تا version جدید را دانلود و راه‌اندازی کنید. با یک singleton، شما نمی‌توانید هر دو version قدیمی و جدید را همزمان اجرا کنید، بنابراین شما باید version قدیمی را برای مدت زمان upgrade، که ممکن است چند دقیقه طول بکشد اگر image شما بزرگ باشد، متوقف کنید. در نتیجه، اگر شما به صورت روزانه استقرار می‌دهید و 2 دقیقه طول می‌کشد تا نرم‌افزار خود را upgrade کنید، شما فقط قادر به اجرای یک service با دو نه (nines) خواهید بود، و اگر به صورت ساعتی استقرار می‌دهید، حتی یک service با یک نه (nine) نخواهد بود. البته، راه‌هایی وجود دارد که می‌توانید استقرار خود را با pre-pulling image جدید بر روی machine قبل از اجرای update، سرعت بخشید. این می‌تواند زمان لازم برای استقرار یک version جدید را به چند ثانیه کاهش دهد، اما trade-off این است که complexity اضافه می‌شود، که چیزی است که ما در وهله اول از آن اجتناب می‌کردیم.
  </p>
<p>
   صرف نظر از این، بسیاری از applications ها (به عنوان مثال، background asynchronous processing) وجود دارند که در آنها چنین SLA ای یک trade-off قابل قبول برای simplicity application است. یکی از اجزای اصلی طراحی یک system توزیع‌شده، تصمیم‌گیری در مورد زمان complex بودن غیرضروری قسمت "distributed" است. اما مطمئناً موقعیت‌هایی وجود دارد که high availability (چهار+ نه) یک component critical از application است، و در چنین systems ها شما نیاز دارید که چندین replica از service را اجرا کنید، که در آن فقط یک replica، owner تعیین شده است. طراحی این نوع از systems ها در بخش‌های زیر شرح داده شده است.
  </p>
<h4><strong>The Basics of Master Election</strong></h4>
<p>
   تصور کنید که یک service به نام Foo با سه replica وجود دارد: Foo-1، Foo-2 و Foo-3. همچنین یک object به نام Bar وجود دارد که فقط باید توسط یکی از replicas ها (به عنوان مثال، Foo-1) در یک زمان "owned" شود. اغلب این replica master نامیده می‌شود، از این رو اصطلاح master election برای
  </p>
<p>
   The Basics of Master Election
   | 95
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0109</div>
            </div>
        </div>
        <!-- Page 0110 -->
        <div class="chapter" id="page-0110">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   describe the process از چگونگی انتخاب این master و همچنین چگونگی انتخاب یک master جدید در صورت failure آن master.
  </p>
<p>
   دو راه برای پیاده‌سازی این master election وجود دارد. اولین راه این است که یک algorithm consensus توزیع‌شده مانند Paxos یا RAFT را پیاده‌سازی کنید، اما پیچیدگی این algorithms ها آنها را فراتر از محدوده این کتاب قرار می‌دهد و پیاده‌سازی آنها ارزشمند نیست. پیاده‌سازی یکی از این algorithms ها شبیه پیاده‌سازی locks ها بر روی دستورالعمل‌های compare-and-swap از assembly code است. این یک تمرین جالب برای یک دوره علوم کامپیوتر در مقطع کارشناسی است، اما چیزی نیست که به طور کلی در عمل ارزش انجام دادن داشته باشد.
  </p>
<p>
   خوشبختانه، تعداد زیادی از distributed key-value stores ها وجود دارد که چنین algorithms های consensus را برای شما پیاده‌سازی کرده‌اند. در یک سطح کلی، این systems ها یک data store replicated و قابل اعتماد و primitive های لازم برای ساختن abstractions های locking و election پیچیده‌تر در بالای آن، ارائه می‌دهند. نمونه‌هایی از این stores های توزیع‌شده شامل etcd، ZooKeeper و consul هستند. primitive های اساسی که این systems ها ارائه می‌دهند، توانایی انجام یک operation compare-and-swap برای یک key خاص است. اگر قبلاً compare-and-swap را ندیده‌اید، اساساً یک operation atomic است که به این صورت است:
  </p>
<pre>
   <code class="language-go">var lock = sync.Mutex{}
var store = map[string]string{}
func compareAndSwap(key, nextValue, currentValue string) (bool, error) {
  lock.Lock()
  defer lock.Unlock()
  if _, found := store[key]; found {
    if len(currentValue) == 0 {
      store[key] = nextValue
      return true, nil
    }
    return false, fmt.Errorf("Expected value %s for key %s, but
    found empty", currentValue, key)
  }
  if store[key] == currentValue {
    store[key] = nextValue
    return true, nil
  }
  return false, nil
}
   </code>
  </pre>
<p>
   Compare-and-swap به طور atomic یک value جدید را می‌نویسد اگر value موجود با value مورد انتظار مطابقت داشته باشد. اگر value مطابقت نداشته باشد، false برمی‌گرداند. اگر value وجود نداشته باشد و currentValue null نباشد، یک error برمی‌گرداند.
  </p>
<p>
   علاوه بر compare-and-swap، key-value stores ها به شما اجازه می‌دهند تا یک time-to-live (TTL) را برای یک key تنظیم کنید. هنگامی که TTL منقضی می‌شود، key به حالت خالی برمی‌گردد.
  </p>
<p>
   با هم، این functions ها برای پیاده‌سازی انواع مختلفی از distributed synchronization primitives کافی هستند.
  </p>
<p>
   96
   | Chapter 9: Ownership Election
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0110</div>
            </div>
        </div>
        <!-- Page 0111 -->
        <div class="chapter" id="page-0111">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h4><strong>Hands On: Deploying etcd</strong></h4>
<p>
   etcd یک distributed lock server است که توسط CoreOS توسعه داده شده است. این در production و در مقیاس بالا، قوی و ثابت شده است، و توسط انواع پروژه‌ها از جمله Kubernetes استفاده می‌شود.
  </p>
<p>
   استقرار etcd خوشبختانه به دلیل توسعه دو پروژه متن‌باز (open source) مختلف، بسیار آسان شده است:
  </p>
<ul>
<li>Helm: یک package manager از نوع Kubernetes که توسط Microsoft Azure پشتیبانی می‌شود</li>
<li>The etcd operator که توسط CoreOS توسعه داده شده است</li>
</ul>
<p>
   Operators یک موضوع جالب است که توسط CoreOS بررسی می‌شود. An operator یک program آنلاین است که در داخل container orchestrator شما با هدف صریح اجرای یک یا چند application اجرا می‌شود. operator مسئول ایجاد، scaling و نگهداری از عملکرد موفقیت‌آمیز program است. Users، application را از طریق یک desired state API پیکربندی می‌کنند. به عنوان مثال، the etcd operator مسئول نظارت بر خود etcd است. Operators هنوز یک ایده جدید است اما یک جهت جدید مهم در ساختن systems توزیع‌شده قابل اعتماد را نشان می‌دهد.
  </p>
<p>
   برای استقرار the etcd operator برای CoreOS، ما از ابزار package management helm استفاده خواهیم کرد. Helm یک package manager متن‌باز (open source) است که بخشی از پروژه Kubernetes است و توسط Deis توسعه داده شده است. Deis توسط Microsoft Azure در سال 2017 خریداری شد و مایکروسافت همچنان از توسعه متن‌باز (open source) بیشتر Helm پشتیبانی می‌کند.
  </p>
<p>
   اگر این اولین بار است که از helm استفاده می‌کنید، شما نیاز دارید که ابزار helm را نصب کنید، با دنبال کردن دستورالعمل‌های موجود در اینجا: https://github.com/kubernetes/helm/releases.
  </p>
<p>
   هنگامی که شما ابزار helm را در محیط خود نصب کردید، شما می‌توانید the etcd operator را با استفاده از helm به صورت زیر نصب کنید:
  </p>
<pre>
   <code class="language-bash"># Initialize helm
helm init
# Install the etcd operator
helm install stable/etcd-operator
   </code>
  </pre>
<p>
   پس از نصب operator، یک resource از نوع Kubernetes custom را برای نشان دادن cluster etcd ایجاد می‌کند. operator در حال اجرا است، اما هنوز هیچ cluster etcd وجود ندارد. برای ایجاد یک cluster etcd، شما نیاز دارید که یک configuration declarative را ایجاد کنید:
  </p>
<pre>
   <code class="language-yaml">apiVersion: "etcd.coreos.com/v1beta1"
kind: "Cluster"
metadata:
  # Whatever name you want here
  name: "my-etcd-cluster"
   </code>
  </pre>
<p>
   The Basics of Master Election
   | 97
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 111" src="page_0111/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0111</div>
            </div>
        </div>
        <!-- Page 0112 -->
        <div class="chapter" id="page-0112">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre>
   <code class="language-yaml"> spec:
  # 1, 3, 5 are the options for size
  size: 3
  # The version of etcd to install
  version: "3.1.0"
   </code>
  </pre>
<p>
   این configuration را در etcd-cluster.yaml ذخیره کنید و سپس cluster را با استفاده از دستور زیر ایجاد کنید:
  </p>
<pre>
   <code class="language-bash">kubectl create -f etcd-cluster.yaml.
   </code>
  </pre>
<p>
   ایجاد این cluster باعث می‌شود که operator ها، pods ها را برای replicas های cluster etcd ایجاد کند. شما می‌توانید replicas های در حال اجرا را با استفاده از دستور زیر مشاهده کنید:
  </p>
<pre>
   <code class="language-bash">kubectl get pods
   </code>
  </pre>
<p>
   هنگامی که هر سه replica در حال اجرا هستند، شما می‌توانید endpoints های آنها را با استفاده از دستور زیر دریافت کنید:
  </p>
<pre>
   <code class="language-bash">export ETCD_ENDPOINTS=kubectl get endpoints example-etcd-cluster
"-o=jsonpath={.subsets[*].addresses[*].ip}:2379,"
   </code>
  </pre>
<p>
   سپس شما می‌توانید چیزی را با استفاده از دستور زیر در etcd ذخیره کنید:
  </p>
<pre>
   <code class="language-bash">kubectl exec my-etcd-cluster-0000 -- sh -c "ETCD_API=3 etcdctl
--endpoints=${ETCD_ENDPOINTS} set foo bar"
   </code>
  </pre>
<h4><strong>Implementing Locks</strong></h4>
<p>
   ساده‌ترین شکل synchronization، mutual exclusion lock (همچنین به نام Mutex) است. هر کسی که برنامه‌نویسی همزمان را بر روی یک machine واحد انجام داده است، با locks ها آشنا است، و همان مفهوم را می‌توان برای distributed replicas اعمال کرد. به جای حافظه محلی و دستورالعمل‌های assembly، این distributed locks ها را می‌توان بر حسب distributed key-value stores که قبلاً شرح داده شد، پیاده‌سازی کرد.
  </p>
<p>
   همانند locks در memory، اولین قدم این است که lock را به دست آورید:
  </p>
<pre>
   <code class="language-go">func (Lock l) simpleLock() boolean {
  // compare and swap "1" for "0"
  locked, _ = compareAndSwap(l.lockName, "1", "0")
  return locked
}
   </code>
  </pre>
<p>
   اما البته، این امکان وجود دارد که lock قبلاً وجود نداشته باشد، زیرا ما اولین کسی هستیم که آن را claim کرده‌ایم، بنابراین ما باید آن مورد را نیز مدیریت کنیم:
  </p>
<pre>
   <code class="language-go">func (Lock l) simpleLock() boolean {
  // compare and swap "1" for "0"
  locked, error = compareAndSwap(l.lockName, "1", "0")
  // lock doesn't exist, try to write "1" with a previous value of
  // non-existent
  if error != nil {
    locked, _ = compareAndSwap(l.lockName, "1", nil)
  }
  return locked
}
   </code>
  </pre>
<p>
   98
   | Chapter 9: Ownership Election
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0112</div>
            </div>
        </div>
        <!-- Page 0113 -->
        <div class="chapter" id="page-0113">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   پیاده‌سازی‌های سنتی lock، تا زمان به دست آوردن lock، block می‌شوند، بنابراین ما در واقع چیزی شبیه به این می‌خواهیم:
  </p>
<pre>
   <code class="language-go">func (Lock l) lock() {
  while (!l.simpleLock()) {
    sleep(2)
  }
}
   </code>
  </pre>
<p>
   این پیاده‌سازی، اگرچه ساده است، این مشکل را دارد که شما همیشه حداقل یک ثانیه پس از آزاد شدن lock صبر می‌کنید تا lock را به دست آورید. خوشبختانه، بسیاری از key-value stores ها به شما اجازه می‌دهند که به جای polling، برای changes ها watch کنید، بنابراین شما می‌توانید پیاده‌سازی کنید:
  </p>
<pre>
   <code class="language-go">func (Lock l) lock() {
  while (!l.simpleLock()) {
    waitForChanges(l.lockName)
  }
}
   </code>
  </pre>
<p>
   با توجه به این locking function، ما همچنین می‌توانیم unlock را پیاده‌سازی کنیم:
  </p>
<pre>
   <code class="language-go">func (Lock l) unlock() {
  compareAndSwap(l.lockName, "0", "1")
}
   </code>
  </pre>
<p>
   شما ممکن است اکنون فکر کنید که ما کارمان تمام شده است، اما به یاد داشته باشید که ما این را برای یک system توزیع‌شده می‌سازیم. یک process می‌تواند در اواسط نگه داشتن lock با شکست مواجه شود، و در آن مرحله هیچ‌کس برای release کردن آن باقی نمی‌ماند. در چنین موقعیتی، system ما stuck خواهد شد. برای حل این مشکل، ما از functionality TTL از key-value store استفاده می‌کنیم. ما function simpleLock خود را تغییر می‌دهیم تا همیشه با یک TTL بنویسد، بنابراین اگر ما در یک زمان معین unlock نکنیم، lock به طور خودکار unlock می‌شود.
  </p>
<pre>
   <code class="language-go">func (Lock l) simpleLock() boolean {
  // compare and swap "1" for "0"
  locked, error = compareAndSwap(l.lockName, "1", "0", l.ttl)
  // lock doesn't exist, try to write "1" with a previous value of
  // non-existent
  if error != nil {
    locked, _ = compareAndSwap(l.lockName, "1", nil, l.ttl)
  }
  return locked
}
   </code>
  </pre>
<p>
   هنگام استفاده از distributed locks ها، اطمینان از اینکه هر processing ای که شما انجام می‌دهید، بیشتر از TTL از lock طول نمی‌کشد، بسیار مهم است. یک practice خوب این است که یک watchdog timer را هنگامی که lock را به دست می‌آورید، تنظیم کنید.
   watchdog حاوی یک assertion است که اگر TTL از lock منقضی شود قبل از اینکه شما unlock را فراخوانی کنید، program شما را crash می‌کند.
  </p>
<p>
   The Basics of Master Election
   | 99
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 113" src="page_0113/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0113</div>
            </div>
        </div>
        <!-- Page 0114 -->
        <div class="chapter" id="page-0114">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   با اضافه کردن TTL به locks های خود، ما در واقع یک bug را در function unlock خود معرفی کرده‌ایم. سناریوی زیر را در نظر بگیرید:
  </p>
<p>
   1. Process-1 lock را با TTL t به دست می‌آورد.
  </p>
<p>
   2. Process-1 به دلایلی بسیار کندتر از t اجرا می‌شود.
  </p>
<p>
   3. The lock منقضی می‌شود.
  </p>
<p>
   4. Process-2 lock را به دست می‌آورد، زیرا Process-1 آن را به دلیل TTL از دست داده است.
  </p>
<p>
   5. Process-1 تمام می‌شود و unlock را فراخوانی می‌کند.
  </p>
<p>
   6. Process-3 lock را به دست می‌آورد.
  </p>
<p>
   در این مرحله، Process-1 معتقد است که lock ای را که در ابتدا در اختیار داشت، unlock کرده است. او نمی‌فهمد که در واقع lock را به دلیل TTL از دست داده است، و در واقع lock را که توسط Process-2 در اختیار داشت، unlock کرده است. سپس Process-3 می‌آید و lock را نیز می‌گیرد. اکنون هم Process-2 و هم Process-3 معتقدند که lock را در اختیار دارند، و hilarity ادامه دارد.
  </p>
<p>
   خوشبختانه، key-value store یک resource version را برای هر write ای که انجام می‌شود، ارائه می‌دهد. function lock ما می‌تواند این resource version را ذخیره کند و compareAnd Swap را افزایش دهد تا اطمینان حاصل شود که نه تنها value همانطور که انتظار می‌رود است، بلکه resource version نیز با زمانی که operation lock رخ داد، یکسان است. این function simple Lock ما را به این صورت تغییر می‌دهد:
  </p>
<pre>
   <code class="language-go">func (Lock l) simpleLock() boolean {
  // compare and swap "1" for "0"
  locked, l.version, error = compareAndSwap(l.lockName, "1", "0", l.ttl)
  // lock doesn't exist, try to write "1" with a previous value of
  // non-existent
  if error != null {
    locked, l.version, _ = compareAndSwap(l.lockName, "1", nil, l.ttl)
  }
  return locked
}
   </code>
  </pre>
<p>
   و سپس function unlock به این صورت به نظر می‌رسد:
  </p>
<pre>
   <code class="language-go">func (Lock l) unlock() {
  compareAndSwap(l.lockName, "0", "1", l.version)
}
   </code>
  </pre>
<p>
   این تضمین می‌کند که lock فقط در صورتی unlock می‌شود که TTL منقضی نشده باشد.
  </p>
<h4><strong>Hands On: Implementing Locks in etcd</strong></h4>
<p>
   برای پیاده‌سازی locks ها در etcd، شما می‌توانید از یک key به عنوان نام lock و pre- condition writes استفاده کنید تا اطمینان حاصل کنید که فقط یک lock holder در یک زمان مجاز است. برای سادگی، ما از command line از نوع etcdctl برای lock و unlock کردن lock استفاده خواهیم کرد. در واقعیت، از
   100
   | Chapter 9: Ownership Election
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0114</div>
            </div>
        </div>
        <!-- Page 0115 -->
        <div class="chapter" id="page-0115">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   البته، شما می‌خواهید از یک زبان برنامه‌نویسی استفاده کنید. etcd clients ها برای اکثر زبان‌های برنامه‌نویسی محبوب وجود دارد.
  </p>
<p>
   بیایید با ایجاد یک lock با نام my-lock شروع کنیم:
  </p>
<pre>
   <code class="language-bash">kubectl exec my-etcd-cluster-0000 -- sh -c \
  "ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} set my-lock unlocked"
   </code>
  </pre>
<p>
   این یک key را در etcd با نام my-lock ایجاد می‌کند و value اولیه را به unlocked تنظیم می‌کند.
  </p>
<p>
   اکنون بیایید فرض کنیم که Alice و Bob هر دو می‌خواهند ownership از my-lock را بر عهده بگیرند. Alice و Bob هر دو تلاش می‌کنند نام خود را با استفاده از یک precondition که value از lock، unlocked است، در lock بنویسند.
  </p>
<p>
   Alice ابتدا اجرا می‌شود:
  </p>
<pre>
   <code class="language-bash">kubectl exec my-etcd-cluster-0000 -- sh -c \
  "ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \
      set --swap-with-value unlocked my-lock alice"
   </code>
  </pre>
<p>
   و lock را به دست می‌آورد. اکنون Bob تلاش می‌کند تا lock را به دست آورد:
  </p>
<pre>
   <code class="language-bash">kubectl exec my-etcd-cluster-0000 -- sh -c \
  "ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \
      set --swap-with-value unlocked my-lock bob"
   </code>
  </pre>
<pre>
   <code class="language-bash">Error:  101: Compare failed ([unlocked != alice]) [6]
   </code>
  </pre>
<p>
   شما می‌توانید ببینید که تلاش Bob برای claim کردن lock با شکست مواجه شده است، زیرا Alice در حال حاضر lock را در اختیار دارد.
  </p>
<p>
   برای unlock کردن lock، Alice، unlocked را با یک precondition value از alice می‌نویسد:
  </p>
<pre>
   <code class="language-bash">kubectl exec my-etcd-cluster-0000 -- sh -c \
  "ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \
      set --swap-with-value alice my-lock unlocked"
   </code>
  </pre>
<h4><strong>Implementing Ownership</strong></h4>
<p>
   در حالی که locks ها برای ایجاد ownership موقت از یک component critical عالی هستند، گاهی اوقات شما می‌خواهید ownership را برای مدت زمانی که component در حال اجرا است، بر عهده بگیرید. به عنوان مثال، در یک deployment high available از Kubernetes، چندین replica از scheduler وجود دارد اما فقط یک replica به طور فعال در حال تصمیم‌گیری در مورد scheduling است. علاوه بر این، هنگامی که به active scheduler تبدیل می‌شود، تا زمانی که آن process به دلایلی با شکست مواجه شود، active scheduler باقی می‌ماند.
  </p>
<p>
   بدیهی است، یک راه برای انجام این کار این است که TTL را برای lock به یک دوره بسیار طولانی (مثلاً یک هفته یا بیشتر) گسترش دهید، اما این مشکل مهم را دارد که اگر owner lock فعلی با شکست مواجه شود، یک owner lock جدید تا زمان انقضای TTL یک هفته بعد انتخاب نمی‌شود.
  </p>
<p>
   در عوض، ما باید یک renewable lock ایجاد کنیم، که می‌تواند به طور دوره‌ای توسط owner تمدید شود تا lock بتواند برای یک دوره زمانی دلخواه حفظ شود.
  </p>
<p>
   The Basics of Master Election
   | 101
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0115</div>
            </div>
        </div>
        <!-- Page 0116 -->
        <div class="chapter" id="page-0116">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   ما می‌توانیم Lock موجود را که قبلاً تعریف کردیم، گسترش دهیم تا یک renewable lock ایجاد کنیم، که به lock holder این امکان را می‌دهد تا lock را تمدید کند:
  </p>
<pre>
   <code class="language-go">func (Lock l) renew() boolean {
  locked, _ = compareAndSwap(l.lockName, "1", "1", l.version, ttl)
  return locked
}
   </code>
  </pre>
<p>
   البته، شما احتمالاً می‌خواهید این کار را به طور مکرر در یک thread جداگانه انجام دهید تا lock را به طور نامحدود در اختیار داشته باشید. توجه داشته باشید که lock هر ttl/2 ثانیه تمدید می‌شود. به این ترتیب خطر اینکه lock به طور تصادفی به دلیل ظرافت‌های زمان‌بندی منقضی شود، به طور قابل توجهی کمتر است:
  </p>
<pre>
   <code class="language-go">for {
  if !l.renew() {
    handleLockLost()
  }
  sleep(ttl/2)
}
   </code>
  </pre>
<p>
   البته، شما نیاز دارید function handleLockLost() را پیاده‌سازی کنید تا تمام فعالیت‌هایی را که در وهله اول نیازمند lock بودند، خاتمه دهید. در یک system container orchestration، ساده‌ترین راه برای انجام این کار ممکن است به سادگی خاتمه دادن به application و اجازه دادن به orchestrator برای راه‌اندازی مجدد آن باشد. این ایمن است، زیرا یک replica دیگر در این بین، lock را گرفته است، و هنگامی که application راه‌اندازی مجدد می‌شود، به یک listener ثانویه تبدیل می‌شود که منتظر آزاد شدن lock است.
  </p>
<h4><strong>Hands On: Implementing Leases in etcd</strong></h4>
<p>
   برای دیدن نحوه پیاده‌سازی leases ها با استفاده از etcd، ما به مثال locking قبلی خود بازمی‌گردیم و flag --ttl=<seconds> را به calls های create و update lock خود اضافه می‌کنیم. flag ttl یک زمان را تعریف می‌کند که پس از آن lock که ما ایجاد می‌کنیم، حذف می‌شود. از آنجایی که lock بعد از منقضی شدن ttl ناپدید می‌شود، به جای ایجاد با value از نوع unlocked، ما فرض می‌کنیم که عدم وجود lock به این معنی است که آن unlock شده است. برای انجام این کار، ما از دستور mk به جای دستور set استفاده می‌کنیم. etcdctl mk فقط در صورتی موفق می‌شود که key در حال حاضر وجود نداشته باشد.
  </seconds></p>
<p>
   بنابراین، برای lock کردن یک leased lock، Alice اجرا می‌کند:
  </p>
<pre>
   <code class="language-bash">kubectl exec my-etcd-cluster-0000 -- \
    sh -c "ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \
        --ttl=10 mk my-lock alice"
   </code>
  </pre>
<p>
   این یک leased lock با مدت زمان 10 ثانیه ایجاد می‌کند.
  </p>
<p>
   برای اینکه Alice همچنان lock را در اختیار داشته باشد، او باید اجرا کند:
  </p>
<pre>
   <code class="language-bash">kubectl exec my-etcd-cluster-0000 -- \
    sh -c "ETCD_API=3 etcdctl --endpoints=${ETCD_ENDPOINTS} \
        set --ttl=10 --swap-with-value alice my-lock alice"
   </code>
  </pre>
<p>
   102
   | Chapter 9: Ownership Election
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0116</div>
            </div>
        </div>
        <!-- Page 0117 -->
        <div class="chapter" id="page-0117">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   ممکن است عجیب به نظر برسد که Alice به طور مداوم نام خود را در lock بازنویسی می‌کند، اما این روشی است که lease lock از 10-second TTL فراتر می‌رود.
  </p>
<p>
   اگر به هر دلیلی، TTL منقضی شود، پس update lock با شکست مواجه می‌شود، و Alice به ایجاد lock با استفاده از دستور etcd mk برمی‌گردد، یا Bob ممکن است از دستور mk برای به دست آوردن lock برای خود استفاده کند. Bob به همین ترتیب نیاز دارد که lock را هر 10 ثانیه تنظیم و update کند تا ownership را حفظ کند.
  </p>
<h4><strong>Handling Concurrent Data Manipulation</strong></h4>
<p>
   حتی با تمام mechanisms های locking که ما شرح داده‌ایم، هنوز هم ممکن است که دو replica به طور همزمان معتقد باشند که lock را برای یک دوره زمانی بسیار کوتاه در اختیار دارند.
  </p>
<p>
   برای درک اینکه چگونه این اتفاق می‌تواند رخ دهد، تصور کنید که lock holder اصلی آنقدر غرق شده است که processor آن برای چند دقیقه در یک زمان متوقف می‌شود. این می‌تواند در machine های بیش از حد برنامه‌ریزی شده رخ دهد. در چنین موردی، lock time out می‌شود و برخی از replica های دیگر، lock را در اختیار خواهند داشت. اکنون processor، replica ای را که lock holder اصلی بود، آزاد می‌کند. بدیهی است، function handleLockLost() به سرعت فراخوانی می‌شود، اما یک دوره کوتاه وجود خواهد داشت که replica هنوز معتقد است که lock را در اختیار دارد. اگرچه چنین event ای نسبتاً بعید است، systems ها باید طوری ساخته شوند که در برابر این رخدادها مقاوم باشند. اولین قدم این است که دوباره بررسی کنید که آیا lock هنوز در اختیار است یا خیر، با استفاده از یک function مانند این:
  </p>
<pre>
   <code class="language-go">func (Lock l) isLocked() boolean {
  return l.locked &amp;&amp; l.lockTime + 0.75 * l.ttl &gt; now()
}
   </code>
  </pre>
<p>
   اگر این function قبل از هر code ای که نیاز به محافظت توسط یک lock دارد، اجرا شود، احتمال فعال بودن دو master به طور قابل توجهی کاهش می‌یابد، اما – مهم است که توجه داشته باشید – به طور کامل حذف نمی‌شود. timeout lock همیشه می‌تواند بین زمانی که lock بررسی شد و code محافظت شده اجرا شد، رخ دهد. برای محافظت در برابر این سناریوها، system که از replica فراخوانی می‌شود، نیاز دارد که اعتبار‌سنجی کند که replica که در حال ارسال یک request است، در واقع همچنان master است. برای انجام این کار، hostname از replica که lock را در اختیار دارد، علاوه بر state از lock، در key-value store ذخیره می‌شود. به این ترتیب، دیگران می‌توانند دوباره بررسی کنند که آیا یک replica که ادعا می‌کند master است، در واقع master است یا خیر.
  </p>
<p>
   این diagram system در Figure 9-2 نشان داده شده است. در اولین تصویر، shard2 owner از lock است، و هنگامی که یک request به worker ارسال می‌شود، worker با lock server دوباره بررسی می‌کند و اعتبار‌سنجی می‌کند که shard2 در واقع owner فعلی است.
  </p>
<p>
   Handling Concurrent Data Manipulation
   | 103
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0117</div>
            </div>
        </div>
        <!-- Page 0118 -->
        <div class="chapter" id="page-0118">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<em>Figure 9-2. A worker double-checking to validate that the requester who sent a message is actually the current owner of the shard</em>
</p>
<p>
   در مورد دوم، shard2، ownership از lock را از دست داده است، اما هنوز این موضوع را درک نکرده است، بنابراین به ارسال requests ها به worker node ادامه می‌دهد. این بار، هنگامی که worker node یک request را از shard2 دریافت می‌کند، با service lock دوباره بررسی می‌کند و متوجه می‌شود که shard2 دیگر owner از lock نیست، و بنابراین requests ها رد می‌شوند.
  </p>
<p>
   برای اضافه کردن یک wrinkle نهایی پیچیده‌کننده دیگر، همیشه این امکان وجود دارد که ownership بتواند به دست آید، از دست برود، و سپس دوباره توسط system به دست آید، که در واقع می‌تواند باعث شود یک request با موفقیت انجام شود، در حالی که در واقع باید رد شود. برای درک چگونگی امکان این امر، توالی رویدادهای زیر را در نظر بگیرید:
  </p>
<p>
   1. Shard-1 ownership را به دست می‌آورد تا master شود.
  </p>
<p>
   2. Shard-1 یک request به نام R1 را به عنوان master در زمان T1 ارسال می‌کند.
  </p>
<p>
   3. network دچار hiccup می‌شود و delivery از R1 به تأخیر می‌افتد.
  </p>
<p>
   4. Shard-1 به دلیل network، TTL را از دست می‌دهد و lock را به Shard-2 از دست می‌دهد.
  </p>
<p>
   5. Shard-2 به master تبدیل می‌شود و یک request به نام R2 را در زمان T2 ارسال می‌کند.
  </p>
<p>
   6. Request R2 دریافت و پردازش می‌شود.
  </p>
<p>
   7. Shard-2 crash می‌کند و ownership را به Shard-1 از دست می‌دهد.
  </p>
<p>
   8. Request R1 در نهایت می‌رسد، و Shard-1 master فعلی است، بنابراین پذیرفته می‌شود، اما این بد است زیرا R2 قبلاً پردازش شده است.
  </p>
<p>
   چنین توالی از events ها، byzantine به نظر می‌رسند، اما در واقعیت، در هر system بزرگ، آنها با فرکانس نگران‌کننده‌ای رخ می‌دهند. خوشبختانه، این شبیه به موردی است که قبلاً توضیح داده شد، که ما با resource version در etcd حل کردیم. ما می‌توانیم همین کار را اینجا انجام دهیم. علاوه بر ذخیره نام owner فعلی در etcd، ما همچنین resource version را همراه با هر request ارسال می‌کنیم. بنابراین در مثال قبلی، R1 به (R1, Version1) تبدیل می‌شود. اکنون وقتی request دریافت شد، double-check، هر دو را اعتبار‌سنجی می‌کند
  </p>
<p>
   104
   | Chapter 9: Ownership Election
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 118" src="page_0118/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0118</div>
            </div>
        </div>
        <!-- Page 0119 -->
        <div class="chapter" id="page-0119">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Handling Concurrent Data Manipulation</h3>
<p>
   متن اصلی به نظر می‌رسد ناقص است و اطلاعات کافی برای ترجمه کامل را ندارد. با این حال، من سعی می‌کنم با توجه به اطلاعات موجود، ترجمه را انجام دهم و نکات لازم را رعایت کنم.
  </p>
<p>
   به نظر می رسد که این متن در مورد مدیریت همزمانی داده‌ها است.
  </p>
<p>
   عبارت "the current owner and the resource version of the request. If either match fails, the request is rejected" اشاره به بررسی <strong>owner</strong> فعلی و <strong>resource version</strong> درخواست دارد. اگر هر کدام از این موارد با هم تطابق نداشته باشند، درخواست رد می‌شود.
  </p>
<p>
   این بخش از متن احتمالا به توضیح چگونگی <strong>patch</strong> کردن یک <strong>example</strong> می‌پردازد تا با مشکل همزمانی داده‌ها مقابله کند.
  </p>
<p>
   متاسفانه، با توجه به اطلاعات موجود، ترجمه بیشتر از این امکان‌پذیر نیست. برای ترجمه دقیق‌تر، نیاز به متن کامل‌تری است.
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0119</div>
            </div>
        </div>
        <!-- Page 0121 -->
        <div class="chapter" id="page-0121">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>PART III</strong></h3>
<h4><strong>Batch Computational Patterns</strong></h4>
<p>
   فصل‌های قبلی، الگوهایی را برای applications های server قابل اعتماد و طولانی‌مدت شرح دادند. این بخش، الگوهایی برای batch processing را توصیف می‌کند. در مقابل applications های طولانی‌مدت، انتظار می‌رود که process های batch فقط برای یک دوره زمانی کوتاه اجرا شوند. نمونه‌هایی از یک process batch شامل ایجاد aggregation از data telemetry user، تجزیه و تحلیل data فروش برای گزارش‌دهی روزانه یا هفتگی، یا transcoding video files است.
  </p>
<p>
   Process های Batch به طور کلی با نیاز به پردازش مقادیر زیادی data به سرعت با استفاده از parallelism برای سرعت بخشیدن به processing، مشخص می‌شوند. معروف‌ترین pattern برای distributed batch processing، pattern MapReduce است، که خود به یک صنعت کامل تبدیل شده است. با این حال، چندین pattern دیگر وجود دارد که برای batch processing مفید هستند، که در فصل‌های زیر شرح داده شده‌اند.
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0121</div>
            </div>
        </div>
        <!-- Page 0123 -->
        <div class="chapter" id="page-0123">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3><strong>CHAPTER 10</strong></h3>
<h4><strong>Work Queue Systems</strong></h4>
<p>
   ساده‌ترین شکل batch processing، یک work queue است. در یک system work queue، یک batch از work وجود دارد که باید انجام شود. هر قطعه از work کاملاً مستقل از دیگری است و می‌تواند بدون هیچ تعاملی پردازش شود. به طور کلی، اهداف system work queue این است که اطمینان حاصل شود که هر قطعه از work در یک مقدار زمان مشخص پردازش می‌شود. Workers ها برای اطمینان از اینکه work قابل مدیریت است، scale up یا scale down می‌شوند. یک تصویر از یک work queue عمومی در Figure 10-1 نشان داده شده است.
  </p>
<p>
<em>Figure 10-1. A generic work queue</em>
</p>
<h4><strong>A Generic Work Queue System</strong></h4>
<p>
   work queue یک راه ایده‌آل برای نشان دادن قدرت الگوهای system توزیع‌شده است. بیشتر logic در work queue کاملاً مستقل از work واقعی است که انجام می‌شود، و در بسیاری از موارد، delivery از work نیز می‌تواند به شیوه‌ای مستقل انجام شود. برای نشان دادن این نکته، work queue را که در Figure 10-1 نشان داده شده است، در نظر بگیرید. اگر ما دوباره به آن نگاهی بیندازیم و عملکردی را که می‌تواند توسط یک مجموعه مشترک از container های library ارائه شود، شناسایی کنیم، مشخص می‌شود که بیشتر پیاده‌سازی یک work queue از نوع containerized، می‌تواند در سراسر طیف گسترده‌ای از users ها به اشتراک گذاشته شود، همانطور که در Figure 10-2 نشان داده شده است.
  </p>
<p>
   109
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 123" src="page_0123/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0123</div>
            </div>
        </div>
        <!-- Page 0124 -->
        <div class="chapter" id="page-0124">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 10-2.</h3>
<p>
   The same work queue as shown in Figure 10-1, but this time using reusable containers. The reusable system containers are shown in white while the user-supplied container is shown is grey/blue.
  </p>
<p>
<strong>Building a reusable container-based work queue</strong> نیازمند تعریف <strong>interfaces</strong> بین <strong>generic library containers</strong> و <strong>user-defined application logic</strong> است. در <strong>containerized work queue</strong>، دو <strong>interface</strong> وجود دارد: <strong>the source container interface</strong>، که یک <strong>stream</strong> از <strong>work items</strong> را فراهم می‌کند که نیاز به پردازش دارند، و <strong>the worker container interface</strong>، که می‌داند چگونه یک <strong>work item</strong> را پردازش کند.
  </p>
<h4>The Source Container Interface</h4>
<p>
   برای عمل کردن، هر <strong>work queue</strong> به مجموعه‌ای از <strong>work items</strong> نیاز دارد که باید پردازش شوند. منابع مختلفی از <strong>work items</strong> برای <strong>work queue</strong> وجود دارد که به <strong>application</strong> خاص <strong>work queue</strong> بستگی دارد. با این حال، پس از دریافت مجموعه <strong>items</strong>، عملکرد واقعی <strong>work queue</strong> کاملاً <strong>generic</strong> است. در نتیجه، ما می‌توانیم <strong>application-specific queue source logic</strong> را از <strong>generic queue processing logic</strong> جدا کنیم. با توجه به الگوهای از پیش تعریف شده <strong>container groups</strong>، این را می‌توان به عنوان نمونه‌ای از <strong>ambassador pattern</strong> که قبلاً تعریف شده است، در نظر گرفت. <strong>The generic work queue container</strong>، <strong>primary application container</strong> است و <strong>application-specific source container</strong>، <strong>ambassador</strong> است که <strong>requests</strong> مربوط به <strong>generic work queue</strong> را به سمت تعریف مشخص <strong>work queue</strong> در دنیای واقعی <strong>proxies</strong> می‌کند. این <strong>container group</strong> در شکل 10-3 نشان داده شده است.
  </p>
<h3>Figure 10-3.</h3>
<p>
   The work queue container group
  </p>
<p>
   110
  </p>
<p>
   Chapter 10: Work Queue Systems
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 124" src="page_0124/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0124</div>
            </div>
        </div>
        <!-- Page 0125 -->
        <div class="chapter" id="page-0125">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   جالب توجه است، در حالی که <strong>ambassador container</strong> به وضوح <strong>application-specific</strong> است، همچنین انواع مختلفی از پیاده‌سازی‌های <strong>generic</strong> از <strong>work queue source API</strong> وجود دارد. به عنوان مثال، یک <strong>source</strong> ممکن است لیستی از تصاویر ذخیره شده در یک <strong>cloud storage API</strong>، مجموعه‌ای از فایل‌های ذخیره شده در <strong>network storage</strong>، یا یک <strong>queue</strong> در یک سیستم <strong>pub/sub</strong> مانند <strong>Kafka</strong> یا <strong>Redis</strong> باشد. در این موارد، اگرچه کاربر، <strong>ambassador</strong> خاص <strong>work queue</strong> را که متناسب با سناریوی آن‌ها است، انتخاب می‌کند، باید یک پیاده‌سازی "کتابخانه‌ای" <strong>generic</strong> از خود <strong>container</strong> را دوباره استفاده کنند. این کار، کار را به حداقل می‌رساند و استفاده مجدد از کد را به حداکثر می‌رساند.
  </p>
<h3>Work queue API</h3>
<p>
   با توجه به این هماهنگی بین <strong>generic work-queue manager</strong> و <strong>application-specific ambassador</strong>، ما به یک تعریف رسمی از <strong>interface</strong> بین دو <strong>containers</strong> نیاز داریم. اگرچه پروتکل‌های مختلفی وجود دارد، یک <strong>HTTP RESTful API</strong> هم ساده‌ترین راه برای پیاده‌سازی است و هم استاندارد <strong>de facto</strong> برای چنین <strong>interface</strong> است. <strong>The master work queue</strong> انتظار دارد که <strong>ambassador</strong>، <strong>URLs</strong> زیر را پیاده‌سازی کند:
  </p>
<ul>
<li>GET http://localhost/api/v1/items</li>
<li>GET http://localhost/api/v1/items/&lt;item-name&gt;</li>
</ul>
<p>
   ممکن است از خود بپرسید چرا ما یک v1 را در تعریف <strong>API</strong> قرار می‌دهیم. آیا تا به حال نسخه‌ی v2 از این <strong>interface</strong> وجود خواهد داشت؟ منطقی به نظر نمی‌رسد، اما <strong>versioning</strong> <strong>API</strong> در هنگام تعریف اولیه آن، هزینه بسیار کمی دارد. از سوی دیگر، <strong>Refactoring versioning</strong> روی یک <strong>API</strong> بدون آن، بسیار پرهزینه است. در نتیجه، این یک <strong>best practice</strong> است که همیشه نسخه‌ها را به <strong>APIs</strong> خود اضافه کنید، حتی اگر مطمئن نیستید که آیا آن‌ها تا به حال تغییر خواهند کرد یا خیر. احتیاط شرط عقل است.
  </p>
<p>
<strong>/items/ URL</strong> یک لیست کامل از همه <strong>items</strong> را برمی‌گرداند:
  </p>
<pre><code class="language-javascript">
{
   kind: ItemList,
   apiVersion: v1,
   items: [
      “item-1”,
      “item-2”,
      ….
   ]
}
  </code></pre>
<p>
<strong>The /items/&lt;item-name&gt; URL</strong> جزئیات یک <strong>item</strong> خاص را ارائه می‌دهد:
  </p>
<pre><code class="language-javascript">
{
  kind: Item,
  apiVersion: v1,
  data: {
  </code></pre>
<p>
   A Generic Work Queue System
  </p>
<p>
   111
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 125" src="page_0125/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0125</div>
            </div>
        </div>
        <!-- Page 0126 -->
        <div class="chapter" id="page-0126">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre><code class="language-javascript">
  “some”: “json”,
    “object”: “here”,
  }
}
  </code></pre>
<p>
   مهمتر از همه، متوجه خواهید شد که این <strong>API</strong>، هیچ <strong>affordances</strong>ای برای ثبت اینکه یک <strong>work item</strong> پردازش شده است، ندارد. ما می‌توانستیم یک <strong>API</strong> پیچیده‌تر طراحی کنیم و سپس پیاده‌سازی بیشتری را به داخل <strong>ambassador container</strong> منتقل کنیم، اما به یاد داشته باشید، هدف از این تلاش، قرار دادن تا حد امکان، پیاده‌سازی <strong>generic</strong> در داخل <strong>generic work queue manager</strong> است. به همین منظور، خود <strong>work queue manager</strong> مسئول ردیابی <strong>items</strong>ی است که پردازش شده‌اند و <strong>items</strong>ی که همچنان باقی مانده‌اند.
  </p>
<p>
   جزئیات <strong>item</strong> از این <strong>API</strong> به دست می‌آید و فیلد <strong>item.data</strong> به <strong>worker interface</strong> برای پردازش منتقل می‌شود.
  </p>
<h3>The Worker Container Interface</h3>
<p>
   هنگامی که یک <strong>work item</strong> خاص توسط <strong>work queue manager</strong> دریافت شد، باید توسط یک <strong>worker</strong> پردازش شود. این دومین <strong>container interface</strong> در <strong>generic work queue</strong> ما است. این <strong>container</strong> و <strong>interface</strong> به چند دلیل، کمی با <strong>work queue source interface</strong> قبلی متفاوت هستند. اولین مورد این است که این یک <strong>API</strong> یک‌باره است: یک فراخوانی واحد برای شروع کار انجام می‌شود و هیچ فراخوانی <strong>API</strong> دیگری در طول عمر <strong>worker container</strong> انجام نمی‌شود. ثانیاً، <strong>worker container</strong>، داخل یک <strong>container group</strong> با <strong>work queue manager</strong> نیست. در عوض، از طریق یک <strong>container orchestration API</strong> راه‌اندازی می‌شود و برای <strong>container group</strong> خود زمان‌بندی می‌شود. این بدان معناست که <strong>work queue manager</strong> باید یک <strong>remote call</strong> به <strong>worker container</strong> انجام دهد تا کار را شروع کند. همچنین این بدان معناست که ممکن است ما نیاز داشته باشیم که در مورد امنیت بیشتر دقت کنیم تا از تزریق کار اضافی توسط یک کاربر مخرب در <strong>cluster</strong> خود به سیستم جلوگیری کنیم.
  </p>
<p>
   با <strong>work queue source API</strong>، ما از یک <strong>API</strong> ساده مبتنی بر <strong>HTTP</strong> برای ارسال <strong>items</strong> به <strong>work queue manager</strong> استفاده کردیم. دلیلش این بود که ما نیاز داشتیم چندین بار <strong>API</strong> را فراخوانی کنیم و از آنجایی که همه چیز روی <strong>localhost</strong> اجرا می‌شد، امنیت نگرانی نداشت. با <strong>worker container</strong>، ما فقط نیاز داریم یک فراخوانی واحد انجام دهیم، و می‌خواهیم اطمینان حاصل کنیم که کاربران دیگر در سیستم نمی‌توانند به طور تصادفی یا به طور مخرب، کاری را به <strong>workers</strong> ما اضافه کنند. در نتیجه، برای <strong>worker container</strong>، ما از یک <strong>API</strong> مبتنی بر فایل استفاده خواهیم کرد.
  </p>
<p>
   به عبارت دیگر، هنگامی که <strong>worker container</strong> ایجاد می‌شود، یک <strong>environment variable</strong> به نام <strong>WORK_ITEM_FILE</strong> دریافت می‌کند. این به یک فایل در <strong>filesystem</strong> محلی <strong>container</strong> اشاره می‌کند، جایی که فیلد <strong>data</strong> از یک <strong>work item</strong> در یک فایل نوشته شده است. به طور مشخص، همانطور که در زیر خواهید دید، این <strong>API</strong> را می‌توان از طریق یک <strong>Kubernetes ConfigMap object</strong> که می‌تواند به عنوان یک فایل در یک <strong>container group</strong> نصب شود، پیاده‌سازی کرد، همانطور که در شکل 10-4 نشان داده شده است.
  </p>
<p>
   112
  </p>
<p>
   Chapter 10: Work Queue Systems
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0126</div>
            </div>
        </div>
        <!-- Page 0127 -->
        <div class="chapter" id="page-0127">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 10-4.</h3>
<p>
   The work queue worker API
  </p>
<p>
   این <strong>file-based API pattern</strong>، پیاده‌سازی را برای <strong>container</strong> نیز آسان‌تر می‌کند. اغلب یک <strong>work queue worker</strong>، به سادگی یک <strong>shell script</strong> در میان چند <strong>command line tools</strong> است. در این زمینه، راه‌اندازی یک <strong>web server</strong> برای مدیریت کاری که باید انجام شود، یک پیچیدگی غیرضروری است. همانطور که در مورد پیاده‌سازی <strong>work queue source</strong> صدق می‌کرد، اکثر <strong>worker containers</strong>، <strong>container images</strong> با هدف خاص هستند که برای <strong>work queue applications</strong> خاص ساخته شده‌اند، اما <strong>workers</strong> <strong>generic</strong> نیز وجود دارند که می‌توانند برای چندین <strong>work queue applications</strong> مختلف اعمال شوند.
  </p>
<p>
   مثال یک <strong>work queue worker</strong> را در نظر بگیرید که یک فایل را از <strong>cloud storage</strong> دانلود می‌کند و یک <strong>shell script</strong> را با آن فایل به عنوان ورودی اجرا می‌کند و سپس خروجی آن را به <strong>cloud storage</strong> کپی می‌کند. چنین <strong>container</strong> می‌تواند عمدتاً <strong>generic</strong> باشد، اما سپس <strong>script</strong> خاصی را که باید اجرا شود، به عنوان یک <strong>runtime parameter</strong> به آن ارائه می‌دهد. به این ترتیب، بیشتر کارهای مربوط به <strong>file handling</strong> می‌تواند توسط چندین کاربر/<strong>work queues</strong> به اشتراک گذاشته شود و تنها موارد خاص پردازش فایل باید توسط کاربر نهایی ارائه شود.
  </p>
<h3>The Shared Work Queue Infrastructure</h3>
<p>
   با توجه به پیاده‌سازی‌های دو <strong>container interface</strong> که قبلاً توضیح داده شد، برای پیاده‌سازی مجدد <strong>work queue</strong> ما چه چیزی باقی مانده است؟ <strong>algorithm</strong> اساسی برای <strong>work queue</strong> کاملاً ساده است:
  </p>
<ol>
<li>بارگیری کار موجود با فراخوانی در <strong>source container interface</strong>.</li>
<li>با <strong>work queue state</strong> مشورت کنید تا مشخص کنید کدام <strong>work items</strong> پردازش شده‌اند یا در حال حاضر در حال پردازش هستند.</li>
<li>برای این <strong>items</strong>، <strong>jobs</strong>ی را راه‌اندازی کنید که از <strong>worker container interface</strong> برای پردازش <strong>work item</strong> استفاده می‌کنند.</li>
<li>هنگامی که یکی از این <strong>worker containers</strong> با موفقیت به پایان رسید، ثبت کنید که <strong>work item</strong> تکمیل شده است.</li>
</ol>
<p>
   در حالی که بیان این <strong>algorithm</strong> با کلمات ساده است، پیاده‌سازی آن در واقعیت تا حدودی پیچیده‌تر است. خوشبختانه برای ما، <strong>Kubernetes container orchestrator</strong> حاوی تعدادی از ویژگی‌ها است که پیاده‌سازی آن را به طور قابل توجهی آسان‌تر می‌کند. به طور خاص،
  </p>
<p>
   A Generic Work Queue System
  </p>
<p>
   113
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 127" src="page_0127/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0127</div>
            </div>
        </div>
        <!-- Page 0128 -->
        <div class="chapter" id="page-0128">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<strong>Kubernetes</strong> شامل یک <strong>Job object</strong> است که امکان اجرای قابل اطمینان <strong>work queue</strong> را فراهم می‌کند. <strong>Job</strong> را می‌توان طوری پیکربندی کرد که <strong>worker container</strong> را یک بار اجرا کند یا آن را تا زمانی که با موفقیت تکمیل شود، اجرا کند. اگر <strong>worker container</strong> برای اجرا تا تکمیل تنظیم شده باشد، حتی اگر یک ماشین در <strong>cluster</strong> شکست بخورد، <strong>job</strong> در نهایت با موفقیت اجرا می‌شود. این امر وظیفه ساختن یک <strong>work queue</strong> را به طور چشمگیری ساده می‌کند زیرا <strong>orchestrator</strong> مسئولیت عملیات قابل اطمینان هر <strong>work item</strong> را بر عهده می‌گیرد.
  </p>
<p>
   علاوه بر این، <strong>Kubernetes</strong> دارای <strong>annotations</strong> برای هر <strong>Job object</strong> است که ما را قادر می‌سازد تا هر <strong>job</strong> را با <strong>work item</strong>ی که پردازش می‌کند، علامت‌گذاری کنیم. این ما را قادر می‌سازد تا <strong>items</strong>ی که در حال پردازش هستند و همچنین <strong>items</strong>ی که در نتیجه شکست یا موفقیت تکمیل شده‌اند را درک کنیم.
  </p>
<p>
   روی هم رفته، این بدان معنی است که ما می‌توانیم یک <strong>work queue</strong> را بر روی لایه <strong>Kubernetes orchestration</strong> بدون استفاده از هیچ <strong>storage</strong> خودمان پیاده‌سازی کنیم. این امر وظیفه ساخت زیرساخت <strong>work queue</strong> را به طور چشمگیری ساده می‌کند.
  </p>
<p>
   بنابراین، عملیات توسعه یافته <strong>container work queue</strong> ما به این صورت است:
  </p>
<ol>
<li>تا ابد تکرار کن</li>
<li>لیست <strong>work items</strong> را از <strong>work source container interface</strong> دریافت کنید.</li>
<li>لیست تمام <strong>jobs</strong>ی را که برای سرویس‌دهی به این <strong>work queue</strong> ایجاد شده‌اند، دریافت کنید.</li>
<li>این لیست‌ها را <strong>difference</strong> کنید تا مجموعه‌ای از <strong>work items</strong> را که پردازش نشده‌اند، پیدا کنید.</li>
<li>برای این <strong>items</strong> پردازش نشده، <strong>Job objects</strong> جدیدی ایجاد کنید که <strong>worker container</strong> مناسب را راه‌اندازی می‌کنند.</li>
</ol>
<p>
   در اینجا یک <strong>Python script</strong> ساده وجود دارد که این <strong>work queue</strong> را پیاده‌سازی می‌کند:
  </p>
<pre><code class="language-python">
import requests
import json
from kubernetes import client, config
import time
namespace = "default"
def make_container(item, obj):
    container = client.V1Container()
    container.image = "my/worker-image"
    container.name = "worker"
    return container
def make_job(item):
    response = requests.get("http://localhost:8000/items/{}".format(item))
    obj = json.loads(response.text)
    job = client.V1Job()
    job.metadata = client.V1ObjectMeta()
    job.metadata.name = item
  </code></pre>
<p>
   114
  </p>
<p>
   Chapter 10: Work Queue Systems
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0128</div>
            </div>
        </div>
        <!-- Page 0129 -->
        <div class="chapter" id="page-0129">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre><code class="language-python">
                job.spec = client.V1JobSpec()
    job.spec.template = client.V1PodTemplate()
    job.spec.template.spec = client.V1PodTemplateSpec()
    job.spec.template.spec.restart_policy = "Never"
    job.spec.template.spec.containers = [
        make_container(item, obj)
    ]
    return job
def update_queue(batch):
    response = requests.get("http://localhost:8000/items")
    obj = json.loads(response.text)
    items = obj['items']
    ret = batch.list_namespaced_job(namespace, watch=False)
    for item in items:
        found = False
        for i in ret.items:
            if i.metadata.name == item:
                found = True
        if not found:
            # This function creates the job object, omitted for
            # brevity
            job = make_job(item)
            batch.create_namespaced_job(namespace, job)
config.load_kube_config()
batch = client.BatchV1Api()
while True:
    update_queue(batch)
    time.sleep(10)
  </code></pre>
<h3>Hands On: Implementing a Video Thumbnailer</h3>
<p>
   برای ارائه یک مثال عینی از نحوه استفاده ما از یک <strong>work queue</strong>، وظیفه ایجاد <strong>thumbnails</strong> برای ویدیوها را در نظر بگیرید. این <strong>thumbnails</strong> به کاربران کمک می‌کند تا تعیین کنند کدام ویدیوها را می‌خواهند تماشا کنند.
  </p>
<p>
   برای پیاده‌سازی این <strong>video thumbnailer</strong>، ما به دو <strong>user containers</strong> مختلف نیاز داریم. اولین مورد <strong>work item source container</strong> است. ساده‌ترین راه برای کار این است که <strong>work items</strong> در یک دیسک مشترک، مانند یک <strong>Network File System (NFS) share</strong>، ظاهر شوند. <strong>work item source</strong> به سادگی فایل‌ها را در این دایرکتوری فهرست می‌کند و آنها را به تماس‌گیرنده برمی‌گرداند.
  </p>
<p>
   در اینجا یک برنامه <strong>node</strong> ساده وجود دارد که این کار را انجام می‌دهد:
  </p>
<pre><code class="language-javascript">
const http = require('http');
const fs = require('fs');
const port = 8080;
  </code></pre>
<p>
   Hands On: Implementing a Video Thumbnailer
  </p>
<p>
   115
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0129</div>
            </div>
        </div>
        <!-- Page 0130 -->
        <div class="chapter" id="page-0130">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre><code class="language-javascript">
const path = process.env.MEDIA_PATH;
const requestHandler = (request, response) =&gt; {
 
console.log(request.url);
 
fs.readdir(path + '/*.mp4', (err, items) =&gt; {
 
 
var msg = {
 
 
 
'kind': 'ItemList',
 
 
 
'apiVersion': 'v1',
 
 
 
'items': []
 
 
};
 
 
if (!items) {
 
 
 
return msg;
 
 
}
 
 
for (var i = 0; i &lt; items.length; i++) {
 
 
 
msg.items.push(items[i]);
 
 
}
 
 
response.end(JSON.stringify(msg));
 
});
}
const server = http.createServer(requestHandler);
server.listen(port, (err) =&gt; {
 
if (err) {
 
 
return console.log('Error starting server', err);
 
}
 
console.log(server is active on ${port})
});
  </code></pre>
<p>
   این <strong>source</strong>، <strong>queue</strong> فیلم‌ها را برای <strong>thumbnail</strong> تعریف می‌کند. ما از <strong>ffmpeg utility</strong> برای انجام کار <strong>thumbnailing</strong> استفاده می‌کنیم.
  </p>
<p>
   شما می‌توانید یک <strong>container</strong> ایجاد کنید که از موارد زیر به عنوان <strong>command line</strong> خود استفاده کند:
  </p>
<pre><code class="language-bash">
ffmpeg -i ${INPUT_FILE} -frames:v 100 thumb.png
  </code></pre>
<p>
   این <strong>command</strong> یک فریم را هر 100 فریم (که پرچم -frames:v 100 است) می‌گیرد و آن را به عنوان یک فایل <strong>PNG</strong> (به عنوان مثال، thumb1.png، thumb2.png و غیره) خروجی می‌دهد.
  </p>
<p>
   شما می‌توانید این <strong>image processing</strong> را با استفاده از یک <strong>ffmpeg Docker image</strong> موجود پیاده‌سازی کنید.
   <strong>jrottenberg/ffmpeg Docker image</strong> یک انتخاب محبوب است.
  </p>
<p>
   با تعریف یک <strong>source container</strong> ساده و همچنین یک <strong>worker container</strong> ساده‌تر، ما می‌توانیم قدرت و سودمندی یک سیستم <strong>queuing</strong> مبتنی بر <strong>generic, container</strong> را به وضوح ببینیم. این امر، زمان/فاصله بین یک ایده برای پیاده‌سازی یک <strong>work queue</strong> و پیاده‌سازی عینی مربوطه را به طور چشمگیری کاهش می‌دهد.
  </p>
<p>
   116
  </p>
<p>
   Chapter 10: Work Queue Systems
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0130</div>
            </div>
        </div>
        <!-- Page 0131 -->
        <div class="chapter" id="page-0131">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Dynamic Scaling of the Workers</h3>
<p>
<strong>work queue</strong> که قبلاً توضیح داده شد برای پردازش <strong>work items</strong> به سرعت مناسب است، اما این می‌تواند منجر به بارگذاری <strong>bursty resource</strong> بر روی یک <strong>container orchestrator cluster</strong> شود. این خوب است اگر شما تعداد زیادی <strong>workloads</strong> مختلف دارید که در زمان‌های مختلف <strong>burst</strong> خواهند شد و در نتیجه زیرساخت شما به طور مساوی مورد استفاده قرار می‌گیرد. اما اگر تعداد کافی <strong>workloads</strong> مختلف ندارید، این رویکرد <strong>feast or famine</strong> برای مقیاس‌بندی <strong>work queue</strong> شما ممکن است مستلزم این باشد که منابع را بیش از حد <strong>provision</strong> کنید تا از <strong>bursts</strong>ی که بیکار خواهند ماند (و هزینه زیادی را به همراه خواهد داشت) پشتیبانی کنید، در حالی که شما کاری برای انجام دادن ندارید.
  </p>
<p>
   برای حل این مشکل، می‌توانید تعداد کلی <strong>Job objects</strong>ی را که <strong>work queue</strong> شما مایل به ایجاد آن است، محدود کنید. این به طور طبیعی به محدود کردن تعداد <strong>work items</strong>ی که به صورت موازی پردازش می‌کنید و در نتیجه محدود کردن حداکثر مقدار منابعی که در یک زمان خاص استفاده می‌کنید، کمک می‌کند. با این حال، انجام این کار، زمان تکمیل (<strong>latency</strong>) را برای هر <strong>work item</strong> در حال تکمیل در زیر بار سنگین افزایش می‌دهد.
  </p>
<p>
   اگر بار <strong>bursty</strong> باشد، این احتمالاً اشکالی ندارد زیرا می‌توانید از زمان‌های <strong>slack</strong> برای رسیدگی به <strong>backlog</strong>ی که در طی یک <strong>burst of usage</strong> ایجاد شده است، استفاده کنید. با این حال، اگر استفاده <strong>steady-state</strong> شما بیش از حد زیاد باشد، <strong>work queue</strong> شما ممکن است هرگز نتواند به آن برسد و زمان تکمیل به سادگی طولانی‌تر و طولانی‌تر می‌شود.
  </p>
<p>
   هنگامی که <strong>work queue</strong> شما با این موقعیت مواجه می‌شود، باید آن را به صورت <strong>dynamic</strong> تنظیم کنید تا موازی‌سازی را که مایل به ایجاد آن است (و متناظر با منابعی که مایل به استفاده از آن است) افزایش دهد تا بتواند با کار ورودی همگام شود. خوشبختانه، فرمول‌های ریاضی وجود دارد که می‌توانیم از آن‌ها برای تعیین زمان نیاز به <strong>dynamic</strong> مقیاس‌بندی <strong>work queue</strong> خود استفاده کنیم.
  </p>
<p>
   یک <strong>work queue</strong> را در نظر بگیرید که در آن یک <strong>work item</strong> جدید به طور متوسط ​​یک بار در هر دقیقه می‌رسد و هر <strong>work item</strong> به طور متوسط ​​30 ثانیه طول می‌کشد تا تکمیل شود. چنین سیستمی قادر به همگامی با تمام کارهایی است که دریافت می‌کند. حتی اگر تعداد زیادی کار به یکباره برسد و یک <strong>backlog</strong> ایجاد کند، به طور متوسط ​​<strong>work queue</strong> دو <strong>work item</strong> را برای هر <strong>work item</strong>ی که می‌رسد پردازش می‌کند، و بنابراین قادر خواهد بود به تدریج از طریق <strong>backlog</strong> خود کار کند.
  </p>
<p>
   اگر، به جای آن، یک <strong>work item</strong> جدید به طور متوسط ​​یک بار در هر دقیقه برسد و پردازش هر <strong>work item</strong> به طور متوسط ​​یک دقیقه طول بکشد، سیستم کاملاً متعادل است، اما <strong>variance</strong> را به خوبی مدیریت نمی‌کند. این می‌تواند با <strong>bursts</strong> مقابله کند - اما مدتی طول می‌کشد، و هیچ <strong>slack</strong> یا ظرفیتی برای جذب افزایش مداوم در میزان ورود <strong>work items</strong> جدید ندارد. این احتمالاً یک راه ایده‌آل برای اجرا نیست، زیرا برای حفظ یک سیستم پایدار به مقداری <strong>safety margin</strong> برای رشد و سایر افزایش‌های مداوم در کار (یا کاهش‌های غیرمنتظره در پردازش) نیاز است.
  </p>
<p>
   در نهایت، سیستمی را در نظر بگیرید که در آن یک <strong>work item</strong> هر دقیقه می‌رسد و پردازش هر مورد 2 دقیقه طول می‌کشد. در چنین سیستمی، ما همیشه در حال از دست دادن زمین هستیم. <strong>queue</strong>
</p>
<p>
   Dynamic Scaling of the Workers
  </p>
<p>
   117
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0131</div>
            </div>
        </div>
        <!-- Page 0132 -->
        <div class="chapter" id="page-0132">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   ازدیاد کار، بدون محدودیت رشد خواهد کرد و <strong>latency</strong> هر <strong>item</strong> در <strong>queue</strong> به سمت بی‌نهایت افزایش می‌یابد (و کاربران ما بسیار ناامید خواهند شد).
  </p>
<p>
   بنابراین، ما می‌توانیم هر دوی این <strong>metrics</strong> را برای <strong>work queue</strong> خود پیگیری کنیم، و میانگین زمان بین <strong>work items</strong> در یک دوره طولانی مدت (تعداد <strong>work items</strong> / 24 ساعت) زمان بین ورود برای کار جدید را به ما می‌دهد. ما همچنین می‌توانیم میانگین زمان برای پردازش هر <strong>item</strong> را پس از شروع کار بر روی آن، پیگیری کنیم (بدون احتساب زمان در <strong>queue</strong>). برای داشتن یک <strong>work queue</strong> پایدار، باید تعداد منابع را طوری تنظیم کنیم که زمان پردازش هر <strong>item</strong> کمتر از زمان بین ورود <strong>items</strong> جدید باشد. اگر ما در حال پردازش <strong>work items</strong> به صورت موازی هستیم، زمان پردازش را برای یک <strong>work item</strong> بر موازی‌سازی تقسیم می‌کنیم. به عنوان مثال، اگر پردازش هر <strong>item</strong> یک دقیقه طول بکشد، اما ما چهار <strong>item</strong> را به صورت موازی پردازش می‌کنیم، زمان مؤثر برای پردازش یک <strong>item</strong> 15 ثانیه است، و بنابراین ما می‌توانیم یک دوره بین ورود 16 ثانیه یا بیشتر را حفظ کنیم.
  </p>
<p>
   این رویکرد، ساخت یک <strong>autoscaler</strong> را برای <strong>dynamic</strong> اندازه کردن <strong>work queue</strong> ما کاملاً ساده می‌کند. کوچک کردن <strong>work queue</strong> تا حدودی دشوارتر است، اما شما می‌توانید از همان ریاضیات و همچنین یک <strong>heuristic</strong> برای مقدار ظرفیت <strong>spare</strong> برای <strong>safety margin</strong> که می‌خواهید حفظ کنید، استفاده کنید. به عنوان مثال، می‌توانید موازی‌سازی را کاهش دهید تا زمان پردازش برای یک <strong>item</strong> 90٪ از زمان بین ورود برای <strong>items</strong> جدید باشد.
  </p>
<h3>The Multi-Worker Pattern</h3>
<p>
   یکی از موضوعات اصلی این کتاب، استفاده از <strong>containers</strong> برای <strong>encapsulation</strong> و استفاده مجدد از کد بوده است. همین امر در مورد الگوهای <strong>work queue</strong> که در این فصل توضیح داده شده است، صدق می‌کند. علاوه بر الگوهایی برای استفاده مجدد از <strong>containers</strong> برای هدایت خود <strong>work queue</strong>، شما همچنین می‌توانید از چندین <strong>containers</strong> مختلف برای ترکیب یک پیاده‌سازی <strong>worker</strong> استفاده کنید. فرض کنید، به عنوان مثال، شما سه نوع مختلف کار دارید که می‌خواهید بر روی یک <strong>work queue item</strong> خاص انجام دهید. به عنوان مثال، ممکن است بخواهید چهره‌ها را در یک تصویر شناسایی کنید، آن چهره‌ها را با هویت‌ها برچسب‌گذاری کنید و سپس چهره‌ها را در تصویر محو کنید. شما می‌توانید یک <strong>worker</strong> واحد برای انجام این مجموعه کامل از وظایف بنویسید، اما این یک راه‌حل <strong>bespoke</strong> خواهد بود که دفعه بعد که می‌خواهید چیز دیگری مانند ماشین‌ها را شناسایی کنید، اما همچنان همان محو کردن را ارائه دهید، قابل استفاده مجدد نخواهد بود.
  </p>
<p>
   برای دستیابی به این نوع استفاده مجدد از کد، <strong>multi-worker pattern</strong> چیزی شبیه به یک <strong>specialization</strong> از <strong>adapter pattern</strong> است که در فصل‌های قبل توضیح داده شد. در این مورد، <strong>multi-worker pattern</strong> مجموعه‌ای از <strong>worker containers</strong> مختلف را به یک <strong>container</strong> واحد تبدیل می‌کند که <strong>worker interface</strong> را پیاده‌سازی می‌کند، اما کار واقعی را به مجموعه‌ای از <strong>containers</strong> مختلف و قابل استفاده مجدد واگذار می‌کند. این فرآیند در شکل 10-5 نشان داده شده است.
  </p>
<p>
   118
  </p>
<p>
   Chapter 10: Work Queue Systems
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0132</div>
            </div>
        </div>
        <!-- Page 0133 -->
        <div class="chapter" id="page-0133">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 10-5.</h3>
<p>
   The multi-worker aggregator pattern as a group of containers
  </p>
<p>
   به دلیل این استفاده مجدد از کد، ترکیب چندین <strong>worker containers</strong> مختلف به معنای افزایش استفاده مجدد از کد و کاهش تلاش برای افرادی است که <strong>batch-oriented distributed systems</strong> را طراحی می‌کنند.
  </p>
<p>
   The Multi-Worker Pattern
  </p>
<p>
   119
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 133" src="page_0133/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0133</div>
            </div>
        </div>
        <!-- Page 0135 -->
        <div class="chapter" id="page-0135">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>CHAPTER 11</h3>
<h3>Event-Driven Batch Processing</h3>
<p>
   در فصل قبل، ما یک <strong>framework generic</strong> برای پردازش <strong>work queue</strong> و همچنین تعدادی از <strong>example applications</strong> برای پردازش <strong>work queue</strong> ساده را مشاهده کردیم. <strong>Work queues</strong> برای فعال کردن تبدیل‌های فردی از یک ورودی به یک خروجی عالی هستند. با این حال، تعدادی از <strong>batch applications</strong> وجود دارد که در آن شما می‌خواهید بیش از یک عمل انجام دهید، یا ممکن است نیاز داشته باشید که چندین خروجی مختلف را از یک ورودی داده واحد تولید کنید. در این موارد، شما شروع به پیوند دادن <strong>work queues</strong> به یکدیگر می‌کنید تا خروجی یک <strong>work queue</strong> به ورودی یک یا چند <strong>work queue</strong> دیگر تبدیل شود و به همین ترتیب ادامه می‌یابد. این یک سری مراحل پردازش را تشکیل می‌دهد که به <strong>events</strong> پاسخ می‌دهند، <strong>events</strong> تکمیل مرحله قبلی در <strong>work queue</strong> هستند که قبل از آن آمده است.
  </p>
<p>
   این نوع سیستم‌های پردازش مبتنی بر <strong>event</strong> اغلب سیستم‌های <strong>workflow</strong> نامیده می‌شوند، زیرا جریانی از کار از طریق یک نمودار جهت‌دار و غیر چرخه‌ای وجود دارد که مراحل مختلف و هماهنگی آن‌ها را توصیف می‌کند. یک تصویر اساسی از چنین سیستمی در شکل 11-1 نشان داده شده است.
  </p>
<p>
   سرراست‌ترین کاربرد این نوع سیستم، به سادگی خروجی یک <strong>queue</strong> را به ورودی <strong>queue</strong> بعدی متصل می‌کند. اما با پیچیده‌تر شدن سیستم‌ها، یک سری الگوهای مختلف برای پیوند دادن یک سری <strong>work queues</strong> به یکدیگر ظاهر می‌شود. درک و طراحی بر اساس این الگوها برای درک چگونگی عملکرد سیستم مهم است. عملکرد یک پردازشگر <strong>batch</strong> مبتنی بر <strong>event</strong> مشابه <strong>event-driven FaaS</strong> است. در نتیجه، بدون یک <strong>blueprint</strong> کلی برای نحوه ارتباط <strong>event queues</strong> مختلف با یکدیگر، درک کامل نحوه عملکرد سیستم می‌تواند دشوار باشد.
  </p>
<p>
   121
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0135</div>
            </div>
        </div>
        <!-- Page 0136 -->
        <div class="chapter" id="page-0136">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 11-1.</h3>
<p>
   This workflow combines copying work into multiple queues (Stage 2a, 2b) parallel processing of those queues, and combining the result back into a single queue (Stage 3)
  </p>
<h3>Patterns of Event-Driven Processing</h3>
<p>
   فراتر از <strong>work queue</strong> ساده‌ای که در فصل قبل توضیح داده شد، تعدادی الگو برای پیوند دادن <strong>work queues</strong> به یکدیگر وجود دارد. ساده‌ترین الگو—الگویی که در آن خروجی یک <strong>queue</strong> واحد به ورودی <strong>queue</strong> دوم تبدیل می‌شود—به اندازه‌ای سر راست است که ما آن را در اینجا پوشش نمی‌دهیم. ما الگوهایی را توصیف خواهیم کرد که شامل هماهنگی چندین <strong>queues</strong> مختلف یا اصلاح خروجی یک یا چند <strong>work queues</strong> است.
  </p>
<h3>Copier</h3>
<p>
   اولین الگو برای هماهنگی <strong>work queues</strong> یک <strong>copier</strong> است. وظیفه یک <strong>copier</strong> این است که یک <strong>stream</strong> واحد از <strong>work items</strong> را دریافت کند و آن را به دو یا چند <strong>streams</strong> یکسان تکرار کند. این الگو زمانی مفید است که کارهای مختلف زیادی وجود داشته باشد که باید بر روی یک <strong>work item</strong> یکسان انجام شود. نمونه‌ای از این می‌تواند رندر کردن یک ویدیو باشد. هنگام رندر کردن یک ویدیو، انواع مختلفی از فرمت‌ها وجود دارد که بسته به
  </p>
<p>
   122
  </p>
<p>
   Chapter 11: Event-Driven Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 136" src="page_0136/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0136</div>
            </div>
        </div>
        <!-- Page 0137 -->
        <div class="chapter" id="page-0137">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   جایی که ویدیو قرار است نشان داده شود. ممکن است یک فرمت 4-KB با وضوح بالا برای پخش از یک <strong>hard drive</strong>، یک رندر 1080 پیکسلی برای <strong>digital streaming</strong>، یک فرمت با وضوح پایین برای <strong>streaming</strong> به کاربران تلفن همراه در شبکه‌های کند و یک <strong>animated GIF thumbnail</strong> برای نمایش در رابط کاربری انتخاب فیلم وجود داشته باشد. همه این <strong>work items</strong> را می‌توان به عنوان <strong>work queues</strong> جداگانه برای هر <strong>render</strong> مدل‌سازی کرد، اما ورودی هر <strong>work item</strong> یکسان است. یک تصویر از <strong>copier pattern</strong> اعمال شده برای <strong>transcoding</strong> در شکل 11-2 نشان داده شده است.
  </p>
<h3>Figure 11-2.</h3>
<p>
   The copier batch pattern for transcoding
  </p>
<h3>Filter</h3>
<p>
   الگوی دوم برای پردازش <strong>batch</strong> مبتنی بر <strong>event</strong> یک <strong>filter</strong> است. نقش یک <strong>filter</strong> کاهش یک <strong>stream</strong> از <strong>work items</strong> به یک <strong>stream</strong> کوچکتر از <strong>work items</strong> با حذف <strong>work items</strong>ی است که معیارهای خاصی را ندارند. به عنوان نمونه‌ای از این، تنظیم یک <strong>batch workflow</strong> را در نظر بگیرید که کاربران جدید را که برای یک <strong>service</strong> ثبت نام می‌کنند، مدیریت می‌کند. برخی از آن کاربران، کادر تأیید را علامت زده‌اند که نشان می‌دهد مایلند از طریق ایمیل برای تبلیغات و سایر اطلاعات با آنها تماس گرفته شود. در چنین <strong>workflow</strong>، شما می‌توانید مجموعه کاربران تازه ثبت‌نام شده را فیلتر کنید تا فقط شامل کسانی باشد که صریحاً برای تماس انتخاب شده‌اند.
  </p>
<p>
   در حالت ایده‌آل، شما یک منبع <strong>work queue filter</strong> را به عنوان یک <strong>ambassador</strong> که یک منبع <strong>work queue</strong> موجود را در بر می‌گیرد، ترکیب می‌کنید. <strong>source container</strong> اصلی، لیست کاملی از <strong>items</strong> را که باید روی آن‌ها کار شود، ارائه می‌دهد، و سپس <strong>filter container</strong> آن لیست را بر اساس معیارهای <strong>filter</strong> تنظیم می‌کند و فقط آن نتایج فیلتر شده را به زیرساخت <strong>work queue</strong> برمی‌گرداند. یک تصویر از این استفاده از <strong>adapter pattern</strong> در شکل 11-3 نشان داده شده است.
  </p>
<p>
   Patterns of Event-Driven Processing
  </p>
<p>
   123
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 137" src="page_0137/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0137</div>
            </div>
        </div>
        <!-- Page 0138 -->
        <div class="chapter" id="page-0138">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 11-3.</h3>
<p>
   An example of a filter pattern that removes all odd-numbered work items
  </p>
<h3>Splitter</h3>
<p>
   گاهی اوقات شما نمی‌خواهید فقط با انداختن <strong>items</strong>، آن‌ها را فیلتر کنید، بلکه دو نوع ورودی مختلف در مجموعه <strong>work items</strong> خود دارید و می‌خواهید آن‌ها را بدون حذف هیچ‌کدام از آن‌ها به دو <strong>work queues</strong> جداگانه تقسیم کنید. برای این کار، شما می‌خواهید از یک <strong>splitter</strong> استفاده کنید. نقش یک <strong>splitter</strong> ارزیابی برخی از معیارها—درست مانند یک <strong>filter</strong>—است، اما به جای حذف ورودی، <strong>splitter</strong> ورودی‌های مختلف را بر اساس آن معیارها به <strong>queues</strong> مختلف ارسال می‌کند.
  </p>
<p>
   مثالی از یک <strong>application</strong> از <strong>splitter pattern</strong> پردازش سفارشات آنلاین است که در آن افراد می‌توانند اعلان‌های حمل و نقل را از طریق ایمیل یا پیام متنی دریافت کنند. با توجه به یک <strong>work queue</strong> از <strong>items</strong> که ارسال شده‌اند، <strong>splitter</strong> آن را به دو <strong>queues</strong> مختلف تقسیم می‌کند: یکی که مسئول ارسال ایمیل است و دیگری که به ارسال پیام‌های متنی اختصاص دارد. اگر یک کاربر در مثال قبلی هم پیام‌های متنی و هم اعلان‌های ایمیل را انتخاب کند، یک <strong>splitter</strong> همچنین می‌تواند یک <strong>copier</strong> باشد اگر همان خروجی را به چندین <strong>queues</strong> ارسال کند. جالب است که توجه داشته باشید که یک <strong>splitter</strong> در واقع می‌تواند توسط یک <strong>copier</strong> و دو <strong>filter</strong> مختلف نیز پیاده‌سازی شود. اما <strong>splitter pattern</strong> یک نمایش فشرده‌تر است که کار <strong>splitter</strong> را با دقت بیشتری به تصویر می‌کشد. نمونه‌ای از استفاده از <strong>splitter pattern</strong> برای ارسال اعلان‌های حمل و نقل به کاربران در شکل 11-4 نشان داده شده است.
  </p>
<p>
   124
  </p>
<p>
   Chapter 11: Event-Driven Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 138" src="page_0138/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0138</div>
            </div>
        </div>
        <!-- Page 0139 -->
        <div class="chapter" id="page-0139">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 11-4.</h3>
<p>
   An example of the batch splitter pattern splitting shipping notifications into two different queues
  </p>
<h3>Sharder</h3>
<p>
   یک فرم کمی <strong>generic</strong>تر از <strong>splitter</strong>، یک <strong>sharder</strong> است. درست مانند سرور <strong>sharded</strong> که در فصل‌های قبل دیدیم، نقش یک <strong>sharder</strong> در یک <strong>workflow</strong> تقسیم یک <strong>queue</strong> واحد به مجموعه‌ای از <strong>work items</strong> که به طور مساوی تقسیم شده‌اند، بر اساس نوعی تابع <strong>sharding</strong> است. دلایل مختلفی وجود دارد که چرا ممکن است <strong>sharding</strong> <strong>workflow</strong> خود را در نظر بگیرید. یکی از اولین موارد برای قابلیت اطمینان است. اگر <strong>work queue</strong> خود را <strong>shard</strong> کنید، خرابی یک <strong>workflow</strong> واحد به دلیل یک <strong>update</strong> بد، خرابی زیرساخت یا مشکل دیگر فقط بر کسری از <strong>service</strong> شما تأثیر می‌گذارد.
  </p>
<p>
   به عنوان مثال، تصور کنید که شما یک <strong>update</strong> بد را به <strong>worker container</strong> خود منتقل می‌کنید، که باعث می‌شود <strong>workers</strong> شما خراب شوند و <strong>queue</strong> شما پردازش <strong>work items</strong> را متوقف کند. اگر فقط یک <strong>work queue</strong> دارید که <strong>items</strong> را پردازش می‌کند، پس شما یک <strong>outage</strong> کامل برای <strong>service</strong> خود خواهید داشت که همه کاربران تحت تأثیر قرار می‌گیرند. اگر، در عوض، <strong>work queue</strong> خود را به چهار <strong>shards</strong> مختلف تقسیم کرده‌اید، این فرصت را دارید که یک <strong>staged rollout</strong> از <strong>worker container</strong> جدید خود انجام دهید. با فرض اینکه شما شکست را در فاز اول <strong>staged rollout</strong> بگیرید، <strong>sharding queue</strong> شما به چهار <strong>shards</strong> مختلف به این معنی است که تنها یک چهارم از کاربران شما تحت تأثیر قرار می‌گیرند.
  </p>
<p>
   یک دلیل اضافی برای <strong>shard work queue</strong> شما، توزیع یکنواخت‌تر کار در منابع مختلف است. اگر واقعاً برای شما مهم نیست که از کدام منطقه یا <strong>datacenter</strong> برای پردازش یک مجموعه خاص از <strong>work items</strong> استفاده می‌شود، می‌توانید از یک <strong>sharder</strong> برای پخش یکنواخت کار در چندین <strong>datacenters</strong> استفاده کنید تا استفاده از تمام <strong>datacenters/regions</strong> را یکنواخت کنید. همانطور که در مورد <strong>updates</strong>، پخش <strong>work queue</strong> شما در چندین منطقه خرابی نیز مزایای
  </p>
<p>
   Patterns of Event-Driven Processing
  </p>
<p>
   125
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 139" src="page_0139/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0139</div>
            </div>
        </div>
        <!-- Page 0140 -->
        <div class="chapter" id="page-0140">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<strong>fit</strong> برای ارائه قابلیت اطمینان در برابر خرابی <strong>datacenter</strong> یا منطقه. تصویری از یک <strong>sharded queue</strong> زمانی که همه چیز به درستی کار می‌کند در شکل 11-5 نشان داده شده است.
  </p>
<h3>Figure 11-5.</h3>
<p>
   An example of the sharding pattern in a healthy operation
  </p>
<p>
   هنگامی که تعداد <strong>shards</strong> سالم به دلیل خرابی کاهش می‌یابد، <strong>algorithm sharding</strong> به صورت <strong>dynamic</strong> تنظیم می‌شود تا کار را به <strong>work queues</strong> سالم باقی‌مانده ارسال کند، حتی اگر فقط یک <strong>queue</strong> باقی بماند. این در شکل 11-6 نشان داده شده است.
  </p>
<h3>Figure 11-6.</h3>
<p>
   When one work queue is unhealthy the remaining work spills over to a dif ferent queue
  </p>
<p>
   126
  </p>
<p>
   Chapter 11: Event-Driven Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 140" src="page_0140/image_1.png"/></div>
<div class="page-image"><img alt="Image from page 140" src="page_0140/image_2.png"/></div>
</div>
                <div class="page-number">صفحه 0140</div>
            </div>
        </div>
        <!-- Page 0141 -->
        <div class="chapter" id="page-0141">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Merger</h3>
<p>
   آخرین الگو برای سیستم‌های <strong>batch</strong> مبتنی بر <strong>event</strong> یا <strong>workflow</strong> یک <strong>merger</strong> است. یک <strong>merger</strong> برعکس یک <strong>copier</strong> است. وظیفه یک <strong>merger</strong> این است که دو <strong>work queues</strong> مختلف را بگیرد و آن‌ها را به یک <strong>work queue</strong> واحد تبدیل کند. به عنوان مثال، فرض کنید که شما تعداد زیادی <strong>source repositories</strong> مختلف دارید که همزمان <strong>commits</strong> جدیدی را اضافه می‌کنند. شما می‌خواهید هر یک از این <strong>commits</strong> را بگیرید و یک <strong>build-and-test</strong> برای آن انجام دهید. ایجاد یک زیرساخت <strong>build</strong> جداگانه برای هر <strong>source repository</strong> مقیاس‌پذیر نیست. ما می‌توانیم هر یک از <strong>source repositories</strong> مختلف را به عنوان یک منبع <strong>work queue</strong> جداگانه مدل‌سازی کنیم که مجموعه‌ای از <strong>commits</strong> را ارائه می‌دهد. ما می‌توانیم تمام این ورودی‌های <strong>work queue</strong> مختلف را با استفاده از یک <strong>merger adapter</strong> به یک مجموعه ورودی ادغام شده تبدیل کنیم. سپس این <strong>stream</strong> ادغام شده از <strong>commits</strong>، منبع واحدی برای سیستم <strong>build</strong> است که <strong>build</strong> واقعی را انجام می‌دهد. <strong>merger</strong> مثال دیگری از <strong>adapter pattern</strong> است، اگرچه در این مورد، <strong>adapter</strong> در واقع در حال تطبیق چندین <strong>source containers</strong> در حال اجرا به یک <strong>source</strong> ادغام شده است.
  </p>
<p>
   این <strong>multi-adapter pattern</strong> در شکل 11-7 نشان داده شده است.
  </p>
<h3>Figure 11-7.</h3>
<p>
   Using multiple levels of containers to adapt multiple independent work queues into a single shared queue
  </p>
<p>
   Patterns of Event-Driven Processing
  </p>
<p>
   127
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 141" src="page_0141/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0141</div>
            </div>
        </div>
        <!-- Page 0142 -->
        <div class="chapter" id="page-0142">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Hands On: Building an Event-Driven Flow for New User Sign-Up</h3>
<p>
   یک مثال عینی از یک <strong>workflow</strong> به نشان دادن چگونگی قرار دادن این الگوها در کنار هم برای تشکیل یک سیستم عامل کامل کمک می‌کند. مشکلی که این مثال در نظر خواهد گرفت، یک <strong>new-user signup flow</strong> است.
  </p>
<p>
   تصور کنید که قیف <strong>user acquisition</strong> ما دو مرحله دارد. اولین مورد <strong>user verification</strong> است. پس از اینکه یک کاربر جدید ثبت نام کرد، کاربر باید یک اعلان ایمیل برای تأیید ایمیل خود دریافت کند. هنگامی که کاربر ایمیل خود را تأیید کرد، یک ایمیل تأیید برای او ارسال می‌شود. سپس آنها به صورت اختیاری برای ایمیل، پیام متنی، هر دو یا هیچ‌کدام برای <strong>notifications</strong> ثبت‌نام می‌کنند.
  </p>
<p>
   اولین گام در <strong>event-driven workflow</strong>، تولید ایمیل تأیید است. برای دستیابی به این هدف به طور قابل اعتماد، ما از الگوی <strong>shard</strong> برای تقسیم کاربران در چندین منطقه خرابی جغرافیایی مختلف استفاده خواهیم کرد. این اطمینان می‌دهد که ما به پردازش <strong>new user signups</strong> ادامه می‌دهیم، حتی در صورت وجود خرابی‌های جزئی. هر <strong>work queue shard</strong> یک ایمیل تأیید به کاربر نهایی ارسال می‌کند. در این مرحله، این مرحله فرعی از <strong>workflow</strong> تکمیل می‌شود. این مرحله اول از <strong>flow</strong> در شکل 11-8 نشان داده شده است.
  </p>
<h3>Figure 11-8.</h3>
<p>
   The first stage of the workflow for user sign-up
  </p>
<p>
<strong>workflow</strong> دوباره شروع می‌شود زمانی که ما یک ایمیل تأیید از کاربر نهایی دریافت می‌کنیم. این ایمیل‌ها به <strong>events</strong> جدید در یک <strong>workflow</strong> جداگانه (اما به وضوح مرتبط) تبدیل می‌شوند که ایمیل‌های خوش‌آمدگویی را ارسال می‌کند و <strong>notifications</strong> را تنظیم می‌کند. اولین مرحله از این <strong>workflow</strong> نمونه‌ای از <strong>copier pattern</strong> است، جایی که کاربر به دو <strong>work queues</strong> کپی می‌شود. اولین <strong>work queue</strong> مسئول ارسال ایمیل خوش‌آمدگویی است و <strong>work queue</strong> دوم مسئول راه‌اندازی <strong>user notifications</strong> است.
  </p>
<p>
   هنگامی که <strong>work items</strong> بین <strong>queues</strong> تکرار شدند، <strong>email-sending queue</strong> به سادگی مراقب ارسال یک پیام ایمیل است و <strong>workflow</strong> خارج می‌شود. اما به یاد داشته باشید که به دلیل استفاده از <strong>copier pattern</strong>، همچنان یک
  </p>
<p>
   128
  </p>
<p>
   Chapter 11: Event-Driven Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 142" src="page_0142/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0142</div>
            </div>
        </div>
        <!-- Page 0143 -->
        <div class="chapter" id="page-0143">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<strong>copy</strong> از <strong>event</strong> فعال در <strong>workflow</strong> ما. این <strong>copier</strong> یک <strong>work queue</strong> اضافی را برای رسیدگی به تنظیمات <strong>notification</strong> راه‌اندازی می‌کند. این <strong>work queue</strong> به نمونه‌ای از <strong>filter pattern</strong> وارد می‌شود که <strong>work queue</strong> را به <strong>queues</strong> اعلان ایمیل و پیام متنی جداگانه تقسیم می‌کند. این <strong>queues</strong> خاص، کاربر را برای اعلان‌های ایمیل، متن یا هر دو ثبت می‌کنند.
  </p>
<p>
   بقیه این <strong>workflow</strong> در شکل 11-9 نشان داده شده است.
  </p>
<h3>Figure 11-9.</h3>
<p>
   The user notification and welcome email work queue
  </p>
<h3>Publisher/Subscriber Infrastructure</h3>
<p>
   ما انواع مختلفی از الگوهای انتزاعی را برای پیوند دادن الگوهای پردازش <strong>batch</strong> مبتنی بر <strong>event</strong> دیده‌ایم. اما وقتی زمان ساخت چنین سیستمی فرا می‌رسد، ما باید بفهمیم چگونه <strong>stream</strong> داده‌ای را که از طریق <strong>event-driven workflow</strong> عبور می‌کند، مدیریت کنیم. ساده‌ترین کار این است که هر عنصر را در <strong>work queue</strong> به یک دایرکتوری خاص در یک <strong>filesystem</strong> محلی بنویسیم و سپس هر مرحله آن دایرکتوری را برای ورودی نظارت کند. اما البته انجام این کار با یک <strong>filesystem</strong> محلی، <strong>workflow</strong> ما را به عملکرد در یک گره واحد محدود می‌کند. ما می‌توانیم یک <strong>network file-system</strong> را برای توزیع فایل‌ها به چندین گره معرفی کنیم، اما این امر پیچیدگی فزاینده‌ای را هم در کد ما و هم در استقرار <strong>batch workflow</strong> ایجاد می‌کند.
  </p>
<p>
   در عوض، یک رویکرد محبوب برای ساختن یک <strong>workflow</strong> مانند این، استفاده از یک <strong>publisher/subscriber (pub/sub) API</strong> یا <strong>service</strong> است. یک <strong>pub/sub API</strong> به کاربر اجازه می‌دهد تا یک
  </p>
<p>
   Publisher/Subscriber Infrastructure
  </p>
<p>
   129
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 143" src="page_0143/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0143</div>
            </div>
        </div>
        <!-- Page 0144 -->
        <div class="chapter" id="page-0144">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
<strong>tion</strong> از <strong>queues</strong> (که گاهی اوقات <strong>topics</strong> نامیده می‌شود). یک یا چند <strong>publisher</strong> پیام‌هایی را به این <strong>queues</strong> منتشر می‌کند. به همین ترتیب، یک یا چند <strong>subscriber</strong> در حال گوش دادن به این <strong>queues</strong> برای پیام‌های جدید هستند. هنگامی که یک پیام منتشر می‌شود، به طور قابل اطمینانی توسط <strong>queue</strong> ذخیره می‌شود و متعاقباً به <strong>subscribers</strong> به روشی قابل اطمینان تحویل داده می‌شود.
  </p>
<p>
   در این مرحله، اکثر <strong>public clouds</strong> دارای یک <strong>pub/sub API</strong> مانند <strong>Azure’s EventGrid</strong> یا <strong>Amazon’s Simple Queue Service</strong> هستند. علاوه بر این، پروژه <strong>Kafka</strong> متن‌باز، یک پیاده‌سازی <strong>pub/sub</strong> بسیار محبوب ارائه می‌دهد که می‌توانید آن را روی سخت‌افزار خود و همچنین روی ماشین‌های مجازی <strong>cloud</strong> اجرا کنید. برای بقیه این نمای کلی از <strong>pub/sub APIs</strong> ما از <strong>Kafka</strong> برای مثال‌های خود استفاده خواهیم کرد، اما انتقال آن‌ها به <strong>pub/sub APIs</strong> جایگزین نسبتاً ساده است.
  </p>
<h3>Hands On: Deploying Kafka</h3>
<p>
   بدیهی است راه‌های زیادی برای استقرار <strong>Kafka</strong> وجود دارد و یکی از ساده‌ترین راه‌ها این است که آن را به عنوان یک <strong>container</strong> با استفاده از یک <strong>Kubernetes cluster</strong> و <strong>Helm package manager</strong> اجرا کنید.
  </p>
<p>
<strong>Helm</strong> یک <strong>package manager</strong> برای <strong>Kubernetes</strong> است که استقرار و مدیریت برنامه‌های از پیش بسته‌بندی شده و از پیش آماده شده مانند <strong>Kafka</strong> را آسان می‌کند. اگر از قبل ابزار <strong>command line</strong> را نصب نکرده‌اید، می‌توانید آن را از https://helm.sh نصب کنید.
  </p>
<p>
   هنگامی که ابزار <strong>helm</strong> روی دستگاه شما قرار گرفت، باید آن را مقداردهی اولیه کنید. مقداردهی اولیه <strong>Helm</strong> یک جزء سمت <strong>cluster</strong> به نام <strong>tiller</strong> را در <strong>cluster</strong> شما مستقر می‌کند و برخی از <strong>templates</strong> را در <strong>filesystem</strong> محلی شما نصب می‌کند:
  </p>
<pre><code class="language-bash">
helm init
  </code></pre>
<p>
   اکنون که <strong>helm</strong> را مقداردهی اولیه کرده‌اید، می‌توانید <strong>Kafka</strong> را با استفاده از این دستور نصب کنید:
  </p>
<pre><code class="language-bash">
helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator
helm install --name kafka-service incubator/kafka
  </code></pre>
<p>
<strong>Helm templates</strong> دارای سطوح مختلف <strong>production hardening</strong> و پشتیبانی هستند. <strong>stable templates</strong> سخت‌گیرانه‌ترین بررسی و پشتیبانی را دارند، در حالی که <strong>incubator templates</strong> مانند <strong>Kafka</strong> آزمایشی‌تر هستند و <strong>mileage production</strong> کمتری دارند. صرف نظر از این، <strong>incubator templates</strong> برای اثبات سریع مفاهیم و همچنین مکانی برای شروع هنگام پیاده‌سازی یک استقرار تولیدی از یک <strong>service</strong> مبتنی بر <strong>Kubernetes</strong> مفید هستند.
  </p>
<p>
   هنگامی که <strong>Kafka</strong> را راه‌اندازی و اجرا کردید، می‌توانید یک <strong>topic</strong> برای انتشار ایجاد کنید. به طور کلی در پردازش <strong>batch</strong>، شما می‌خواهید از یک <strong>topic</strong> برای نشان دادن خروجی یک ماژول در <strong>workflow</strong> خود استفاده کنید. این خروجی احتمالاً ورودی برای ماژول دیگری در <strong>workflow</strong> است.
  </p>
<p>
   130
  </p>
<p>
   Chapter 11: Event-Driven Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 144" src="page_0144/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0144</div>
            </div>
        </div>
        <!-- Page 0145 -->
        <div class="chapter" id="page-0145">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   به عنوان مثال، اگر از <strong>Sharder pattern</strong> که قبلاً توضیح داده شد استفاده می‌کنید، شما برای هر یک از <strong>output shards</strong> یک <strong>topic</strong> خواهید داشت. اگر خروجی خود را <strong>Photos</strong> نامیده‌اید و تصمیم دارید سه <strong>shards</strong> داشته باشید، در این صورت سه <strong>topic</strong> خواهید داشت: <strong>Photos-1, Photos-2</strong> و <strong>Photos-3</strong>. ماژول <strong>sharder</strong> شما پیام‌ها را پس از اعمال تابع <strong>sharding</strong> به <strong>topic</strong> مناسب، خروجی می‌دهد.
  </p>
<p>
   در اینجا نحوه ایجاد یک <strong>topic</strong> آمده است. ابتدا، یک <strong>container</strong> در <strong>cluster</strong> ایجاد کنید تا بتوانیم به <strong>Kafka</strong> دسترسی داشته باشیم:
  </p>
<pre><code class="language-bash">
for x in 0 1 2; do
  kubectl run kafka --image=solsson/kafka:0.11.0.0 --rm --attach --command -- \
    ./bin/kafka-topics.sh --create --zookeeper kafka-service-zookeeper:2181 \
      --replication-factor 3 --partitions 10 --topic photos-$x
done
  </code></pre>
<p>
   توجه داشته باشید که دو پارامتر جالب علاوه بر نام <strong>topic</strong> و سرویس <strong>zookeeper</strong> وجود دارد. آنها عبارتند از <strong>--replication-factor</strong> و <strong>--partitions</strong>. <strong>The replication factor</strong> تعداد ماشین‌های مختلفی است که پیام‌ها در <strong>topic</strong> به آن‌ها تکرار می‌شوند. این افزونگی است که در صورت خرابی در دسترس است. مقدار 3 یا 5 توصیه می‌شود. پارامتر دوم تعداد <strong>partitions</strong> برای <strong>topic</strong> است. تعداد <strong>partitions</strong>، حداکثر توزیع <strong>topic</strong> را بر روی چندین ماشین برای اهداف <strong>load balancing</strong> نشان می‌دهد. در این مورد، از آنجایی که 10 <strong>partitions</strong> وجود دارد، حداکثر 10 <strong>replicas</strong> مختلف از <strong>topic</strong> برای <strong>load balancing</strong> وجود دارد.
  </p>
<p>
   اکنون که یک <strong>topic</strong> ایجاد کرده‌ایم، می‌توانیم پیام‌ها را به آن <strong>topic</strong> ارسال کنیم:
  </p>
<pre><code class="language-bash">
kubectl run kafka-producer --image=solsson/kafka:0.11.0.0 --rm -it --command -- \
    ./bin/kafka-console-producer.sh --broker-list kafka-service-kafka:9092 \
    --topic photos-1
  </code></pre>
<p>
   هنگامی که آن دستور اجرا شد و متصل شد، باید <strong>Kafka prompt</strong> را مشاهده کنید و سپس می‌توانید پیام‌ها را به <strong>topic(s)</strong> ارسال کنید. برای دریافت پیام‌ها، می‌توانید اجرا کنید:
  </p>
<pre><code class="language-bash">
kubectl run kafka-consumer --image=solsson/kafka:0.11.0.0 --rm -it --command -- \
    ./bin/kafka-console-consumer.sh --bootstrap-server kafka-service-kafka:9092\
    --topic photos-1 \
        --from-beginning
  </code></pre>
<p>
   البته، اجرای این خطوط <strong>command</strong> تنها به شما چشیدن نحوه برقراری ارتباط از طریق پیام‌های <strong>Kafka</strong> را می‌دهد. برای ساخت یک سیستم پردازش <strong>batch</strong> مبتنی بر <strong>event</strong> در دنیای واقعی، احتمالاً از یک زبان برنامه‌نویسی مناسب و <strong>Kafka SDK</strong> برای دسترسی به <strong>service</strong> استفاده خواهید کرد. اما از سوی دیگر، هرگز قدرت یک <strong>Bash script</strong> خوب را دست کم نگیرید!
  </p>
<p>
   این بخش نشان داده است که چگونه نصب <strong>Kafka</strong> در <strong>Kubernetes cluster</strong> شما می‌تواند وظیفه ساخت یک سیستم مبتنی بر <strong>work queue</strong> را به طور چشمگیری ساده کند.
  </p>
<p>
   Hands On: Deploying Kafka
  </p>
<p>
   131
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0145</div>
            </div>
        </div>
        <!-- Page 0147 -->
        <div class="chapter" id="page-0147">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>CHAPTER 12</h3>
<h3>Coordinated Batch Processing</h3>
<p>
   فصل قبل تعدادی الگو برای تقسیم و زنجیر کردن <strong>queues</strong> به منظور دستیابی به پردازش <strong>batch</strong> پیچیده‌تر را شرح داد. تکرار و تولید چندین خروجی مختلف اغلب بخش مهمی از پردازش <strong>batch</strong> است، اما گاهی اوقات جمع‌آوری چندین خروجی به منظور تولید نوعی خروجی <strong>aggregate</strong> نیز به همان اندازه مهم است. یک تصویر <strong>generic</strong> از چنین الگویی در شکل 12-1 نشان داده شده است.
  </p>
<h3>Figure 12-1.</h3>
<p>
   A generic parallel work distribution and result aggregation batch system
  </p>
<p>
   133
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 147" src="page_0147/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0147</div>
            </div>
        </div>
        <!-- Page 0148 -->
        <div class="chapter" id="page-0148">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   احتمالاً متعارف‌ترین مثال از این <strong>aggregation</strong>، قسمت <strong>reduce</strong> از <strong>MapReduce pattern</strong> است. به راحتی می‌توان دید که مرحله <strong>map</strong> نمونه‌ای از <strong>sharding</strong> یک <strong>work queue</strong> است، و مرحله <strong>reduce</strong> نمونه‌ای از پردازش هماهنگ است که در نهایت تعداد زیادی خروجی را به یک پاسخ <strong>aggregate</strong> واحد کاهش می‌دهد. با این حال، تعدادی الگوهای <strong>aggregate</strong> مختلف برای پردازش <strong>batch</strong> وجود دارد، و این فصل علاوه بر برنامه‌های کاربردی دنیای واقعی، تعدادی از آن‌ها را مورد بحث قرار می‌دهد.
  </p>
<h3>Join (or Barrier Synchronization)</h3>
<p>
   در فصل‌های قبل، الگوهایی را برای تقسیم کار و توزیع آن به موازات در چندین گره دیدیم. به طور خاص، دیدیم که چگونه یک <strong>sharded work queue</strong> می‌تواند کار را به موازات به تعدادی از <strong>work queue shards</strong> مختلف توزیع کند. با این حال، گاهی اوقات هنگام پردازش یک <strong>workflow</strong>، لازم است که مجموعه کاملی از کار را قبل از رفتن به مرحله بعدی <strong>workflow</strong> داشته باشید.
  </p>
<p>
   یک گزینه برای انجام این کار در فصل قبل نشان داده شد، که ادغام چندین <strong>queues</strong> با هم بود. با این حال، <strong>merge</strong> به سادگی خروجی دو <strong>work queues</strong> را در یک <strong>work queue</strong> واحد برای پردازش بیشتر ترکیب می‌کند. در حالی که <strong>merge pattern</strong> در برخی موارد کافی است، این اطمینان را نمی‌دهد که یک مجموعه داده کامل قبل از شروع پردازش وجود داشته باشد. این بدان معناست که نمی‌توان هیچ تضمینی در مورد کامل بودن پردازش انجام شده و همچنین هیچ فرصتی برای محاسبه آمار <strong>aggregate</strong> برای همه عناصری که پردازش شده‌اند، وجود ندارد.
  </p>
<p>
   در عوض، ما به یک <strong>primitive</strong> هماهنگ و قوی‌تر برای پردازش داده‌های <strong>batch</strong> نیاز داریم، و آن <strong>primitive</strong>، <strong>join pattern</strong> است. <strong>Join</strong> شبیه پیوستن به یک <strong>thread</strong> است. ایده اصلی این است که همه کارها به صورت موازی در حال انجام هستند، اما <strong>work items</strong> تا زمانی که همه <strong>work items</strong>ی که به صورت موازی پردازش می‌شوند، تکمیل نشوند، از <strong>join</strong> منتشر نمی‌شوند. این نیز به طور کلی به عنوان <strong>barrier synchronization</strong> در برنامه‌نویسی همزمان شناخته می‌شود. یک تصویر از <strong>join pattern</strong> برای یک <strong>batch</strong> هماهنگ در شکل 12-2 نشان داده شده است.
  </p>
<p>
   هماهنگی از طریق <strong>join</strong> تضمین می‌کند که قبل از انجام نوعی فاز <strong>aggregation</strong>، هیچ داده‌ای وجود ندارد (به عنوان مثال، یافتن مجموع مقداری در یک مجموعه). ارزش <strong>join</strong> این است که تضمین می‌کند همه داده‌های موجود در مجموعه وجود دارند. جنبه منفی <strong>join pattern</strong> این است که مستلزم آن است که همه داده‌ها توسط یک مرحله قبلی پردازش شوند قبل از اینکه محاسبات بعدی بتواند شروع شود. این موازی‌سازی ممکن در <strong>batch workflow</strong> را کاهش می‌دهد و بنابراین <strong>latency</strong> کلی اجرای <strong>workflow</strong> را افزایش می‌دهد.
  </p>
<p>
   134
  </p>
<p>
   Chapter 12: Coordinated Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0148</div>
            </div>
        </div>
        <!-- Page 0149 -->
        <div class="chapter" id="page-0149">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 12-2.</h3>
<p>
   The join pattern for batch processing
  </p>
<h3>Reduce</h3>
<p>
   اگر <strong>sharding</strong> یک <strong>work queue</strong> نمونه‌ای از فاز <strong>map</strong> الگوریتم متعارف <strong>map/reduce</strong> است، پس آنچه باقی می‌ماند فاز <strong>reduce</strong> است. <strong>Reduce</strong> نمونه‌ای از یک الگوی پردازش <strong>batch</strong> هماهنگ است زیرا می‌تواند صرف نظر از نحوه تقسیم ورودی اتفاق بیفتد و مشابه <strong>join</strong> استفاده می‌شود. به این معنی که برای جمع‌آوری خروجی موازی تعدادی از عملیات <strong>batch</strong> مختلف بر روی قطعات مختلف داده‌ها استفاده می‌شود.
  </p>
<p>
   با این حال، بر خلاف <strong>join pattern</strong> که قبلاً توضیح داده شد، هدف <strong>reduce</strong> انتظار برای پردازش تمام داده‌ها نیست، بلکه ادغام خوش‌بینانه تمام <strong>data items</strong> موازی در یک نمایش جامع واحد از مجموعه کامل داده‌ها است. با <strong>reduce pattern</strong>، هر مرحله در <strong>reduce</strong> چندین خروجی مختلف را در یک خروجی واحد ادغام می‌کند. این مرحله «<strong>reduce</strong>» نامیده می‌شود زیرا تعداد کل خروجی‌ها را کاهش می‌دهد. علاوه بر این، داده‌ها را از یک <strong>data item</strong> کامل به سادگی داده‌های نماینده لازم برای تولید پاسخ به یک <strong>computation batch</strong> خاص کاهش می‌دهد. از آنجایی که فاز <strong>reduce</strong> بر روی طیفی از ورودی عمل می‌کند و خروجی مشابهی تولید می‌کند، فاز <strong>reduce</strong> می‌تواند به دفعات لازم تکرار شود تا با موفقیت خروجی را به یک خروجی واحد برای کل مجموعه داده کاهش دهد.
  </p>
<p>
   Reduce
  </p>
<p>
   135
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 149" src="page_0149/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0149</div>
            </div>
        </div>
        <!-- Page 0150 -->
        <div class="chapter" id="page-0150">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   این یک تضاد خوشایند با <strong>join pattern</strong> است، زیرا بر خلاف <strong>join</strong>، به این معنی است که <strong>reduce</strong> می‌تواند به صورت موازی شروع شود در حالی که هنوز هم پردازشی به عنوان بخشی از فاز <strong>map/shard</strong> در حال انجام است. البته، برای تولید یک خروجی کامل، باید در نهایت تمام داده‌ها پردازش شوند، اما توانایی شروع زود هنگام به این معنی است که <strong>batch computation</strong> به طور کلی سریع‌تر اجرا می‌شود.
  </p>
<h3>Hands On: Count</h3>
<p>
   برای درک نحوه عملکرد <strong>reduce pattern</strong>، وظیفه شمارش تعداد دفعات یک کلمه خاص در یک کتاب را در نظر بگیرید. ما ابتدا می‌توانیم از <strong>sharding</strong> برای تقسیم کار شمارش کلمات به تعدادی <strong>work queues</strong> مختلف استفاده کنیم. به عنوان مثال، ما می‌توانیم 10 <strong>work queues</strong> مختلف <strong>sharded</strong> با 10 نفر مختلف ایجاد کنیم که مسئول شمارش کلمات در هر <strong>queue</strong> هستند. ما می‌توانیم کتاب را با نگاه کردن به شماره صفحه، بین این 10 <strong>work queues</strong> <strong>shard</strong> کنیم. همه صفحاتی که به عدد 1 ختم می‌شوند به اولین <strong>queue</strong> می‌روند، همه صفحاتی که به عدد 2 ختم می‌شوند به <strong>queue</strong> دوم می‌روند و غیره.
  </p>
<p>
   هنگامی که همه افراد پردازش صفحات خود را به پایان رساندند، نتایج خود را روی یک تکه کاغذ می‌نویسند. به عنوان مثال، ممکن است بنویسند:
  </p>
<pre><code class="language-none">
a: 50
the: 17
cat: 2
airplane: 1
...
  </code></pre>
<p>
   این را می‌توان به فاز <strong>reduce</strong> خروجی داد. به یاد داشته باشید که <strong>reduce pattern</strong> با ترکیب دو یا چند خروجی در یک خروجی واحد، کاهش می‌یابد.
  </p>
<p>
   با توجه به یک خروجی دوم:
  </p>
<pre><code class="language-none">
a: 30
the: 25
dog: 4
airplane: 2
...
  </code></pre>
<p>
   کاهش با جمع کردن تمام شمارش‌ها برای کلمات مختلف ادامه می‌یابد، که در این مثال تولید می‌کند:
  </p>
<pre><code class="language-none">
a: 80
the 42
dog: 4
cat: 2
airplane: 3
...
  </code></pre>
<p>
   واضح است که این فاز کاهش را می‌توان بر روی خروجی فازهای <strong>reduce</strong> قبلی تکرار کرد تا زمانی که فقط یک خروجی کاهش یافته باقی بماند. این ارزشمند است زیرا این بدان معناست که کاهش‌ها را می‌توان به صورت موازی انجام داد.
  </p>
<p>
   136
  </p>
<p>
   Chapter 12: Coordinated Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0150</div>
            </div>
        </div>
        <!-- Page 0151 -->
        <div class="chapter" id="page-0151">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   در نهایت، در این مثال می‌توانید ببینید که خروجی <strong>reduction</strong> یک خروجی واحد با شمارش تمام کلمات مختلفی که در کتاب وجود دارد، خواهد بود.
  </p>
<h3>Sum</h3>
<p>
   یک شکل مشابه اما کمی متفاوت از <strong>reduction</strong>، جمع‌بندی مجموعه‌ای از مقادیر مختلف است. این شبیه شمارش است، اما به جای اینکه به سادگی برای هر مقدار یک واحد شمارش کنید، در واقع یک مقدار را که در داده‌های خروجی اصلی وجود دارد، با هم جمع می‌کنید.
  </p>
<p>
   به عنوان مثال، فرض کنید می‌خواهید جمعیت کل ایالات متحده را جمع کنید. فرض کنید که شما این کار را با اندازه‌گیری جمعیت در هر شهر و سپس جمع کردن همه آنها با هم انجام خواهید داد.
  </p>
<p>
   اولین قدم ممکن است این باشد که کار را به <strong>work queues</strong> شهرها، <strong>sharded</strong> شده بر اساس ایالت، <strong>shard</strong> کنید. این یک <strong>sharding</strong> عالی در ابتدا است، اما واضح است که حتی زمانی که به صورت موازی توزیع شود، زمان زیادی می‌برد تا یک فرد تعداد افراد را در هر شهر بشمارد. در نتیجه، ما یک <strong>sharding</strong> دوم به مجموعه دیگری از <strong>work queues</strong> انجام می‌دهیم، این بار بر اساس شهرستان.
  </p>
<p>
   در این مرحله، ما ابتدا به سطح ایالت‌ها و سپس به سطح شهرستان‌ها موازی‌سازی کرده‌ایم، و سپس هر <strong>work queue</strong> در هر شهرستان، یک جریان از خروجی‌های <strong>tuples</strong> (شهر، جمعیت) تولید می‌کند.
  </p>
<p>
   اکنون که در حال تولید خروجی هستیم، <strong>reduce pattern</strong> می‌تواند شروع شود.
  </p>
<p>
   در این مورد، <strong>reduce</strong> واقعاً نیازی به آگاهی از <strong>sharding</strong> دو سطحی که انجام داده‌ایم ندارد. این برای <strong>reduce</strong> کافی است که به سادگی دو یا چند آیتم خروجی، مانند (سیاتل، 4,000,000) و (نورثهمپتون، 25,000) را دریافت کرده و آن‌ها را با هم جمع کند تا یک خروجی جدید (سیاتل-نورثهمپتون، 4,025,000) تولید شود. واضح است که، مانند شمارش، این <strong>reduction</strong> را می‌توان به تعداد دلخواه بار با اجرای همان کد در هر فاصله انجام داد، و در پایان، فقط یک خروجی واحد وجود خواهد داشت که شامل جمعیت کامل ایالات متحده است. مهمتر از همه، باز هم، تقریباً تمام محاسبات مورد نیاز به صورت موازی در حال انجام است.
  </p>
<h3>Histogram</h3>
<p>
   به عنوان مثال نهایی از <strong>reduce pattern</strong>، در نظر بگیرید که در حالی که ما در حال شمارش جمعیت ایالات متحده از طریق <strong>sharding/mapping</strong> موازی و <strong>reducing</strong> هستیم، می‌خواهیم یک مدل از خانواده متوسط ​​آمریکایی نیز بسازیم. برای انجام این کار، ما می‌خواهیم یک <strong>histogram</strong> از اندازه خانواده ایجاد کنیم. یعنی مدلی که تعداد کل خانواده‌ها را با صفر تا 10 فرزند تخمین می‌زند. ما <strong>sharding</strong> چند سطحی خود را دقیقاً مانند قبل انجام خواهیم داد (در واقع، احتمالاً می‌توانیم از همان <strong>workers</strong> استفاده کنیم).
  </p>
<p>
   Reduce
  </p>
<p>
   137
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0151</div>
            </div>
        </div>
        <!-- Page 0152 -->
        <div class="chapter" id="page-0152">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   با این حال، این بار، خروجی فاز جمع‌آوری داده‌ها، یک <strong>histogram</strong> در هر شهر است.
  </p>
<pre><code class="language-none">
0: 15%
1: 25%
2: 50%
3: 10%
4: 5%
  </code></pre>
<p>
   از مثال‌های قبلی، می‌توانیم ببینیم که اگر <strong>reduce pattern</strong> را اعمال کنیم، باید بتوانیم همه این <strong>histograms</strong> را ترکیب کنیم تا تصویری جامع از ایالات متحده ایجاد کنیم. در ابتدا، ممکن است درک نحوه ادغام این <strong>histograms</strong> بسیار دشوار به نظر برسد، اما هنگامی که با داده‌های جمعیت از مثال جمع‌بندی ترکیب شود، می‌توانیم ببینیم که اگر هر <strong>histogram</strong> را در جمعیت نسبی آن ضرب کنیم، می‌توانیم جمعیت کل را برای هر <strong>item</strong> در حال ادغام به دست آوریم. اگر سپس این مجموع جدید را بر مجموع جمعیت‌های ادغام شده تقسیم کنیم، واضح است که می‌توانیم چندین <strong>histograms</strong> مختلف را در یک خروجی واحد ادغام و به‌روزرسانی کنیم. با توجه به این، ما می‌توانیم <strong>reduce pattern</strong> را به تعداد دفعات لازم اعمال کنیم تا زمانی که یک خروجی واحد تولید شود.
  </p>
<h3>Hands On: An Image Tagging and Processing Pipeline</h3>
<p>
   برای دیدن چگونگی استفاده از پردازش <strong>batch</strong> هماهنگ برای انجام یک کار <strong>batch</strong> بزرگتر، کار برچسب‌گذاری و پردازش مجموعه‌ای از تصاویر را در نظر بگیرید. اجازه دهید فرض کنیم که ما مجموعه بزرگی از تصاویر بزرگراه‌ها در ساعت شلوغی داریم، و ما می‌خواهیم تعداد اتومبیل‌ها، کامیون‌ها و موتورسیکلت‌ها و همچنین توزیع رنگ‌های هر یک از اتومبیل‌ها را بشماریم. اجازه دهید همچنین فرض کنیم که یک مرحله مقدماتی برای محو کردن پلاک‌های همه اتومبیل‌ها برای حفظ ناشناس بودن وجود دارد.
  </p>
<p>
   تصاویر به صورت یک سری <strong>HTTPS URLs</strong> به ما تحویل داده می‌شوند که هر <strong>URL</strong> به یک <strong>image</strong> خام اشاره دارد. اولین مرحله در <strong>pipeline</strong> یافتن و محو کردن پلاک‌ها است. برای ساده‌سازی هر کار در <strong>work queue</strong>، ما یک <strong>worker</strong> خواهیم داشت که یک پلاک را شناسایی می‌کند و یک <strong>worker</strong> دوم که آن مکان را در تصویر محو می‌کند. ما این دو <strong>worker containers</strong> مختلف را با استفاده از <strong>multi-worker pattern</strong> که در فصل قبل توضیح داده شد، در یک گروه <strong>container</strong> واحد ترکیب خواهیم کرد. این جداسازی نگرانی‌ها ممکن است غیر ضروری به نظر برسد، اما با توجه به اینکه <strong>workers</strong> برای محو کردن تصاویر را می‌توان برای محو کردن سایر خروجی‌ها (به عنوان مثال، چهره افراد) استفاده مجدد کرد، مفید است.
  </p>
<p>
   علاوه بر این، برای اطمینان از قابلیت اطمینان و به حداکثر رساندن پردازش موازی، تصاویر را در چندین <strong>worker queues shard</strong> خواهیم کرد. این <strong>workflow</strong> کامل برای <strong>sharded image blurring</strong> در شکل 12-3 نشان داده شده است.
  </p>
<p>
   138
  </p>
<p>
   Chapter 12: Coordinated Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0152</div>
            </div>
        </div>
        <!-- Page 0153 -->
        <div class="chapter" id="page-0153">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 12-3.</h3>
<p>
   The sharded work queue and the multiple blurring shards
  </p>
<p>
   هنگامی که هر تصویر با موفقیت محو شد، ما آن را در یک مکان متفاوت آپلود می‌کنیم و سپس <strong>originals</strong> را حذف خواهیم کرد. با این حال، ما نمی‌خواهیم <strong>original</strong> را حذف کنیم تا زمانی که همه تصاویر با موفقیت محو شده باشند، در صورتی که نوعی خرابی فاجعه‌بار وجود داشته باشد و ما نیاز به راه‌اندازی مجدد کل این <strong>pipeline</strong> داشته باشیم. بنابراین، برای انتظار برای تکمیل همه محو کردن، ما از <strong>join pattern</strong> برای ادغام خروجی تمام <strong>sharded blurring work queues</strong> به یک <strong>queue</strong> واحد استفاده می‌کنیم که <strong>items</strong> خود را تنها پس از تکمیل کار توسط همه <strong>shards</strong>، منتشر می‌کند.
  </p>
<p>
   اکنون ما آماده‌ایم تا تصاویر <strong>original</strong> را حذف کنیم و همچنین کار را بر روی مدل ماشین و تشخیص رنگ شروع کنیم. باز هم، ما می‌خواهیم <strong>throughput</strong> این <strong>pipeline</strong> را به حداکثر برسانیم، بنابراین از <strong>copier pattern</strong> از فصل قبل برای تکرار <strong>work queue items</strong> به دو <strong>queues</strong> مختلف استفاده خواهیم کرد:
  </p>
<ul>
<li>یک <strong>work queue</strong> که تصاویر <strong>original</strong> را حذف می‌کند</li>
<li>یک <strong>work queue</strong> که نوع وسیله نقلیه (اتومبیل، کامیون، موتورسیکلت) و رنگ وسیله نقلیه را شناسایی می‌کند</li>
</ul>
<p>
   شکل 12-4 این مراحل از <strong>processing pipeline</strong> را نشان می‌دهد.
  </p>
<p>
   Hands On: An Image Tagging and Processing Pipeline
  </p>
<p>
   139
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 153" src="page_0153/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0153</div>
            </div>
        </div>
        <!-- Page 0154 -->
        <div class="chapter" id="page-0154">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>Figure 12-4.</h3>
<p>
   The output join, copier, deletion, and image recognition parts of the pipeline
  </p>
<p>
   در نهایت ما نیاز داریم <strong>queue</strong>ی را طراحی کنیم که وسایل نقلیه و رنگ‌ها را شناسایی کرده و این آمار را در یک شمارش نهایی جمع‌آوری می‌کند. برای انجام این کار، ما ابتدا دوباره <strong>shard pattern</strong> را اعمال می‌کنیم تا کار را به تعدادی از <strong>queues</strong> توزیع کنیم. هر یک از این <strong>queues</strong> دو <strong>workers</strong> مختلف دارند: یکی که موقعیت و نوع هر وسیله نقلیه را شناسایی می‌کند و دیگری که رنگ یک منطقه را شناسایی می‌کند. ما دوباره اینها را با استفاده از <strong>multi-worker pattern</strong> که در فصل قبل توضیح داده شد، به هم متصل می‌کنیم. مانند قبل، جداسازی کد در <strong>containers</strong> مختلف ما را قادر می‌سازد تا از <strong>container</strong> تشخیص رنگ برای چندین کار فراتر از شناسایی رنگ اتومبیل‌ها استفاده مجدد کنیم.
  </p>
<p>
   خروجی این <strong>work queue</strong> یک <strong>JSON tuple</strong> است که به این صورت است:
  </p>
<pre><code class="language-json">
{
  "vehicles": {
     "car": 12,
     "truck": 7,
     "motorcycle": 4
   },
   "colors": {
     "white": 8,
     "black": 3,
     "blue": 6,
     "red": 6
  </code></pre>
<p>
   140
  </p>
<p>
   Chapter 12: Coordinated Batch Processing
  </p>
</div>
</div>
                <div class="page-images">
<div class="page-image"><img alt="Image from page 154" src="page_0154/image_1.png"/></div>
</div>
                <div class="page-number">صفحه 0154</div>
            </div>
        </div>
        <!-- Page 0155 -->
        <div class="chapter" id="page-0155">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<pre><code class="language-json">
   }
 }
  </code></pre>
<p>
   این داده‌ها، اطلاعات موجود در یک تصویر واحد را نشان می‌دهد. برای جمع‌آوری همه این داده‌ها با هم، ما از <strong>reduce pattern</strong> که قبلاً توضیح داده شد استفاده خواهیم کرد و توسط <strong>MapReduce</strong> برای جمع‌بندی همه چیز با هم معروف شده است، درست همانطور که در مثال شمارش در بالا انجام دادیم. در پایان، این مرحله <strong>reduce pipeline</strong>، شمارش نهایی تصاویر و رنگ‌های موجود در مجموعه کاملی از تصاویر را تولید می‌کند.
  </p>
<p>
   Hands On: An Image Tagging and Processing Pipeline
  </p>
<p>
   141
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0155</div>
            </div>
        </div>
        <!-- Page 0157 -->
        <div class="chapter" id="page-0157">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>CHAPTER 13</h3>
<h3>Conclusion: A New Beginning?</h3>
<p>
   هر شرکتی، صرف نظر از منشأ آن، در حال تبدیل شدن به یک شرکت دیجیتالی است. این <strong>trans</strong> -<strong>formation</strong> مستلزم ارائه <strong>APIs</strong> و <strong>services</strong> است که توسط <strong>mobile applications</strong>، دستگاه‌های <strong>internet of things (IoT)</strong> یا حتی وسایل نقلیه و سیستم‌های مستقل استفاده شود. افزایش بحرانی بودن این سیستم‌ها به این معنی است که برای این سیستم‌های آنلاین ضروری است که برای <strong>redundancy</strong>، تحمل خطا و در دسترس بودن بالا ساخته شوند. در عین حال، الزامات کسب‌وکار، چابکی سریع را برای توسعه و راه‌اندازی <strong>software</strong> جدید، تکرار برنامه‌های موجود، یا آزمایش با <strong>user interfaces</strong> و <strong>APIs</strong> جدید ضروری می‌کند. هم‌افزایی این الزامات منجر به افزایش یک مرتبه بزرگی در تعداد سیستم‌های توزیع‌شده شده است که باید ساخته شوند.
  </p>
<p>
   وظیفه ساخت این سیستم‌ها هنوز هم بسیار دشوار است. هزینه کلی توسعه، به‌روزرسانی و نگهداری چنین سیستمی بسیار بالاست. به همین ترتیب، مجموعه افرادی که دارای قابلیت‌ها و مهارت‌های ساخت چنین برنامه‌هایی هستند، برای پاسخگویی به نیاز رو به رشد، بسیار کم است.
  </p>
<p>
   از نظر تاریخی، زمانی که این موقعیت‌ها در توسعه <strong>software</strong> و فناوری ظاهر شدند، لایه‌های انتزاع جدید و الگوهای توسعه <strong>software</strong> برای سریع‌تر، آسان‌تر و قابل اطمینان‌تر کردن ساخت <strong>software</strong> پدیدار شدند. این اولین بار با توسعه اولین <strong>compilers</strong> و زبان‌های برنامه‌نویسی رخ داد. بعداً، توسعه زبان‌های برنامه‌نویسی شی‌گرا و کد مدیریت‌شده رخ داد. به همین ترتیب، در هر یک از این لحظات، این پیشرفت‌های فنی، تقطیر دانش و شیوه‌های متخصصان را به مجموعه‌ای از الگوریتم‌ها و الگوهایی که می‌توانستند توسط گروه بسیار وسیع‌تری از متخصصان اعمال شوند، متبلور کردند. پیشرفت‌های فناوری همراه با ایجاد الگوها، روند توسعه <strong>software</strong> را دموکراتیک کرد و مجموعه توسعه‌دهندگانی را که می‌توانستند برنامه‌ها را بر روی پلتفرم جدید بسازند، گسترش داد. این به نوبه خود منجر به توسعه برنامه‌های بیشتر و تنوع برنامه شد، که به نوبه خود بازار مهارت‌های این توسعه‌دهندگان را گسترش داد.
  </p>
<p>
   143
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0157</div>
            </div>
        </div>
        <!-- Page 0158 -->
        <div class="chapter" id="page-0158">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   باز هم، ما خود را در لحظه‌ای از <strong>technological transformation</strong> می‌یابیم. نیاز به سیستم‌های توزیع‌شده بسیار فراتر از توانایی ما برای ارائه آنها است. خوشبختانه، توسعه فناوری مجموعه دیگری از ابزارها را برای گسترش بیشتر مجموعه توسعه‌دهندگانی که قادر به ساخت این سیستم‌های توزیع‌شده هستند، تولید کرده است. توسعه اخیر <strong>containers</strong> و <strong>container orchestration</strong> ابزارهایی را به ارمغان آورده است که توسعه سریع‌تر و آسان‌تر سیستم‌های توزیع‌شده را ممکن می‌سازد. با خوش‌شانسی، این ابزارها، در صورت ترکیب با الگوها و شیوه‌هایی که در این کتاب توضیح داده شده‌اند، می‌توانند سیستم‌های توزیع‌شده‌ای را که توسط توسعه‌دهندگان فعلی ساخته شده‌اند، تقویت و بهبود بخشند، و مهمتر از آن، یک گروه جدید و گسترده از توسعه‌دهندگان را ایجاد کنند که قادر به ساخت این سیستم‌ها هستند.
  </p>
<p>
   الگوهایی مانند <strong>sidecars</strong>، <strong>ambassadors</strong>، <strong>sharded services</strong>، <strong>FaaS</strong>، <strong>work queues</strong>، و موارد دیگر می‌توانند بنیادی را تشکیل دهند که سیستم‌های توزیع‌شده مدرن بر روی آن ساخته می‌شوند. توسعه‌دهندگان سیستم‌های توزیع‌شده دیگر نباید سیستم‌های خود را از ابتدا به عنوان افراد بسازند، بلکه باید با هم بر روی پیاده‌سازی‌های مشترک و قابل استفاده مجدد از الگوهای متعارف که اساس همه سیستم‌هایی را که ما به طور جمعی مستقر می‌کنیم، همکاری کنند. این امر ما را قادر می‌سازد تا نیازهای <strong>APIs</strong> و <strong>services</strong> مقیاس‌پذیر و قابل اعتماد امروزی را برآورده کنیم و مجموعه جدیدی از برنامه‌ها و خدمات را برای آینده توانمند سازیم.
  </p>
<p>
   144
  </p>
<p>
   Chapter 13: Conclusion: A New Beginning?
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0158</div>
            </div>
        </div>
        <!-- Page 0159 -->
        <div class="chapter" id="page-0159">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   Index
  </p>
<p>
   A
  </p>
<p>
<strong>adapter containers</strong>, 31, 36
  </p>
<p>
<strong>adapter patterns</strong>, 31-39
  </p>
<p>
   about, 31
  </p>
<p>
   for health monitoring of application con‐ tainers, 36-39
  </p>
<p>
   for logging, 34-36
  </p>
<p>
   for monitoring, 32-34
  </p>
<p>
   rich health monitoring for <strong>MySQL</strong>, 36-39
  </p>
<p>
<strong>algorithmic programming</strong>, 3
  </p>
<p>
<strong>ambassador patterns</strong>, 21-29
  </p>
<p>
   basics, 21
  </p>
<p>
   for experimentation, 26-29
  </p>
<p>
   for request splitting, 26-29
  </p>
<p>
   for service brokering, 25
  </p>
<p>
   for sharded cache, 63-66
  </p>
<p>
   implementing 10% experiments, 27-29
  </p>
<p>
   implementing sharded <strong>Redis</strong>, 23-25
  </p>
<p>
   sharding a service with, 22-25
  </p>
<p>
   value of, 21
  </p>
<p>
<strong>Apache Storm</strong>, 36
  </p>
<p>
<strong>APIs</strong>
</p>
<p>
   for microservices, 42
  </p>
<p>
   for <strong>sidecar containers</strong>, 17
  </p>
<p>
   pub/sub, 129
  </p>
<p>
   application containers
  </p>
<p>
<strong>adapter container</strong> and, 31
  </p>
<p>
   adapters for health monitoring, 36-39
  </p>
<p>
<strong>sidecar pattern</strong>, 11
  </p>
<p>
   with <strong>sidecar</strong>, 14
  </p>
<p>
   application-layer replicated services, 49
  </p>
<p>
<strong>authentication</strong>, <strong>FaaS</strong> for, 87-89
  </p>
<p>
   B
  </p>
<p>
   background processing, <strong>FaaS</strong> and, 82
  </p>
<p>
   barrier synchronization, 134
  </p>
<p>
   (see also join pattern)
  </p>
<p>
   batch computational patterns
  </p>
<p>
   coordinated batch processing, 133-141
  </p>
<p>
   event-driven batch processing systems, 121-131
  </p>
<p>
   multi-node batch patterns, 107
  </p>
<p>
   best practices, patterns as collection of, 4
  </p>
<p>
   boundaries, 7
  </p>
<p>
   C
  </p>
<p>
   caching layer
  </p>
<p>
   deploying, 50-53
  </p>
<p>
   deploying <strong>nginx</strong> and <strong>SSL</strong> termination, 55-57
  </p>
<p>
   expanding, 53-57
  </p>
<p>
   for stateless service, 49-53
  </p>
<p>
   introducing, 49-57
  </p>
<p>
   rate limiting as denial-of-service defense, 54
  </p>
<p>
<strong>SSL</strong> termination, 54-57
  </p>
<p>
   caching, sharded (see sharded caching)
  </p>
<p>
   compare-and-swap operation, 96
  </p>
<p>
   concurrent data manipulation, 103-105
  </p>
<p>
   configuration synchronization, 12
  </p>
<p>
   consensus algorithm, 96
  </p>
<p>
   consistent hashing function, 49
  </p>
<p>
<strong>container group</strong>, 11
  </p>
<p>
<strong>container images</strong>, 6
  </p>
<p>
   container patterns, single-node, 7-9
  </p>
<p>
   containerization, goals of, 7
  </p>
<p>
   containers
  </p>
<p>
   documentation, 18
  </p>
<p>
   modular, with <strong>sidecars</strong>, 14
  </p>
<p>
   145
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0159</div>
            </div>
        </div>
        <!-- Page 0160 -->
        <div class="chapter" id="page-0160">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   parameterized, 17
  </p>
<p>
   coordinated batch processing, 133-141
  </p>
<p>
   counting example, 136
  </p>
<p>
   histograms with, 137
  </p>
<p>
   image tagging/processing pipeline, 138-141
  </p>
<p>
   join pattern, 134
  </p>
<p>
   reduce pattern, 135
  </p>
<p>
   summing with, 137
  </p>
<p>
   copier pattern, 122
  </p>
<p>
<strong>CoreOS</strong>, 97
  </p>
<p>
   (see also etcd)
  </p>
<p>
   counting, coordinated batch processing for, 136
  </p>
<p>
   D
  </p>
<p>
   data manipulation, concurrent, 103-105
  </p>
<p>
   debugging, microservices-based systems and,
   43
  </p>
<p>
   decorator pattern, 85
  </p>
<p>
   decoupling of microservices, 42
  </p>
<p>
   deep monitoring, 36-39
  </p>
<p>
   denial-of-service attacks, 54
  </p>
<p>
   dictionary-server service
  </p>
<p>
   caching layer deployment, 51-53
  </p>
<p>
   replicated service for, 47
  </p>
<p>
   distributed consensus algorithm, 96
  </p>
<p>
   distributed ownership, 93
  </p>
<p>
   distributed systems (generally)
  </p>
<p>
   current state of, viii
  </p>
<p>
   defined, 7
  </p>
<p>
   future of, 143
  </p>
<p>
   history of patterns in software development, 2-4
  </p>
<p>
   systems development history, 1
  </p>
<p>
   value of patterns, practices, and compo‐ nents, 4-6
  </p>
<p>
<strong>Dockerfile</strong>, 19
  </p>
<p>
   document search
  </p>
<p>
   scatter/gather pattern for, 75
  </p>
<p>
   with leaf sharding, 77
  </p>
<p>
   documentation, <strong>sidecar container</strong>, 18
  </p>
<p>
   dynamic configuration, 12
  </p>
<p>
   E
  </p>
<p>
   etcd (distributed lock server), 97
  </p>
<p>
   implementing leases in, 102
  </p>
<p>
   implementing locks in, 100
  </p>
<p>
   event handling, <strong>FaaS</strong> and, 87-89
  </p>
<p>
   event-based pipelines
  </p>
<p>
<strong>FaaS</strong> and, 89-91
  </p>
<p>
   for new-user signup, 89-91
  </p>
<p>
   event-driven batch processing systems, 121-131
  </p>
<p>
   copier pattern, 122
  </p>
<p>
   filter pattern, 123
  </p>
<p>
   for new-user signup, 128
  </p>
<p>
<strong>Kafka</strong> deployment, 130
  </p>
<p>
   merger pattern, 127
  </p>
<p>
   patterns of, 122-127
  </p>
<p>
   publisher/subscriber infrastructure, 129
  </p>
<p>
   sharder, 125
  </p>
<p>
   splitter pattern, 124
  </p>
<p>
   event-driven <strong>FaaS</strong>, 81
  </p>
<p>
   event-driven processing, functions and, 81-91
  </p>
<p>
   (see also function-as-a-service)
  </p>
<p>
   events, requests vs., 87
  </p>
<p>
   experimentation
  </p>
<p>
<strong>ambassador patterns</strong> for, 26-29
  </p>
<p>
   implementing 10% experiments, 27-29
  </p>
<p>
   F
  </p>
<p>
   filter pattern, 123
  </p>
<p>
   fluentd, 35
  </p>
<p>
   function-as-a-service (<strong>FaaS</strong>), 81-91
  </p>
<p>
   adding request defaulting prior to request processing, 86
  </p>
<p>
   and need to hold data in memory, 83
  </p>
<p>
   and situations that require background pro‐ cessing, 82
  </p>
<p>
   benefits of, 82
  </p>
<p>
   challenges of, 82
  </p>
<p>
   costs of sustained request-based processing, 84
  </p>
<p>
   decorator pattern, 85
  </p>
<p>
   event-based pipelines, 89-91
  </p>
<p>
   handling events, 87-89
  </p>
<p>
   implementing two-factor authentication, 87-89
  </p>
<p>
   patterns for, 84-91
  </p>
<p>
   serverless computing and, 81
  </p>
<p>
   when to use, 82-84
  </p>
<p>
   G
  </p>
<p>
   Gamma, Erich, 3
  </p>
<p>
   H
  </p>
<p>
   hashing function
  </p>
<p>
   consistent, 49
  </p>
<p>
   sharding function and, 67
  </p>
<p>
   146
  </p>
<p>
   Index
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0160</div>
            </div>
        </div>
        <!-- Page 0161 -->
        <div class="chapter" id="page-0161">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   health monitoring
  </p>
<p>
   for <strong>MySQL</strong>, 37-39
  </p>
<p>
   of application containers, 36-39
  </p>
<p>
<strong>Helm</strong>, 97, 130
  </p>
<p>
   histograms, 137
  </p>
<p>
   hit rate, 50, 61
  </p>
<p>
   horizontally scalable systems, 46
  </p>
<p>
   hot sharding systems, 70
  </p>
<p>
<strong>HTTP</strong> requests, 69
  </p>
<p>
<strong>HTTPS</strong>, adding to a legacy web service with sidecar patterns, 11
  </p>
<p>
   I
  </p>
<p>
   image tagging/processing <strong>pipeline</strong>, 138-141
  </p>
<p>
   index, with scatter/gather pattern, 75
  </p>
<p>
   J
  </p>
<p>
   join pattern
  </p>
<p>
   coordinated batch processing, 134
  </p>
<p>
   reduce pattern vs., 136
  </p>
<p>
   K
  </p>
<p>
<strong>Kafka</strong>, deployment with event-driven batch processing system, 130
  </p>
<p>
   key, sharding function, 67
  </p>
<p>
   key-value stores, 96, 99
  </p>
<p>
   Knuth, Donald, 3
  </p>
<p>
<strong>Kubeless</strong>, 86
  </p>
<p>
<strong>Kubernetes</strong>
</p>
<p>
   creating a replicated service in, 47
  </p>
<p>
   etcd and, 97
  </p>
<p>
<strong>Kafka</strong> deployment as container, 130
  </p>
<p>
<strong>Kubeless</strong> and, 86
  </p>
<p>
   pod definition for <strong>Redis</strong> server, 33
  </p>
<p>
   sharded memcache deployment, 63
  </p>
<p>
   sharded <strong>Redis service</strong> deployment, 23-25
  </p>
<p>
   L
  </p>
<p>
<strong>Label Schema project</strong>, 19
  </p>
<p>
   latency
  </p>
<p>
   caching, 61
  </p>
<p>
   containerization, 7
  </p>
<p>
   leaf sharding
  </p>
<p>
   choosing the right number of leaves, 78
  </p>
<p>
   document search with, 77
  </p>
<p>
   scatter/gather with, 76-79
  </p>
<p>
   leases, 102
  </p>
<p>
   load-balanced services (see replicated load- balanced services)
  </p>
<p>
   lock (see mutual exclusion lock)
  </p>
<p>
   logging
  </p>
<p>
<strong>adapter patterns</strong> for, 34-36
  </p>
<p>
   normalizing different formats with fluentd, 35
  </p>
<p>
   M
  </p>
<p>
<strong>MapReduce pattern</strong>, 134-135, 137, 141
  </p>
<p>
   master election
  </p>
<p>
   basics, 95-103
  </p>
<p>
   determining need for master election, 94
  </p>
<p>
   etcd deployment, 97
  </p>
<p>
   implementing leases in etcd, 102
  </p>
<p>
   implementing locks, 98-101
  </p>
<p>
   implementing ownership, 101
  </p>
<p>
   memcache, sharded, 63-66
  </p>
<p>
   merger pattern, 127
  </p>
<p>
   micro-containers, 18
  </p>
<p>
   microservices
  </p>
<p>
   advantages of, 42
  </p>
<p>
   basics, 41-43
  </p>
<p>
   deploying experiment framework as, 27
  </p>
<p>
   disadvantages of, 43
  </p>
<p>
   event-based pipelines vs., 89
  </p>
<p>
   modular application containers, 14
  </p>
<p>
   modularity, designing sidecars for, 16-19
  </p>
<p>
   modulo (%) operator, 67
  </p>
<p>
   monitoring
  </p>
<p>
<strong>adapter patterns</strong> for, 32-34
  </p>
<p>
   of application containers, 36-39
  </p>
<p>
   rich health monitoring for <strong>MySQL</strong>, 36-39
  </p>
<p>
   with <strong>Prometheus</strong>, 33
  </p>
<p>
   monolithic systems, microservices vs., 41
  </p>
<p>
   multi-node batch patterns, 107
  </p>
<p>
   multi-node patterns, 41-43
  </p>
<p>
   mutual exclusion lock (Mutex)
  </p>
<p>
   implementing, 98-101
  </p>
<p>
   in etcd, 100
  </p>
<p>
   MySQL database
  </p>
<p>
<strong>ambassador patterns</strong> for service brokering with, 25
  </p>
<p>
   rich health monitoring for, 37-39
  </p>
<p>
   N
  </p>
<p>
   new-user signup
  </p>
<p>
   event-driven flow for, 128
  </p>
<p>
   implementing a <strong>pipeline</strong> for, 89-91
  </p>
<p>
   Index
  </p>
<p>
   147
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0161</div>
            </div>
        </div>
        <!-- Page 0162 -->
        <div class="chapter" id="page-0162">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   nginx <strong>server</strong>
</p>
<p>
   as <strong>ambassador</strong>, 27-29
  </p>
<p>
<strong>SSL</strong>-terminating, 55-56
  </p>
<p>
   O
  </p>
<p>
   object-oriented programming, patterns for, 3
  </p>
<p>
   open source <strong>software</strong>, 3
  </p>
<p>
   ownership election, 93-105
  </p>
<p>
   (see also master election)
  </p>
<p>
   determining need for master election, 94
  </p>
<p>
   handling concurrent data manipulation, 103-105
  </p>
<p>
   master election basics, 95-103
  </p>
<p>
   P
  </p>
<p>
<strong>PaaS</strong> (see platform as a service)
  </p>
<p>
   parameterized <strong>sidecar containers</strong>, 17
  </p>
<p>
   patterns, 2-4
  </p>
<p>
   (see also specific types, e.g.: container pat‐ terns)
  </p>
<p>
   as collection of best practices, 4
  </p>
<p>
   as shared language, 5
  </p>
<p>
   defined, 4
  </p>
<p>
   event-driven batch processing systems, 122-127
  </p>
<p>
   for <strong>FaaS</strong>, 84-91
  </p>
<p>
   formalization of algorithmic programming, 3
  </p>
<p>
   identifying shared components with, 5
  </p>
<p>
   object-oriented programming and, 3
  </p>
<p>
   open source <strong>software</strong> and, 3
  </p>
<p>
   value of, 4-6
  </p>
<p>
   pipelines (see event-based pipelines)
  </p>
<p>
   platform as a service (<strong>PaaS</strong>), 15
  </p>
<p>
   pod, 9
  </p>
<p>
   pricing, <strong>FaaS</strong> and, 84
  </p>
<p>
<strong>Prometheus</strong>, 33
  </p>
<p>
   publisher/subscriber <strong>API</strong>, 129
  </p>
<p>
<strong>Python</strong>
</p>
<p>
   decorator pattern, 85
  </p>
<p>
   R
  </p>
<p>
   rate limiting, 54
  </p>
<p>
   readiness probes, 46
  </p>
<p>
<strong>Redis</strong>
</p>
<p>
   and <strong>adapter pattern</strong>, 33, 35
  </p>
<p>
   sharded, 23-25
  </p>
<p>
   reduce pattern, 135
  </p>
<p>
   (see also <strong>MapReduce pattern</strong>)
  </p>
<p>
   renewable lock, 101
  </p>
<p>
   replicated load-balanced services, 45-57
  </p>
<p>
   application-layer services, 49
  </p>
<p>
   creating a service in <strong>Kubernetes</strong>, 47
  </p>
<p>
   expanding the caching layer, 53-57
  </p>
<p>
   introducing a caching layer, 49-53
  </p>
<p>
   readiness probes for load balancing, 46
  </p>
<p>
   session tracked services, 48
  </p>
<p>
   stateless services, 45-48
  </p>
<p>
   request decorator, 85
  </p>
<p>
   request splitting
  </p>
<p>
<strong>ambassador patterns</strong> for, 26-29
  </p>
<p>
   implementing 10% experiments, 27-29
  </p>
<p>
   request-based processing, <strong>FaaS</strong> and, 84
  </p>
<p>
   requests, events vs., 87
  </p>
<p>
   resource isolation, 7
  </p>
<p>
   resource version, 100
  </p>
<p>
   response decorator, 85
  </p>
<p>
   root (load-balancing node), 59
  </p>
<p>
   S
  </p>
<p>
   scaling
  </p>
<p>
   assignment (see ownership election)
  </p>
<p>
   cache, 50
  </p>
<p>
   consistent hashing function and, 49
  </p>
<p>
<strong>FaaS</strong>, 84
  </p>
<p>
   horizontal, 46
  </p>
<p>
   hot sharding systems and, 70
  </p>
<p>
   microservice decoupling and, 42
  </p>
<p>
   scatter/gather pattern (see scatter/gather pattern)
  </p>
<p>
   sharding (see sharded services)
  </p>
<p>
   straggler problem, 78
  </p>
<p>
   teams, 8
  </p>
<p>
   scatter/gather pattern, 73-80
  </p>
<p>
   distributed document search, 75
  </p>
<p>
   leaf sharding, 76-79
  </p>
<p>
   root distribution, 74
  </p>
<p>
   scaling for reliability and scale, 79
  </p>
<p>
   separation of concerns
  </p>
<p>
<strong>ambassador pattern</strong>, 23
  </p>
<p>
   containerization, 8
  </p>
<p>
   serverless computing, <strong>FaaS</strong> vs., 81
  </p>
<p>
   service broker, defined, 25
  </p>
<p>
   service brokering, <strong>ambassador</strong> for, 25
  </p>
<p>
   service discovery, 25
  </p>
<p>
   serving patterns
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0162</div>
            </div>
        </div>
        <!-- Page 0163 -->
        <div class="chapter" id="page-0163">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<p>
   functions and event-driven processing, 81-91
  </p>
<p>
   multi-node patterns, 41-43
  </p>
<p>
   replicated load-balanced services, 45-57
  </p>
<p>
   scatter/gather, 73-80
  </p>
<p>
   sharded services, 59-70
  </p>
<p>
   stateless services, 45-48
  </p>
<p>
   session tracked services, 48
  </p>
<p>
   sessions, requests and, 87
  </p>
<p>
   shard router service, 65
  </p>
<p>
   shard, defined, 59
  </p>
<p>
   sharded caching, 59-66
  </p>
<p>
   defined, 59
  </p>
<p>
   deploying <strong>ambassador</strong> and memcache for, 63-66
  </p>
<p>
   reasons to use, 60
  </p>
<p>
   replicated, sharded caches, 62
  </p>
<p>
   role in system performance, 61
  </p>
<p>
   sharded services, 59-70
  </p>
<p>
   hot sharding systems, 70
  </p>
<p>
   sharded caching, 59-66
  </p>
<p>
   sharding functions, 66-70
  </p>
<p>
   shared replicated serving, 70
  </p>
<p>
   sharding, 66-70
  </p>
<p>
   building a consistent <strong>HTTP sharding proxy</strong>, 69
  </p>
<p>
   consistent hashing functions, 68
  </p>
<p>
   event-driven batch processing systems, 125
  </p>
<p>
   leaf (see leaf sharding)
  </p>
<p>
<strong>Redis</strong>, 23-25
  </p>
<p>
   selecting a key, 67
  </p>
<p>
   with <strong>ambassador patterns</strong>, 22-25
  </p>
<p>
   sharding <strong>ambassador proxy</strong>, 23
  </p>
<p>
<strong>sidecar container</strong>, 5, 11
  </p>
<p>
<strong>sidecar patterns</strong>, 11-20
  </p>
<p>
   adding <strong>HTTPS</strong> to a legacy service, 11
  </p>
<p>
   container documentation, 18
  </p>
<p>
   defining container <strong>APIs</strong>, 17
  </p>
<p>
   designing for modularity and reusability, 16-19
  </p>
<p>
   dynamic configuration with, 12
  </p>
<p>
   elements of, 11
  </p>
<p>
   modular application containers, 14
  </p>
<p>
   parameterized containers for, 17
  </p>
<p>
   simple <strong>PaaS</strong> with, 15
  </p>
<p>
   web cache deployment, 50
  </p>
<p>
   single-node container patterns, 7-9
  </p>
<p>
<strong>ambassadors</strong>, 21-29
  </p>
<p>
   reasons for using, 7-9
  </p>
<p>
<strong>sidecar patterns</strong>, 11-20
  </p>
<p>
   single-node patterns
  </p>
<p>
   adapters, 31-39
  </p>
<p>
   container patterns, 7-9
  </p>
<p>
   splitter pattern, 124
  </p>
<p>
<strong>SSL</strong> termination, caching layer for, 54-57
  </p>
<p>
   stateless services, 45-48
  </p>
<p>
   caching layer, 49-53
  </p>
<p>
   creating a service in <strong>Kubernetes</strong>, 47
  </p>
<p>
   defined, 45
  </p>
<p>
   readiness probes for load balancing, 46
  </p>
<p>
   storage layer sharding, 22-25
  </p>
<p>
   straggler problem, 78
  </p>
<p>
   sums, coordinated batch processing for, 137
  </p>
<p>
   systems development, history of, 1
  </p>
<p>
   T
  </p>
<p>
   team scaling, 8
  </p>
<p>
   teeing, 26
  </p>
<p>
   three-nines service, 46
  </p>
<p>
   time-to-live (<strong>TTL</strong>), 96, 99
  </p>
<p>
   topz <strong>sidecar</strong>, 14
  </p>
<p>
   twemproxy, 24
  </p>
<p>
   two-factor authentication, <strong>FaaS</strong> for, 87-89
  </p>
<p>
   U
  </p>
<p>
   user signup
  </p>
<p>
   event-driven flow for, 128
  </p>
<p>
   implementing a <strong>pipeline</strong> for, 89-91
  </p>
<p>
   V
  </p>
<p>
<strong>Varnish</strong>, 50, 52-53
  </p>
<p>
   W
  </p>
<p>
<strong>workflow systems</strong>, 121
  </p>
<p>
   (see also event-driven batch processing sys‐ tems)
  </p>
<p>
   Index
  </p>
<p>
   149
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0163</div>
            </div>
        </div>
        <!-- Page 0164 -->
        <div class="chapter" id="page-0164">
            <div class="chapter-content">
                <div class="translated-content">
<div>
<h3>About the Author</h3>
<p>
   Brendan Burns is a distinguished engineer at Microsoft and a cofounder of the <strong>Kubernetes</strong> open source project. At Microsoft he works on Azure, focusing on <strong>Containers</strong> and <strong>DevOps</strong>. Prior to Microsoft, he worked at Google in the Google <strong>Cloud Platform</strong>, where he helped build <strong>APIs</strong> like <strong>Deployment Manager</strong> and <strong>Cloud DNS</strong>.
  </p>
<p>
   Before working on <strong>cloud computing</strong>, he worked on Google’s web-search infrastructure, with a focus on low-latency indexing. He has a <strong>PhD</strong> in computer science from the University of Massachusetts Amherst with a specialty in robotics. He lives in Seattle with his wife, Robin Sanders, their two children, and a cat, Mrs. Paws, who rules over their household with an iron paw.
  </p>
<h3>Colophon</h3>
<p>
   The animal on the cover of Designing Distributed Systems is a Java sparrow. This bird is loathed in the wild but loved in captivity. The Java’s scientific name is Padda oryzivora. Padda stands for paddy, the method of cultivating rice, and Oryza is the genus for domestic rice. Therefore, Padda oryzivora means “rice paddy eater.” Farmers destroy thousands of wild Javas each year to prevent the flocks from devouring their crops. They also trap the birds for food or sell them in the international bird trade.
  </p>
<p>
   Despite this battle, the species continues to thrive in Java and Bali in Indonesia, as well as Australia, Mexico, and North America.
  </p>
<p>
   Its plumage is pearly-grey, turning pinkish on the front and white towards the tail. It has a black head with white cheeks. Its large bill, legs, and eye circles are bright pink.
  </p>
<p>
   The song of the Java sparrow begins with single notes, like a bell, before developing into a continuous trilling and clucking, mixed with high-pitched and deeper notes.
  </p>
<p>
   The main part of their diet is rice, but they also eat small seeds, grasses, insects, and flowering plants. In the wild, these birds will build a nest out of dried grass normally under the roofs of buildings or in bushes or treetops. The Java will lay a clutch of three or four eggs between February to August, with most eggs laid in April or May.
  </p>
<p>
   Its striking plumage, enchanting sounds, and ease of care create a demand for these birds in the cage-bird trade. Conservation efforts are underway to ensure that the market demand is met by captive-bred birds rather than wild caught.
  </p>
<p>
   Many of the animals on O’Reilly covers are endangered; all of them are important to the world. To learn more about how you can help, go to animals.oreilly.com.
  </p>
<p>
   The cover image is from Lydekker’s Royal Natural History. The cover fonts are URW Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.
  </p>
</div>
</div>
                <div class="page-images">
</div>
                <div class="page-number">صفحه 0164</div>
            </div>
        </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script>
        document.addEventListener('DOMContentLoaded', (event) => {
            if (window.Prism) {
                Prism.highlightAll();
            }
        });
    </script>
</body>
</html>