80 
Part Two  Quality Attributes	
5—Availability
the system from entering a hazardous state and recovering or limiting the damage 
when it does.
Fundamentally, availability is about minimizing service outage time by mit-
igating faults. Failure implies visibility to a system or human observer in the en-
vironment. That is, a failure is the deviation of the system from its specification, 
where the deviation is externally visible. One of the most demanding tasks in 
building a high-availability, fault-tolerant system is to understand the nature of 
the failures that can arise during operation (see the sidebar “Planning for Fail-
ure”). Once those are understood, mitigation strategies can be designed into the 
software.
A failure’s cause is called a fault. A fault can be either internal or external to 
the system under consideration. Intermediate states between the occurrence of a 
fault and the occurrence of a failure are called errors. Faults can be prevented, tol-
erated, removed, or forecast. In this way a system becomes “resilient” to faults.
Among the areas with which we are concerned are how system faults are 
detected, how frequently system faults may occur, what happens when a fault 
occurs, how long a system is allowed to be out of operation, when faults or fail-
ures may occur safely, how faults or failures can be prevented, and what kinds of 
notifications are required when a failure occurs. 
Because a system failure is observable by users, the time to repair is the time 
until the failure is no longer observable. This may be a brief delay in the response 
time or it may be the time it takes someone to fly to a remote location in the An-
des to repair a piece of mining machinery (as was recounted to us by a person 
responsible for repairing the software in a mining machine engine). The notion 
of “observability” can be a tricky one: the Stuxnet virus, as an example, went un-
observed for a very long time even though it was doing damage. In addition, we 
are often concerned with the level of capability that remains when a failure has 
occurred—a degraded operating mode.
The distinction between faults and failures allows discussion of automatic 
repair strategies. That is, if code containing a fault is executed but the system is 
able to recover from the fault without any deviation from specified behavior be-
ing observable, there is no failure. 
The availability of a system can be calculated as the probability that it will 
provide the specified services within required bounds over a specified time inter-
val. When referring to hardware, there is a well-known expression used to derive 
steady-state availability:
MTBF
(MTBF + MTTR)
where MTBF refers to the mean time between failures and MTTR refers to the 
mean time to repair. In the software world, this formula should be interpreted 
to mean that when thinking about availability, you should think about what will 
make your system fail, how likely that is to occur, and that there will be some 
time required to repair it.
