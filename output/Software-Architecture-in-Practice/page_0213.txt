192 
Part Two  Quality Attributes	
12—Other Quality Attributes
has no effect whatsoever that is observable outside of itself, it probably 
serves no purpose.
There are notorious examples of software-related failures. The Siberian 
hydroelectric plant catastrophe mentioned in the text, the Therac-25 fatal 
radiation overdose, the Ariane 5 explosion, and a hundred lesser known 
accidents all caused harm because the software was part of a system that 
included a turbine, an X-ray emitter, or a rocket’s steering controls, in the 
examples just cited. In these cases, flawed software commanded some 
hardware in the system to take a disastrous action, and the hardware sim-
ply obeyed. Actuators are devices that connect hardware to software; they 
are the bridge between the world of 0’s and 1’s and the world of motion and 
control. Send a digital value to an actuator (or write a bit string in the hard-
ware register corresponding to the actuator) and that value is translated to 
some mechanical action, for better or worse. 
But connection to an actuator is not required for software-related disas-
ters. Sometimes all the computer has to do is send erroneous information 
to its human operators. In September 1983, a Soviet satellite sent data 
to its ground system computer, which interpreted that data as a missile 
launched from the United States aimed at Moscow. Seconds later, the 
computer reported a second missile in flight. Soon, a third, then a fourth, 
and then a fifth appeared. Soviet Strategic Rocket Forces lieutenant colonel 
Stanislav Yevgrafovich Petrov made the astonishing decision to ignore the 
warning system, believing it to be in error. He thought it extremely unlikely 
that the U.S. would have fired just a few missiles, thereby inviting total 
retaliatory destruction. He decided to wait it out, to see if the missiles were 
real—that is, to see if his country’s capital city was going to be incinerated. 
As we know, it wasn’t. The Soviet system had mistaken a rare sunlight con-
dition for missiles in flight. Similar mistakes have occurred on the U.S. side.
Of course, the humans don’t always get it right. On the dark and stormy 
night of June 1, 2009, Air France flight 447 from Rio de Janeiro to Paris 
plummeted into the Atlantic Ocean, killing all on board. The Airbus A-330’s 
flight recorders were not recovered until May 2011, and as this book goes 
to publication it appears that the pilots never knew that the aircraft had en-
tered a high-altitude stall. The sensors that measure airspeed had become 
clogged with ice and therefore unreliable. The software was required to dis-
engage the autopilot in this situation, which it did. The human pilots thought 
the aircraft was going too fast (and in danger of structural failure) when in 
fact it was going too slow (and falling). During the entire three-minute-plus 
plunge from 38,000 feet, the pilots kept trying to pull the nose up and throt-
tles back to lower the speed. It’s a good bet that adding to the confusion 
was the way the A-330’s stall warning system worked. When the system 
detects a stall, it emits a loud audible alarm. The computers deactivate the 
stall warning when they “think” that the angle of attack measurements are 
invalid. This can occur when the airspeed readings are very low. That is ex-
actly what happened with Air France 447: Its forward speed dropped below 
60 knots, and the angle of attack was extremely high. As a consequence 
of a rule in the flight control software, the stall warning stopped and started 
